{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# install necessary packages\n",
    "!pip install gdown lightning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1",
   "metadata": {},
   "source": [
    "## Get data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "!gdown https://drive.google.com/uc?id=1fZG_-FwADGMI_TqbPaA8z10ZMGPvb_vG\n",
    "!unzip -q data.zip\n",
    "!mv an2dl2526c2v2 data\n",
    "!ls -l data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the trash list\n",
    "!gdown https://drive.google.com/uc?id=1D8tGrxR4oHZmOxKvINNQR5x-zI6MavVP\n",
    "!mv trash_list.txt data/trash_list.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls -l"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5",
   "metadata": {},
   "source": [
    "## Import Libraries\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from typing import Any, Dict, List, Optional, Tuple\n",
    "\n",
    "import cv2\n",
    "import lightning as L\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import timm\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from PIL import Image\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchmetrics import AUROC, Accuracy, ConfusionMatrix, F1Score\n",
    "from torchvision import transforms"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7",
   "metadata": {},
   "source": [
    "## Utilities\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8",
   "metadata": {},
   "source": [
    "### Config\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Config:\n",
    "    # Data paths\n",
    "    DATA_DIR = \"./data\"\n",
    "    TRAIN_DATA_DIR = \"./data/train_data\"\n",
    "    TEST_DATA_DIR = \"./data/test_data\"\n",
    "    TRAIN_LABELS_PATH = \"./data/train_labels.csv\"\n",
    "    OUTPUT_PATH = \"./predictions.csv\"\n",
    "    # Class labels\n",
    "    CLASSES = [\"Luminal A\", \"Luminal B\", \"HER2(+)\", \"Triple negative\"]\n",
    "    NUM_CLASSES = 4\n",
    "\n",
    "    # Image settings\n",
    "    IMG_SIZE = 224  # Larger size for histopathology\n",
    "    # Tissue detection settings\n",
    "    MIN_TISSUE_AREA = 0.05  # Minimum tissue area ratio\n",
    "\n",
    "    # Patch-based settings (for very large images)\n",
    "    # Stain normalization\n",
    "    USE_STAIN_NORMALIZATION = False\n",
    "\n",
    "    # Training settings\n",
    "    BATCH_SIZE = 16\n",
    "    NUM_WORKERS = 2\n",
    "    MAX_EPOCHS = 200\n",
    "    LEARNING_RATE = 1e-4\n",
    "\n",
    "    # Validation split\n",
    "    VAL_SPLIT = 0.2\n",
    "    RANDOM_SEED = 42"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10",
   "metadata": {},
   "source": [
    "### Extract tissue\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "\n",
    "\n",
    "class TissueExtractor:\n",
    "    \"\"\"\n",
    "    Extract patches from images using existing masks.\n",
    "    Designed for workflow where ground truth masks are already available.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, patch_size: int = 224, min_tissue_ratio: float = 0.05):\n",
    "        self.patch_size = patch_size\n",
    "        self.min_tissue_ratio = min_tissue_ratio\n",
    "\n",
    "    def _validate_inputs(self, img: np.ndarray, mask: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"Validate inputs and return processed mask.\"\"\"\n",
    "        # Check dimensions match\n",
    "        if img.shape[:2] != mask.shape[:2]:\n",
    "            raise ValueError(\n",
    "                f\"Image shape {img.shape[:2]} doesn't match mask shape {mask.shape[:2]}\"\n",
    "            )\n",
    "\n",
    "        # Check if image is large enough\n",
    "        h, w = img.shape[:2]\n",
    "        if h < self.patch_size or w < self.patch_size:\n",
    "            raise ValueError(\n",
    "                f\"Image dimensions ({h}, {w}) smaller than patch_size ({self.patch_size})\"\n",
    "            )\n",
    "\n",
    "        # Convert multi-channel mask to single channel\n",
    "        if len(mask.shape) == 3:\n",
    "            mask = mask[:, :, 0]\n",
    "\n",
    "        # Handle float masks (threshold at 0.5 if float, otherwise use > 0)\n",
    "        if mask.dtype in [np.float32, np.float64]:\n",
    "            warnings.warn(\"Float mask detected, thresholding at 0.5\")\n",
    "            mask = (mask > 0.5).astype(np.uint8)\n",
    "\n",
    "        return mask\n",
    "\n",
    "    def get_valid_patches(\n",
    "        self,\n",
    "        img: np.ndarray,\n",
    "        mask: np.ndarray,\n",
    "        num_patches: int = 8,\n",
    "        strategy: str = \"random\",\n",
    "        stride: Optional[int] = None,\n",
    "        shuffle: bool = True,\n",
    "        min_distance: int = None,  # NEW: minimum distance between random patches\n",
    "    ) -> Tuple[List[np.ndarray], List[np.ndarray]]:\n",
    "        \"\"\"\n",
    "        Extract patches from image based on mask.\n",
    "\n",
    "        Args:\n",
    "            img: RGB image (H, W, 3)\n",
    "            mask: Binary or Index mask (H, W). Assumes tissue > 0.\n",
    "            num_patches: Number of patches to extract per image.\n",
    "            strategy: 'random' samples points from mask; 'grid' slides across image.\n",
    "            stride: Step size for grid strategy. Defaults to patch_size (no overlap).\n",
    "            shuffle: Whether to shuffle grid patches before selecting.\n",
    "            min_distance: Minimum pixel distance between patch centers (random strategy).\n",
    "\n",
    "        Returns:\n",
    "            images: List of RGB patches\n",
    "            masks: List of corresponding Mask patches\n",
    "        \"\"\"\n",
    "        # Validate inputs\n",
    "        mask = self._validate_inputs(img, mask)\n",
    "        h, w = img.shape[:2]\n",
    "\n",
    "        # Find all tissue pixel indices\n",
    "        tissue_indices = np.where(mask > 0)\n",
    "\n",
    "        if len(tissue_indices[0]) == 0:\n",
    "            warnings.warn(\"No tissue found in mask!\")\n",
    "            return [], []\n",
    "\n",
    "        if strategy == \"random\":\n",
    "            patches_img, patches_mask = self._extract_random(\n",
    "                img, mask, tissue_indices, num_patches, h, w, min_distance\n",
    "            )\n",
    "        elif strategy == \"grid\":\n",
    "            patches_img, patches_mask = self._extract_grid(\n",
    "                img, mask, num_patches, h, w, stride, shuffle\n",
    "            )\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown strategy: {strategy}. Use 'random' or 'grid'.\")\n",
    "\n",
    "        # Warn if fewer patches than requested\n",
    "        if len(patches_img) < num_patches:\n",
    "            warnings.warn(\n",
    "                f\"Only {len(patches_img)} patches extracted (requested {num_patches})\"\n",
    "            )\n",
    "\n",
    "        return patches_img, patches_mask\n",
    "\n",
    "    def _extract_random(\n",
    "        self,\n",
    "        img: np.ndarray,\n",
    "        mask: np.ndarray,\n",
    "        tissue_indices: Tuple[np.ndarray, np.ndarray],\n",
    "        num_patches: int,\n",
    "        h: int,\n",
    "        w: int,\n",
    "        min_distance: Optional[int] = None,\n",
    "    ) -> Tuple[List[np.ndarray], List[np.ndarray]]:\n",
    "        \"\"\"Random sampling strategy with optional minimum distance between patches.\"\"\"\n",
    "\n",
    "        patches_img = []\n",
    "        patches_mask = []\n",
    "        selected_centers = []  # Track selected patch centers\n",
    "\n",
    "        if min_distance is None:\n",
    "            min_distance = self.patch_size // 2  # Default: half patch size\n",
    "\n",
    "        attempts = 0\n",
    "        max_attempts = num_patches * 100  # Increased for distance constraint\n",
    "\n",
    "        while len(patches_img) < num_patches and attempts < max_attempts:\n",
    "            attempts += 1\n",
    "\n",
    "            idx = np.random.randint(len(tissue_indices[0]))\n",
    "            cy, cx = tissue_indices[0][idx], tissue_indices[1][idx]\n",
    "\n",
    "            # Check minimum distance from existing patches\n",
    "            if min_distance > 0 and selected_centers:\n",
    "                too_close = False\n",
    "                for prev_cy, prev_cx in selected_centers:\n",
    "                    dist = np.sqrt((cy - prev_cy) ** 2 + (cx - prev_cx) ** 2)\n",
    "                    if dist < min_distance:\n",
    "                        too_close = True\n",
    "                        break\n",
    "                if too_close:\n",
    "                    continue\n",
    "\n",
    "            # Calculate patch bounds (centered on the selected pixel)\n",
    "            half_size = self.patch_size // 2\n",
    "            y_min = cy - half_size\n",
    "            x_min = cx - half_size\n",
    "            y_max = y_min + self.patch_size\n",
    "            x_max = x_min + self.patch_size\n",
    "\n",
    "            # Boundary check\n",
    "            if y_min < 0 or x_min < 0 or y_max > h or x_max > w:\n",
    "                continue\n",
    "\n",
    "            # Extract and validate mask patch\n",
    "            mask_patch = mask[y_min:y_max, x_min:x_max]\n",
    "            current_ratio = np.count_nonzero(mask_patch) / mask_patch.size\n",
    "\n",
    "            if current_ratio >= self.min_tissue_ratio:\n",
    "                img_patch = img[y_min:y_max, x_min:x_max]\n",
    "                patches_img.append(img_patch)\n",
    "                patches_mask.append(mask_patch)\n",
    "                selected_centers.append((cy, cx))\n",
    "\n",
    "        return patches_img, patches_mask\n",
    "\n",
    "    def _extract_grid(\n",
    "        self,\n",
    "        img: np.ndarray,\n",
    "        mask: np.ndarray,\n",
    "        num_patches: int,\n",
    "        h: int,\n",
    "        w: int,\n",
    "        stride: Optional[int] = None,\n",
    "        shuffle: bool = True,\n",
    "    ) -> Tuple[List[np.ndarray], List[np.ndarray]]:\n",
    "        \"\"\"Grid strategy: systematically slide across the TISSUE REGION only.\"\"\"\n",
    "\n",
    "        if stride is None:\n",
    "            stride = self.patch_size\n",
    "\n",
    "        if stride <= 0:\n",
    "            raise ValueError(f\"Stride must be positive, got {stride}\")\n",
    "\n",
    "        # ===== NEW: Find tissue bounding box =====\n",
    "        tissue_rows = np.any(mask > 0, axis=1)\n",
    "        tissue_cols = np.any(mask > 0, axis=0)\n",
    "\n",
    "        if not tissue_rows.any() or not tissue_cols.any():\n",
    "            return [], []  # No tissue found\n",
    "\n",
    "        y_min_tissue, y_max_tissue = np.where(tissue_rows)[0][[0, -1]]\n",
    "        x_min_tissue, x_max_tissue = np.where(tissue_cols)[0][[0, -1]]\n",
    "\n",
    "        # Add padding (half patch size) to ensure we cover edges\n",
    "        padding = self.patch_size // 2\n",
    "        y_start = max(0, y_min_tissue - padding)\n",
    "        y_end = min(h, y_max_tissue + padding)\n",
    "        x_start = max(0, x_min_tissue - padding)\n",
    "        x_end = min(w, x_max_tissue + padding)\n",
    "\n",
    "        # ===== Grid within tissue bounding box =====\n",
    "        y_positions = list(range(y_start, y_end - self.patch_size + 1, stride))\n",
    "        x_positions = list(range(x_start, x_end - self.patch_size + 1, stride))\n",
    "\n",
    "        valid_patches = []\n",
    "\n",
    "        for y_min in y_positions:\n",
    "            for x_min in x_positions:\n",
    "                y_max = y_min + self.patch_size\n",
    "                x_max = x_min + self.patch_size\n",
    "\n",
    "                mask_patch = mask[y_min:y_max, x_min:x_max]\n",
    "                tissue_ratio = np.count_nonzero(mask_patch) / mask_patch.size\n",
    "\n",
    "                if tissue_ratio >= self.min_tissue_ratio:\n",
    "                    valid_patches.append((y_min, x_min, tissue_ratio))\n",
    "\n",
    "        if shuffle:\n",
    "            np.random.shuffle(valid_patches)\n",
    "        else:\n",
    "            # Sort by tissue ratio (highest first) for deterministic selection\n",
    "            valid_patches.sort(key=lambda x: x[2], reverse=True)\n",
    "\n",
    "        patches_img = []\n",
    "        patches_mask = []\n",
    "\n",
    "        for y_min, x_min, _ in valid_patches[:num_patches]:\n",
    "            y_max = y_min + self.patch_size\n",
    "            x_max = x_min + self.patch_size\n",
    "\n",
    "            patches_img.append(img[y_min:y_max, x_min:x_max])\n",
    "            patches_mask.append(mask[y_min:y_max, x_min:x_max])\n",
    "\n",
    "        return patches_img, patches_mask\n",
    "\n",
    "    def get_all_valid_patches(\n",
    "        self,\n",
    "        img: np.ndarray,\n",
    "        mask: np.ndarray,\n",
    "        stride: Optional[int] = None,\n",
    "    ) -> Tuple[List[np.ndarray], List[np.ndarray], List[Tuple[int, int]]]:\n",
    "        \"\"\"Extract ALL valid patches from the image using grid strategy.\"\"\"\n",
    "\n",
    "        mask = self._validate_inputs(img, mask)\n",
    "        h, w = img.shape[:2]\n",
    "\n",
    "        if stride is None:\n",
    "            stride = self.patch_size\n",
    "\n",
    "        y_positions = list(range(0, h - self.patch_size + 1, stride))\n",
    "        x_positions = list(range(0, w - self.patch_size + 1, stride))\n",
    "\n",
    "        patches_img = []\n",
    "        patches_mask = []\n",
    "        coordinates = []\n",
    "\n",
    "        for y_min in y_positions:\n",
    "            for x_min in x_positions:\n",
    "                y_max = y_min + self.patch_size\n",
    "                x_max = x_min + self.patch_size\n",
    "\n",
    "                mask_patch = mask[y_min:y_max, x_min:x_max]\n",
    "                tissue_ratio = np.count_nonzero(mask_patch) / mask_patch.size\n",
    "\n",
    "                if tissue_ratio >= self.min_tissue_ratio:\n",
    "                    patches_img.append(img[y_min:y_max, x_min:x_max])\n",
    "                    patches_mask.append(mask_patch)\n",
    "                    coordinates.append((y_min, x_min))\n",
    "\n",
    "        return patches_img, patches_mask, coordinates"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12",
   "metadata": {},
   "source": [
    "## Custom dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from typing import List, Tuple\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "class PathologyDataset(Dataset):\n",
    "    \"\"\"Dataset optimized for histopathology images.\n",
    "\n",
    "    Args:\n",
    "        data_dir: Directory containing images and masks.\n",
    "        labels_df: DataFrame with 'sample_index' and 'label' columns for training/validation.\n",
    "        transform: torchvision transforms to apply to images.\n",
    "        img_size: Target size to resize images to (img_size x img_size). If using patches, this is ignored.\n",
    "        use_mask: Whether to use existing masks for tissue extraction.\n",
    "        use_patches: Whether to load images as patches.\n",
    "        patch_size: Size of each patch if using patches.\n",
    "        num_patches: Number of patches to extract per image.\n",
    "        patch_strategy: Strategy for patch extraction ('random' or 'grid').\n",
    "        min_tissue_ratio: Minimum tissue ratio for valid patches.\n",
    "        use_stain_norm: Whether to apply stain normalization.\n",
    "        is_test: Whether the dataset is for testing (no labels).\n",
    "        label_encoder: Optional LabelEncoder for encoding labels.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        data_dir: str,\n",
    "        labels_df: Optional[pd.DataFrame] = None,\n",
    "        transform: Optional[transforms.Compose] = None,\n",
    "        use_mask: bool = True,\n",
    "        use_patches: bool = False,\n",
    "        patch_size: int = 224,\n",
    "        num_patches: int = 8,\n",
    "        patch_strategy: str = \"random\",\n",
    "        min_tissue_ratio: float = 0.05,\n",
    "        use_stain_norm: bool = True,\n",
    "        is_test: bool = False,\n",
    "        label_encoder: Optional[LabelEncoder] = None,\n",
    "    ):\n",
    "        self.data_dir = Path(data_dir)\n",
    "        self.transform = transform\n",
    "        self.use_mask = use_mask\n",
    "        self.use_patches = use_patches\n",
    "        self.patch_size = patch_size\n",
    "        self.num_patches = num_patches\n",
    "        self.patch_strategy = patch_strategy\n",
    "        self.use_stain_norm = use_stain_norm\n",
    "        self.is_test = is_test\n",
    "        self.label_encoder = label_encoder\n",
    "\n",
    "        # Initialize helpers\n",
    "        self.tissue_extractor = TissueExtractor(\n",
    "            patch_size=patch_size,\n",
    "            min_tissue_ratio=min_tissue_ratio,\n",
    "        )\n",
    "        self.stain_normalizer = None if use_stain_norm else None\n",
    "\n",
    "        if is_test:\n",
    "            self.samples = self._get_test_samples()\n",
    "            self.labels = None\n",
    "            self.encoded_labels = None\n",
    "        else:\n",
    "            if labels_df is None:\n",
    "                raise ValueError(\"labels_df must be provided for training/validation.\")\n",
    "\n",
    "            self.samples = [\n",
    "                self._clean_sample_idx(str(idx))\n",
    "                for idx in labels_df[\"sample_index\"].tolist()\n",
    "            ]\n",
    "            self.labels = labels_df[\"label\"].tolist()\n",
    "\n",
    "            if self.label_encoder is None:\n",
    "                self.label_encoder = LabelEncoder()\n",
    "                self.label_encoder.fit(\n",
    "                    [\"Luminal A\", \"Luminal B\", \"HER2(+)\", \"Triple negative\"]\n",
    "                )\n",
    "            self.encoded_labels = self.label_encoder.transform(self.labels)\n",
    "\n",
    "    def _clean_sample_idx(self, sample_idx: str) -> str:\n",
    "        \"\"\"Clean sample index by removing prefix and suffix.\"\"\"\n",
    "        sample_idx = str(sample_idx)\n",
    "        if sample_idx.startswith(\"img_\"):\n",
    "            sample_idx = sample_idx[4:]\n",
    "        if sample_idx.endswith(\".png\"):\n",
    "            sample_idx = sample_idx[:-4]\n",
    "        return sample_idx\n",
    "\n",
    "    def _get_test_samples(self) -> List[str]:\n",
    "        \"\"\"Get list of sample indices from test directory.\"\"\"\n",
    "        samples = []\n",
    "        for f in sorted(self.data_dir.glob(\"img_*.png\")):\n",
    "            sample_idx = self._clean_sample_idx(f.stem)\n",
    "            samples.append(sample_idx)\n",
    "        return samples\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return len(self.samples)\n",
    "\n",
    "    def _load_image_and_mask(\n",
    "        self, sample_idx: str\n",
    "    ) -> Tuple[np.ndarray, Optional[np.ndarray]]:\n",
    "        \"\"\"Load image and optionally its mask.\"\"\"\n",
    "        img_path = self.data_dir / f\"img_{sample_idx}.png\"\n",
    "        img = np.array(Image.open(img_path).convert(\"RGB\"))\n",
    "\n",
    "        if self.use_mask:\n",
    "            mask_path = self.data_dir / f\"mask_{sample_idx}.png\"\n",
    "            if mask_path.exists():\n",
    "                mask = np.array(Image.open(mask_path).convert(\"L\"))\n",
    "\n",
    "        return img, mask\n",
    "\n",
    "    def _crop_to_tissue_bbox(self, img: np.ndarray, mask: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"Crop image to bounding box of tissue region.\"\"\"\n",
    "        # Find bounding box of tissue\n",
    "        rows = np.any(mask > 0, axis=1)\n",
    "        cols = np.any(mask > 0, axis=0)\n",
    "\n",
    "        if not rows.any() or not cols.any():\n",
    "            return img  # No tissue found, return original\n",
    "\n",
    "        y_min, y_max = np.where(rows)[0][[0, -1]]\n",
    "        x_min, x_max = np.where(cols)[0][[0, -1]]\n",
    "\n",
    "        # Add small padding\n",
    "        padding = 10\n",
    "        y_min = max(0, y_min - padding)\n",
    "        y_max = min(img.shape[0], y_max + padding)\n",
    "        x_min = max(0, x_min - padding)\n",
    "        x_max = min(img.shape[1], x_max + padding)\n",
    "\n",
    "        return img[y_min:y_max, x_min:x_max]\n",
    "\n",
    "    def _apply_stain_normalization(self, img: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"Apply stain normalization to image.\"\"\"\n",
    "        if self.use_stain_norm and self.stain_normalizer is not None:\n",
    "            try:\n",
    "                return self.stain_normalizer.normalize(img)\n",
    "            except Exception:\n",
    "                pass  # Skip normalization if it fails\n",
    "        return img\n",
    "\n",
    "    def _load_and_preprocess(self, sample_idx: str) -> np.ndarray:\n",
    "        \"\"\"Load and preprocess full image with optional tissue cropping.\"\"\"\n",
    "        img, mask = self._load_image_and_mask(sample_idx)\n",
    "\n",
    "        # Crop to tissue region if mask is available\n",
    "        if mask is not None:\n",
    "            img = self._crop_to_tissue_bbox(img, mask)\n",
    "\n",
    "        # Stain normalization\n",
    "        if self.use_stain_norm:\n",
    "            img = self._apply_stain_normalization(img)\n",
    "\n",
    "        return img\n",
    "\n",
    "    def _load_patches(self, sample_idx: str) -> List[np.ndarray]:\n",
    "        \"\"\"Load image as patches using TissueExtractor.\"\"\"\n",
    "        img, mask = self._load_image_and_mask(sample_idx)\n",
    "\n",
    "        if mask is None:\n",
    "            # If no mask, create a simple one (all tissue)\n",
    "            mask = np.ones(img.shape[:2], dtype=np.uint8) * 255\n",
    "\n",
    "        # Extract patches using TissueExtractor\n",
    "        patches, _ = self.tissue_extractor.get_valid_patches(\n",
    "            img=img,\n",
    "            mask=mask,\n",
    "            num_patches=self.num_patches,\n",
    "            strategy=self.patch_strategy,\n",
    "            stride=self.patch_size // 2,\n",
    "            shuffle=False,\n",
    "        )\n",
    "\n",
    "        # Handle case where fewer patches are found than requested\n",
    "        if len(patches) == 0:\n",
    "            # Fallback: extract center crop\n",
    "            h, w = img.shape[:2]\n",
    "            cy, cx = h // 2, w // 2\n",
    "            half = self.patch_size // 2\n",
    "            y1 = max(0, cy - half)\n",
    "            x1 = max(0, cx - half)\n",
    "            y2 = min(h, y1 + self.patch_size)\n",
    "            x2 = min(w, x1 + self.patch_size)\n",
    "            fallback_patch = img[y1:y2, x1:x2]\n",
    "            fallback_patch = cv2.resize(\n",
    "                fallback_patch, (self.patch_size, self.patch_size)\n",
    "            )\n",
    "            patches = [fallback_patch.copy() for _ in range(self.num_patches)]\n",
    "\n",
    "        elif len(patches) < self.num_patches:\n",
    "            num_missing = self.num_patches - len(patches)\n",
    "            indices = [random.randint(0, len(patches) - 1) for _ in range(num_missing)]\n",
    "            for idx in indices:\n",
    "                patch = patches[idx].copy()\n",
    "                if random.random() > 0.5:\n",
    "                    patch = cv2.flip(patch, 1)\n",
    "\n",
    "                patches.append(patch)\n",
    "\n",
    "        # Apply stain normalization to each patch\n",
    "        normalized_patches = []\n",
    "        for patch in patches:\n",
    "            patch = self._apply_stain_normalization(patch)\n",
    "            normalized_patches.append(patch)\n",
    "\n",
    "        return normalized_patches\n",
    "\n",
    "    def __getitem__(self, idx: int) -> Tuple[torch.Tensor, ...]:\n",
    "        sample_idx = self.samples[idx]\n",
    "\n",
    "        if self.use_patches:\n",
    "            patches = self._load_patches(sample_idx)\n",
    "\n",
    "            # Transform each patch\n",
    "            transformed_patches = []\n",
    "            for patch in patches:\n",
    "                patch_pil = Image.fromarray(patch)\n",
    "                if self.transform:\n",
    "                    patch_tensor = self.transform(patch_pil)\n",
    "                else:\n",
    "                    patch_tensor = transforms.ToTensor()(patch_pil)\n",
    "                transformed_patches.append(patch_tensor)\n",
    "\n",
    "            # Stack patches [num_patches, C, H, W]\n",
    "            img_tensor = torch.stack(transformed_patches)\n",
    "        else:\n",
    "            img = self._load_and_preprocess(sample_idx)\n",
    "            img_pil = Image.fromarray(img)\n",
    "\n",
    "            if self.transform:\n",
    "                img_tensor = self.transform(img_pil)\n",
    "            else:\n",
    "                img_tensor = transforms.ToTensor()(img_pil)\n",
    "\n",
    "        if self.is_test:\n",
    "            return img_tensor, sample_idx\n",
    "        else:\n",
    "            label = self.encoded_labels[idx]\n",
    "            return img_tensor, torch.tensor(label, dtype=torch.long)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14",
   "metadata": {},
   "source": [
    "## Custom data module\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PathologyDataModule(L.LightningDataModule):\n",
    "    \"\"\"Lightning DataModule for histopathology image classification.\n",
    "\n",
    "    Args:\n",
    "        train_data_dir: Directory containing training images and masks.\n",
    "        test_data_dir: Directory containing test images and masks.\n",
    "        train_labels_path: Path to CSV file with training labels.\n",
    "        batch_size: Batch size for dataloaders.\n",
    "        num_workers: Number of workers for dataloaders.\n",
    "        img_size: Target image size (used when not using patches).\n",
    "        use_mask: Whether to use masks for tissue extraction.\n",
    "        use_patches: Whether to use patch-based loading.\n",
    "        patch_size: Size of patches to extract.\n",
    "        num_patches: Number of patches per image.\n",
    "        min_tissue_ratio: Minimum tissue ratio for valid patches.\n",
    "        use_stain_norm: Whether to apply stain normalization.\n",
    "        val_split: Fraction of training data to use for validation.\n",
    "        random_seed: Random seed for reproducibility.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        train_data_dir: str = Config.TRAIN_DATA_DIR,\n",
    "        test_data_dir: str = Config.TEST_DATA_DIR,\n",
    "        train_labels_path: str = Config.TRAIN_LABELS_PATH,\n",
    "        trash_list_path: str = \"data/trash_list.txt\",\n",
    "        batch_size: int = Config.BATCH_SIZE,\n",
    "        num_workers: int = Config.NUM_WORKERS,\n",
    "        img_size: int = 224,\n",
    "        use_mask: bool = True,\n",
    "        use_patches: bool = True,\n",
    "        patch_size: int = 64,\n",
    "        num_patches: int = 10,\n",
    "        min_tissue_ratio: float = Config.MIN_TISSUE_AREA,\n",
    "        use_stain_norm: bool = Config.USE_STAIN_NORMALIZATION,\n",
    "        val_split: float = Config.VAL_SPLIT,\n",
    "        random_seed: int = Config.RANDOM_SEED,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.save_hyperparameters()\n",
    "\n",
    "        self.train_data_dir = train_data_dir\n",
    "        self.test_data_dir = test_data_dir\n",
    "        self.train_labels_path = train_labels_path\n",
    "        self.trash_list_path = trash_list_path\n",
    "        self.batch_size = batch_size\n",
    "        self.num_workers = num_workers\n",
    "        self.img_size = img_size\n",
    "        self.use_mask = use_mask\n",
    "        self.use_patches = use_patches\n",
    "        self.patch_size = patch_size\n",
    "        self.num_patches = num_patches\n",
    "        self.min_tissue_ratio = min_tissue_ratio\n",
    "        self.use_stain_norm = use_stain_norm\n",
    "        self.val_split = val_split\n",
    "        self.random_seed = random_seed\n",
    "\n",
    "        # Initialize label encoder\n",
    "        self.label_encoder = LabelEncoder()\n",
    "        self.label_encoder.fit(Config.CLASSES)\n",
    "\n",
    "        # Will be set in setup()\n",
    "        self.train_df = None\n",
    "        self.val_df = None\n",
    "        self.train_dataset = None\n",
    "        self.val_dataset = None\n",
    "        self.test_dataset = None\n",
    "\n",
    "    def _get_train_transforms(self) -> transforms.Compose:\n",
    "        target_size = self.img_size\n",
    "        return transforms.Compose(\n",
    "            [\n",
    "                transforms.Resize((target_size, target_size)),\n",
    "                transforms.RandomHorizontalFlip(p=0.5),\n",
    "                transforms.RandomVerticalFlip(p=0.5),\n",
    "                transforms.RandomRotation(degrees=90),\n",
    "                transforms.ColorJitter(\n",
    "                    brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1\n",
    "                ),  # ADD\n",
    "                transforms.RandomAffine(degrees=0, translate=(0.1, 0.1)),  # ADD\n",
    "                transforms.ToTensor(),\n",
    "                transforms.Normalize(\n",
    "                    mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]\n",
    "                ),\n",
    "            ]\n",
    "        )\n",
    "\n",
    "    def _get_val_transforms(self) -> transforms.Compose:\n",
    "        \"\"\"Get transforms for validation/test (no augmentation).\"\"\"\n",
    "        target_size = self.img_size\n",
    "        return transforms.Compose(\n",
    "            [\n",
    "                transforms.Resize((target_size, target_size)),\n",
    "                transforms.ToTensor(),\n",
    "                transforms.Normalize(\n",
    "                    mean=[0.485, 0.456, 0.406],\n",
    "                    std=[0.229, 0.224, 0.225],\n",
    "                ),\n",
    "            ]\n",
    "        )\n",
    "\n",
    "    def setup(self, stage: Optional[str] = None):\n",
    "        \"\"\"Setup datasets for each stage.\"\"\"\n",
    "        if stage == \"fit\" or stage is None:\n",
    "            # Load and split training data\n",
    "            full_df = pd.read_csv(self.train_labels_path)\n",
    "\n",
    "            # --- TRASH FILTERING START (DEBUG VERSION) ---\n",
    "            trash_path = Path(self.trash_list_path)\n",
    "            if trash_path.exists():\n",
    "                print(f\"Loading trash list from {trash_path}...\")\n",
    "                with open(trash_path, \"r\") as f:\n",
    "                    trash_files = [\n",
    "                        line.strip() for line in f.readlines() if line.strip()\n",
    "                    ]\n",
    "\n",
    "                print(f\"Total lines in trash_list.txt: {len(trash_files)}\")\n",
    "\n",
    "                # Normalize trash filenames to IDs\n",
    "                trash_ids = set()\n",
    "                for t_file in trash_files:\n",
    "                    clean_id = t_file.replace(\"img_\", \"\").replace(\".png\", \"\")\n",
    "                    trash_ids.add(clean_id)\n",
    "\n",
    "                print(\n",
    "                    f\"Unique IDs in trash list (after deduplication): {len(trash_ids)}\"\n",
    "                )\n",
    "\n",
    "                # Helper to clean DataFrame IDs\n",
    "                def clean_df_id(x):\n",
    "                    return str(x).replace(\"img_\", \"\").replace(\".png\", \"\")\n",
    "\n",
    "                # Get all IDs currently in the CSV\n",
    "                csv_ids = set(full_df[\"sample_index\"].apply(clean_df_id))\n",
    "\n",
    "                # Calculate intersection and difference\n",
    "                ids_to_remove = trash_ids.intersection(csv_ids)\n",
    "                ids_not_found = trash_ids - csv_ids\n",
    "\n",
    "                print(f\"IDs from trash list FOUND in CSV: {len(ids_to_remove)}\")\n",
    "                print(f\"IDs from trash list NOT FOUND in CSV: {len(ids_not_found)}\")\n",
    "\n",
    "                if len(ids_not_found) > 0:\n",
    "                    print(f\"Example missing IDs: {list(ids_not_found)[:5]}\")\n",
    "\n",
    "                # Apply the filter\n",
    "                initial_count = len(full_df)\n",
    "                mask = full_df[\"sample_index\"].apply(clean_df_id).isin(trash_ids)\n",
    "                full_df = full_df[~mask].reset_index(drop=True)\n",
    "\n",
    "                # Print the class distribution before and after\n",
    "                print(\"Class distribution AFTER filtering:\")\n",
    "                print(full_df[\"label\"].value_counts())\n",
    "\n",
    "                dropped_count = initial_count - len(full_df)\n",
    "                print(f\"Final check: Removed {dropped_count} rows from dataframe.\")\n",
    "                print(f\"Remaining samples: {len(full_df)}\")\n",
    "            else:\n",
    "                print(\"No trash_list.txt found, skipping filtering.\")\n",
    "            # --- TRASH FILTERING END ---\n",
    "\n",
    "            self.train_df, self.val_df = train_test_split(\n",
    "                full_df,\n",
    "                test_size=self.val_split,\n",
    "                random_state=self.random_seed,\n",
    "            )\n",
    "\n",
    "            # Calculate class weights for balanced sampling\n",
    "            self.class_weights, self.sample_weights = self._compute_sample_weights(\n",
    "                self.train_df\n",
    "            )\n",
    "\n",
    "            # Training dataset: random strategy with half overlap\n",
    "            self.train_dataset = PathologyDataset(\n",
    "                data_dir=self.train_data_dir,\n",
    "                labels_df=self.train_df,\n",
    "                transform=self._get_train_transforms(),\n",
    "                use_mask=self.use_mask,\n",
    "                use_patches=self.use_patches,\n",
    "                patch_size=self.patch_size,\n",
    "                num_patches=self.num_patches,\n",
    "                patch_strategy=\"random\",\n",
    "                min_tissue_ratio=self.min_tissue_ratio,\n",
    "                use_stain_norm=self.use_stain_norm,\n",
    "                is_test=False,\n",
    "                label_encoder=self.label_encoder,\n",
    "            )\n",
    "\n",
    "            # Validation dataset: grid strategy with no overlap\n",
    "            self.val_dataset = PathologyDataset(\n",
    "                data_dir=self.train_data_dir,\n",
    "                labels_df=self.val_df,\n",
    "                transform=self._get_val_transforms(),\n",
    "                use_mask=self.use_mask,\n",
    "                use_patches=self.use_patches,\n",
    "                patch_size=self.patch_size,\n",
    "                num_patches=self.num_patches,\n",
    "                patch_strategy=\"grid\",  # Grid for validation\n",
    "                min_tissue_ratio=self.min_tissue_ratio,\n",
    "                use_stain_norm=self.use_stain_norm,\n",
    "                is_test=False,\n",
    "                label_encoder=self.label_encoder,\n",
    "            )\n",
    "\n",
    "        if stage == \"test\" or stage == \"predict\" or stage is None:\n",
    "            # Test dataset: grid strategy with no overlap\n",
    "            self.test_dataset = PathologyDataset(\n",
    "                data_dir=self.test_data_dir,\n",
    "                labels_df=None,\n",
    "                transform=self._get_val_transforms(),\n",
    "                use_mask=self.use_mask,\n",
    "                use_patches=self.use_patches,\n",
    "                patch_size=self.patch_size,\n",
    "                num_patches=self.num_patches,\n",
    "                patch_strategy=\"grid\",\n",
    "                min_tissue_ratio=self.min_tissue_ratio,\n",
    "                use_stain_norm=self.use_stain_norm,\n",
    "                is_test=True,\n",
    "                label_encoder=self.label_encoder,\n",
    "            )\n",
    "\n",
    "    def _compute_sample_weights(self, df: pd.DataFrame):\n",
    "        \"\"\"Compute sample weights for balanced sampling.\"\"\"\n",
    "        # Encode labels\n",
    "        labels = self.label_encoder.transform(df[\"label\"].values)\n",
    "\n",
    "        # Count samples per class\n",
    "        class_counts = np.bincount(labels, minlength=len(Config.CLASSES))\n",
    "\n",
    "        # Compute class weights (inverse frequency)\n",
    "        class_weights = 1.0 / (class_counts + 1e-6)  # avoid division by zero\n",
    "        class_weights = (\n",
    "            class_weights / class_weights.sum() * len(Config.CLASSES)\n",
    "        )  # normalize\n",
    "\n",
    "        # Assign weight to each sample based on its class\n",
    "        sample_weights = class_weights[labels]\n",
    "\n",
    "        print(f\"Class counts: {dict(zip(Config.CLASSES, class_counts))}\")\n",
    "        print(f\"Class weights: {dict(zip(Config.CLASSES, class_weights.round(3)))}\")\n",
    "\n",
    "        return class_weights, sample_weights\n",
    "\n",
    "    def train_dataloader(self) -> DataLoader:\n",
    "        return DataLoader(\n",
    "            self.train_dataset,\n",
    "            batch_size=self.batch_size,\n",
    "            shuffle=False,\n",
    "            num_workers=self.num_workers,\n",
    "            pin_memory=True,\n",
    "            drop_last=True,\n",
    "            sampler=torch.utils.data.WeightedRandomSampler(\n",
    "                weights=self.sample_weights,\n",
    "                num_samples=len(self.sample_weights),\n",
    "                replacement=True,\n",
    "            ),\n",
    "        )\n",
    "\n",
    "    def val_dataloader(self) -> DataLoader:\n",
    "        return DataLoader(\n",
    "            self.val_dataset,\n",
    "            batch_size=self.batch_size,\n",
    "            shuffle=False,\n",
    "            num_workers=self.num_workers,\n",
    "            pin_memory=True,\n",
    "            drop_last=False,\n",
    "        )\n",
    "\n",
    "    def test_dataloader(self) -> DataLoader:\n",
    "        return DataLoader(\n",
    "            self.test_dataset,\n",
    "            batch_size=self.batch_size,\n",
    "            shuffle=False,\n",
    "            num_workers=self.num_workers,\n",
    "            pin_memory=True,\n",
    "            drop_last=False,\n",
    "        )\n",
    "\n",
    "    def predict_dataloader(self) -> DataLoader:\n",
    "        return self.test_dataloader()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16",
   "metadata": {},
   "source": [
    "## Define neural network model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17",
   "metadata": {},
   "source": [
    "### Attention\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Optional, Tuple\n",
    "\n",
    "import lightning as L\n",
    "import torch\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# Attention Modules\n",
    "# =============================================================================\n",
    "class SimpleAttention(nn.Module):\n",
    "    \"\"\"Simple attention mechanism with proper softmax.\"\"\"\n",
    "\n",
    "    def __init__(self, feature_dim: int, hidden_dim: int = 256, dropout: float = 0.2):\n",
    "        super().__init__()\n",
    "        self.attention = nn.Sequential(\n",
    "            nn.Linear(feature_dim, hidden_dim),\n",
    "            nn.Tanh(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(hidden_dim, 1),\n",
    "        )\n",
    "\n",
    "    def forward(self, features):\n",
    "        # features: [B, num_patches, feature_dim]\n",
    "        attention_scores = self.attention(features)  # [B, num_patches, 1]\n",
    "        attention_weights = F.softmax(attention_scores, dim=1)\n",
    "        aggregated = torch.sum(attention_weights * features, dim=1)  # [B, feature_dim]\n",
    "        return aggregated\n",
    "\n",
    "\n",
    "class GatedAttention(nn.Module):\n",
    "    \"\"\"\n",
    "    Gated Attention Mechanism (Ilse et al. 2018)\n",
    "    Paper: https://arxiv.org/abs/1802.04712\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, feature_dim: int, hidden_dim: int = 256, dropout: float = 0.25):\n",
    "        super().__init__()\n",
    "        self.attention_V = nn.Sequential(\n",
    "            nn.Linear(feature_dim, hidden_dim),\n",
    "            nn.Tanh(),\n",
    "            nn.Dropout(dropout),\n",
    "        )\n",
    "        self.attention_U = nn.Sequential(\n",
    "            nn.Linear(feature_dim, hidden_dim),\n",
    "            nn.Sigmoid(),\n",
    "            nn.Dropout(dropout),\n",
    "        )\n",
    "        self.attention_w = nn.Linear(hidden_dim, 1)\n",
    "\n",
    "    def forward(self, features):\n",
    "        # features: [B, num_patches, feature_dim]\n",
    "        A_V = self.attention_V(features)  # [B, num_patches, hidden_dim]\n",
    "        A_U = self.attention_U(features)  # [B, num_patches, hidden_dim]\n",
    "        attention_scores = self.attention_w(A_V * A_U)  # [B, num_patches, 1]\n",
    "        attention_weights = F.softmax(attention_scores, dim=1)\n",
    "        aggregated = torch.sum(attention_weights * features, dim=1)  # [B, feature_dim]\n",
    "        return aggregated\n",
    "\n",
    "\n",
    "class CLAMAttention(nn.Module):\n",
    "    \"\"\"\n",
    "    CLAM-style Attention (simplified, no unused parameters)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        feature_dim: int,\n",
    "        hidden_dim: int = 256,\n",
    "        dropout: float = 0.25,\n",
    "        num_classes: int = 4,  # kept for API compatibility but not used\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        # Gated attention network\n",
    "        self.attention_a = nn.Sequential(\n",
    "            nn.Linear(feature_dim, hidden_dim),\n",
    "            nn.Tanh(),\n",
    "            nn.Dropout(dropout),\n",
    "        )\n",
    "        self.attention_b = nn.Sequential(\n",
    "            nn.Linear(feature_dim, hidden_dim),\n",
    "            nn.Sigmoid(),\n",
    "            nn.Dropout(dropout),\n",
    "        )\n",
    "        self.attention_c = nn.Linear(hidden_dim, 1)\n",
    "\n",
    "    def forward(self, features, return_attention=False):\n",
    "        # features: [B, num_patches, feature_dim]\n",
    "\n",
    "        # Gated attention\n",
    "        a = self.attention_a(features)\n",
    "        b = self.attention_b(features)\n",
    "        attention_scores = self.attention_c(a * b)  # [B, num_patches, 1]\n",
    "        attention_weights = F.softmax(attention_scores, dim=1)\n",
    "\n",
    "        # Weighted aggregation\n",
    "        aggregated = torch.sum(attention_weights * features, dim=1)  # [B, feature_dim]\n",
    "\n",
    "        if return_attention:\n",
    "            return aggregated, attention_weights.squeeze(-1)\n",
    "        return aggregated\n",
    "\n",
    "\n",
    "class TransMIL(nn.Module):\n",
    "    \"\"\"\n",
    "    Transformer-based Multiple Instance Learning (Shao et al. 2021)\n",
    "    Paper: https://arxiv.org/abs/2106.00908\n",
    "\n",
    "    Uses transformer encoder with learnable class token for aggregation.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        feature_dim: int,\n",
    "        num_heads: int = 8,\n",
    "        num_layers: int = 2,\n",
    "        dropout: float = 0.1,\n",
    "        max_patches: int = 512,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.feature_dim = feature_dim\n",
    "\n",
    "        # Input projection (in case feature_dim is not divisible by num_heads)\n",
    "        self.input_proj = nn.Linear(feature_dim, feature_dim)\n",
    "\n",
    "        # Learnable positional embedding\n",
    "        self.pos_embedding = nn.Parameter(\n",
    "            torch.randn(1, max_patches + 1, feature_dim) * 0.02\n",
    "        )\n",
    "\n",
    "        # Learnable class token\n",
    "        self.cls_token = nn.Parameter(torch.randn(1, 1, feature_dim) * 0.02)\n",
    "\n",
    "        # Transformer encoder\n",
    "        encoder_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=feature_dim,\n",
    "            nhead=num_heads,\n",
    "            dim_feedforward=feature_dim * 4,\n",
    "            dropout=dropout,\n",
    "            activation=\"gelu\",\n",
    "            batch_first=True,\n",
    "            norm_first=True,  # Pre-norm for better training stability\n",
    "        )\n",
    "        self.transformer = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)\n",
    "\n",
    "        # Final layer norm\n",
    "        self.norm = nn.LayerNorm(feature_dim)\n",
    "\n",
    "    def forward(self, features):\n",
    "        # features: [B, num_patches, feature_dim]\n",
    "        B, N, D = features.shape\n",
    "\n",
    "        # Project input\n",
    "        x = self.input_proj(features)\n",
    "\n",
    "        # Add class token\n",
    "        cls_tokens = self.cls_token.expand(B, -1, -1)\n",
    "        x = torch.cat([cls_tokens, x], dim=1)  # [B, 1+N, D]\n",
    "\n",
    "        # Add positional embedding\n",
    "        x = x + self.pos_embedding[:, : N + 1, :]\n",
    "\n",
    "        # Transformer\n",
    "        x = self.transformer(x)\n",
    "        x = self.norm(x)\n",
    "\n",
    "        # Return class token as bag representation\n",
    "        return x[:, 0]  # [B, feature_dim]\n",
    "\n",
    "\n",
    "class MultiHeadAttentionMIL(nn.Module):\n",
    "    \"\"\"\n",
    "    Multi-Head Self-Attention for MIL with learnable class token.\n",
    "    Simpler than TransMIL but still effective.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, feature_dim: int, num_heads: int = 8, dropout: float = 0.1):\n",
    "        super().__init__()\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = feature_dim // num_heads\n",
    "        self.scale = self.head_dim**-0.5\n",
    "\n",
    "        self.qkv = nn.Linear(feature_dim, feature_dim * 3)\n",
    "        self.attn_drop = nn.Dropout(dropout)\n",
    "        self.proj = nn.Linear(feature_dim, feature_dim)\n",
    "        self.proj_drop = nn.Dropout(dropout)\n",
    "\n",
    "        # Learnable class token\n",
    "        self.cls_token = nn.Parameter(torch.randn(1, 1, feature_dim) * 0.02)\n",
    "\n",
    "        # Layer norm\n",
    "        self.norm1 = nn.LayerNorm(feature_dim)\n",
    "        self.norm2 = nn.LayerNorm(feature_dim)\n",
    "\n",
    "        # FFN\n",
    "        self.ffn = nn.Sequential(\n",
    "            nn.Linear(feature_dim, feature_dim * 4),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(feature_dim * 4, feature_dim),\n",
    "            nn.Dropout(dropout),\n",
    "        )\n",
    "\n",
    "    def forward(self, features):\n",
    "        # features: [B, num_patches, feature_dim]\n",
    "        B, N, D = features.shape\n",
    "\n",
    "        # Add class token\n",
    "        cls_tokens = self.cls_token.expand(B, -1, -1)\n",
    "        x = torch.cat([cls_tokens, features], dim=1)  # [B, 1+N, D]\n",
    "\n",
    "        # Self-attention with residual\n",
    "        x_norm = self.norm1(x)\n",
    "        qkv = (\n",
    "            self.qkv(x_norm)\n",
    "            .reshape(B, N + 1, 3, self.num_heads, self.head_dim)\n",
    "            .permute(2, 0, 3, 1, 4)\n",
    "        )\n",
    "        q, k, v = qkv[0], qkv[1], qkv[2]\n",
    "\n",
    "        attn = (q @ k.transpose(-2, -1)) * self.scale\n",
    "        attn = attn.softmax(dim=-1)\n",
    "        attn = self.attn_drop(attn)\n",
    "\n",
    "        out = (attn @ v).transpose(1, 2).reshape(B, N + 1, D)\n",
    "        out = self.proj(out)\n",
    "        out = self.proj_drop(out)\n",
    "        x = x + out\n",
    "\n",
    "        # FFN with residual\n",
    "        x = x + self.ffn(self.norm2(x))\n",
    "\n",
    "        # Return class token\n",
    "        return x[:, 0]  # [B, feature_dim]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Improved PathologyModel with State-of-the-Art Attention Mechanisms for MIL.\n",
    "\n",
    "Supported aggregation methods:\n",
    "- mean: Simple mean pooling\n",
    "- max: Max pooling\n",
    "- attention: Simple attention with softmax\n",
    "- gated_attention: Gated attention (Ilse et al. 2018)\n",
    "- clam: CLAM-style attention (Lu et al. 2021) - RECOMMENDED for pathology\n",
    "- transmil: Transformer-based MIL (Shao et al. 2021)\n",
    "\"\"\"\n",
    "# =============================================================================\n",
    "# Main Model\n",
    "# =============================================================================\n",
    "\n",
    "\n",
    "class PathologyModel(L.LightningModule):\n",
    "    \"\"\"\n",
    "    Lightning Module for histopathology image classification.\n",
    "\n",
    "    Supports both single-image and multi-instance learning (patch-based) approaches\n",
    "    with flexible backbone architectures and aggregation strategies.\n",
    "\n",
    "    Args:\n",
    "        model_name: Name of the timm model ['resnet', 'convnext_tiny', 'efficientnet_b0', etc.].\n",
    "        num_classes: Number of output classes.\n",
    "        pretrained: Whether to use ImageNet pretrained weights.\n",
    "        learning_rate: Base learning rate for optimizer.\n",
    "        weight_decay: L2 regularization weight.\n",
    "        use_patches: Whether input is patch-based [B, num_patches, C, H, W].\n",
    "        patch_aggregation: Aggregation method:\n",
    "            - 'mean': Simple mean pooling\n",
    "            - 'max': Max pooling\n",
    "            - 'attention': Simple attention with softmax\n",
    "            - 'gated_attention': Gated attention (Ilse et al. 2018)\n",
    "            - 'clam': CLAM attention (Lu et al. 2021) - RECOMMENDED\n",
    "            - 'transmil': Transformer MIL (Shao et al. 2021)\n",
    "            - 'multihead': Multi-head self-attention\n",
    "        dropout_rate: Dropout probability before classifier.\n",
    "        label_smoothing: Label smoothing factor for cross-entropy.\n",
    "        class_weights: Optional class weights for imbalanced datasets.\n",
    "        warmup_epochs: Number of warmup epochs for learning rate.\n",
    "        freeze_backbone_epochs: Number of epochs to freeze backbone.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        model_name: str = \"resnet50\",\n",
    "        num_classes: int = 4,\n",
    "        pretrained: bool = True,\n",
    "        learning_rate: float = 1e-4,\n",
    "        weight_decay: float = 1e-4,\n",
    "        use_patches: bool = True,\n",
    "        patch_aggregation: str = \"clam\",  # Changed default to clam\n",
    "        dropout_rate: float = 0.3,\n",
    "        label_smoothing: float = 0.1,\n",
    "        class_weights: Optional[torch.Tensor] = None,\n",
    "        warmup_epochs: int = 5,\n",
    "        freeze_backbone_epochs: int = 0,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.save_hyperparameters(ignore=[\"class_weights\"])\n",
    "\n",
    "        # Store hyperparameters\n",
    "        self.model_name = model_name\n",
    "        self.num_classes = num_classes\n",
    "        self.learning_rate = learning_rate\n",
    "        self.weight_decay = weight_decay\n",
    "        self.use_patches = use_patches\n",
    "        self.patch_aggregation = patch_aggregation\n",
    "        self.label_smoothing = label_smoothing\n",
    "        self.class_weights = class_weights\n",
    "        self.warmup_epochs = warmup_epochs\n",
    "        self.freeze_backbone_epochs = freeze_backbone_epochs\n",
    "\n",
    "        # Build model architecture\n",
    "        self._build_model(model_name, pretrained, dropout_rate)\n",
    "\n",
    "        # Initialize loss and metrics\n",
    "        self._setup_loss()\n",
    "        self._setup_metrics()\n",
    "\n",
    "        # Freeze backbone if requested\n",
    "        if freeze_backbone_epochs > 0:\n",
    "            self._freeze_backbone()\n",
    "\n",
    "    def _build_model(self, model_name: str, pretrained: bool, dropout_rate: float):\n",
    "        \"\"\"Build the model architecture.\"\"\"\n",
    "        # Create backbone using timm\n",
    "        self.backbone = timm.create_model(\n",
    "            model_name,\n",
    "            pretrained=pretrained,\n",
    "            num_classes=0,\n",
    "            drop_rate=0.0,\n",
    "        )\n",
    "\n",
    "        # Get feature dimension\n",
    "        self.feature_dim = self.backbone.num_features\n",
    "\n",
    "        # Build aggregation module for patches\n",
    "        if self.use_patches:\n",
    "            self.aggregation = self._build_aggregation_module()\n",
    "        else:\n",
    "            self.aggregation = None\n",
    "\n",
    "        # Build classifier head\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.LayerNorm(\n",
    "                self.feature_dim\n",
    "            ),  # LayerNorm often better than BatchNorm for MIL\n",
    "            nn.Dropout(p=dropout_rate),\n",
    "            nn.Linear(self.feature_dim, self.feature_dim // 2),\n",
    "            nn.GELU(),  # GELU often better than ReLU\n",
    "            nn.LayerNorm(self.feature_dim // 2),\n",
    "            nn.Dropout(p=dropout_rate / 2),\n",
    "            nn.Linear(self.feature_dim // 2, self.num_classes),\n",
    "        )\n",
    "\n",
    "    def _build_aggregation_module(self) -> Optional[nn.Module]:\n",
    "        \"\"\"Build patch aggregation module based on strategy.\"\"\"\n",
    "\n",
    "        if self.patch_aggregation in [\"mean\", \"max\"]:\n",
    "            return None\n",
    "\n",
    "        elif self.patch_aggregation == \"attention\":\n",
    "            return SimpleAttention(self.feature_dim)\n",
    "\n",
    "        elif self.patch_aggregation == \"gated_attention\":\n",
    "            return GatedAttention(self.feature_dim)\n",
    "\n",
    "        elif self.patch_aggregation == \"clam\":\n",
    "            return CLAMAttention(\n",
    "                self.feature_dim,\n",
    "                num_classes=self.num_classes,\n",
    "            )\n",
    "\n",
    "        elif self.patch_aggregation == \"transmil\":\n",
    "            return TransMIL(self.feature_dim)\n",
    "\n",
    "        elif self.patch_aggregation == \"multihead\":\n",
    "            return MultiHeadAttentionMIL(self.feature_dim)\n",
    "\n",
    "        else:\n",
    "            raise ValueError(\n",
    "                f\"Unknown aggregation: {self.patch_aggregation}. \"\n",
    "                f\"Choose from: mean, max, attention, gated_attention, clam, transmil, multihead\"\n",
    "            )\n",
    "\n",
    "    def _setup_loss(self):\n",
    "        \"\"\"Initialize loss function.\"\"\"\n",
    "        self.criterion = nn.CrossEntropyLoss(\n",
    "            weight=self.class_weights,\n",
    "            label_smoothing=self.label_smoothing,\n",
    "        )\n",
    "\n",
    "    def _setup_metrics(self):\n",
    "        \"\"\"Initialize metrics for each stage.\"\"\"\n",
    "        metric_kwargs = {\"task\": \"multiclass\", \"num_classes\": self.num_classes}\n",
    "\n",
    "        # Training metrics\n",
    "        self.train_acc = Accuracy(**metric_kwargs)\n",
    "        self.train_f1 = F1Score(**metric_kwargs, average=\"macro\")\n",
    "\n",
    "        # Validation metrics\n",
    "        self.val_acc = Accuracy(**metric_kwargs)\n",
    "        self.val_f1 = F1Score(**metric_kwargs, average=\"macro\")\n",
    "        self.val_auroc = AUROC(**metric_kwargs)\n",
    "\n",
    "        # Test metrics\n",
    "        self.test_acc = Accuracy(**metric_kwargs)\n",
    "        self.test_f1 = F1Score(**metric_kwargs, average=\"macro\")\n",
    "        self.test_auroc = AUROC(**metric_kwargs)\n",
    "        self.test_confmat = ConfusionMatrix(**metric_kwargs)\n",
    "\n",
    "    def _freeze_backbone(self):\n",
    "        \"\"\"Freeze backbone parameters for transfer learning.\"\"\"\n",
    "        for param in self.backbone.parameters():\n",
    "            param.requires_grad = False\n",
    "        print(f\"Backbone frozen for {self.freeze_backbone_epochs} epochs\")\n",
    "\n",
    "    def _unfreeze_backbone(self):\n",
    "        \"\"\"Unfreeze backbone parameters.\"\"\"\n",
    "        for param in self.backbone.parameters():\n",
    "            param.requires_grad = True\n",
    "        print(\"Backbone unfrozen\")\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Forward pass.\n",
    "\n",
    "        Args:\n",
    "            x: Input tensor\n",
    "               - If use_patches=False: [B, C, H, W]\n",
    "               - If use_patches=True: [B, num_patches, C, H, W]\n",
    "\n",
    "        Returns:\n",
    "            Logits of shape [B, num_classes]\n",
    "        \"\"\"\n",
    "        if self.use_patches and x.dim() == 5:\n",
    "            features = self._forward_patches(x)\n",
    "        else:\n",
    "            features = self._forward_single(x)\n",
    "\n",
    "        logits = self.classifier(features)\n",
    "        return logits\n",
    "\n",
    "    def _forward_single(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"Forward pass for single images.\"\"\"\n",
    "        return self.backbone(x)\n",
    "\n",
    "    def _forward_patches(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"Forward pass for patch-based images.\"\"\"\n",
    "        batch_size, num_patches, c, h, w = x.shape\n",
    "\n",
    "        # Reshape to process all patches\n",
    "        x = x.view(batch_size * num_patches, c, h, w)\n",
    "\n",
    "        # Extract features\n",
    "        features = self.backbone(x)  # [B * num_patches, feature_dim]\n",
    "\n",
    "        # Reshape back\n",
    "        features = features.view(batch_size, num_patches, -1)\n",
    "\n",
    "        # Aggregate patches\n",
    "        features = self._aggregate_patches(features)\n",
    "\n",
    "        return features\n",
    "\n",
    "    def _aggregate_patches(self, features: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Aggregate patch features.\n",
    "\n",
    "        Args:\n",
    "            features: [B, num_patches, feature_dim]\n",
    "\n",
    "        Returns:\n",
    "            Aggregated features [B, feature_dim]\n",
    "        \"\"\"\n",
    "        if self.patch_aggregation == \"mean\":\n",
    "            return features.mean(dim=1)\n",
    "\n",
    "        elif self.patch_aggregation == \"max\":\n",
    "            return features.max(dim=1)[0]\n",
    "\n",
    "        else:\n",
    "            # All attention-based methods handle aggregation internally\n",
    "            return self.aggregation(features)\n",
    "\n",
    "    def training_step(self, batch: Tuple, batch_idx: int) -> torch.Tensor:\n",
    "        \"\"\"Training step.\"\"\"\n",
    "        x, y = batch\n",
    "        logits = self(x)\n",
    "        loss = self.criterion(logits, y)\n",
    "\n",
    "        preds = torch.argmax(logits, dim=1)\n",
    "\n",
    "        self.train_acc(preds, y)\n",
    "        self.train_f1(preds, y)\n",
    "\n",
    "        self.log(\"train/loss\", loss, on_step=True, on_epoch=True, prog_bar=True)\n",
    "        self.log(\n",
    "            \"train/acc\", self.train_acc, on_step=False, on_epoch=True, prog_bar=True\n",
    "        )\n",
    "        self.log(\"train/f1\", self.train_f1, on_step=False, on_epoch=True)\n",
    "\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch: Tuple, batch_idx: int):\n",
    "        \"\"\"Validation step.\"\"\"\n",
    "        x, y = batch\n",
    "        logits = self(x)\n",
    "        loss = self.criterion(logits, y)\n",
    "\n",
    "        preds = torch.argmax(logits, dim=1)\n",
    "        probs = F.softmax(logits, dim=1)\n",
    "\n",
    "        self.val_acc(preds, y)\n",
    "        self.val_f1(preds, y)\n",
    "        self.val_auroc(probs, y)\n",
    "\n",
    "        self.log(\"val/loss\", loss, on_step=False, on_epoch=True, prog_bar=True)\n",
    "        self.log(\"val/acc\", self.val_acc, on_step=False, on_epoch=True, prog_bar=True)\n",
    "        self.log(\"val/f1\", self.val_f1, on_step=False, on_epoch=True, prog_bar=True)\n",
    "        self.log(\"val/auroc\", self.val_auroc, on_step=False, on_epoch=True)\n",
    "\n",
    "    def test_step(self, batch: Tuple, batch_idx: int):\n",
    "        \"\"\"Test step.\"\"\"\n",
    "        x, y = batch\n",
    "        logits = self(x)\n",
    "        loss = self.criterion(logits, y)\n",
    "\n",
    "        preds = torch.argmax(logits, dim=1)\n",
    "        probs = F.softmax(logits, dim=1)\n",
    "\n",
    "        self.test_acc(preds, y)\n",
    "        self.test_f1(preds, y)\n",
    "        self.test_auroc(probs, y)\n",
    "        self.test_confmat(preds, y)\n",
    "\n",
    "        self.log(\"test/loss\", loss)\n",
    "        self.log(\"test/acc\", self.test_acc)\n",
    "        self.log(\"test/f1\", self.test_f1)\n",
    "        self.log(\"test/auroc\", self.test_auroc)\n",
    "\n",
    "    def predict_step(self, batch: Tuple, batch_idx: int) -> Dict[str, Any]:\n",
    "        \"\"\"Prediction step.\"\"\"\n",
    "        x, sample_ids = batch\n",
    "        logits = self(x)\n",
    "\n",
    "        probs = F.softmax(logits, dim=1)\n",
    "        preds = torch.argmax(logits, dim=1)\n",
    "\n",
    "        return {\n",
    "            \"sample_ids\": sample_ids,\n",
    "            \"predictions\": preds,\n",
    "            \"probabilities\": probs,\n",
    "            \"logits\": logits,\n",
    "        }\n",
    "\n",
    "    def configure_optimizers(self) -> Dict[str, Any]:\n",
    "        \"\"\"Configure optimizer and scheduler.\"\"\"\n",
    "        # Separate parameters for differential learning rates\n",
    "        backbone_params = list(self.backbone.parameters())\n",
    "        classifier_params = list(self.classifier.parameters())\n",
    "        if self.aggregation is not None:\n",
    "            classifier_params += list(self.aggregation.parameters())\n",
    "\n",
    "        # Differential learning rates\n",
    "        param_groups = [\n",
    "            {\n",
    "                \"params\": backbone_params,\n",
    "                \"lr\": self.learning_rate * 0.1,\n",
    "                \"name\": \"backbone\",\n",
    "            },\n",
    "            {\n",
    "                \"params\": classifier_params,\n",
    "                \"lr\": self.learning_rate,\n",
    "                \"name\": \"classifier\",\n",
    "            },\n",
    "        ]\n",
    "\n",
    "        optimizer = torch.optim.AdamW(\n",
    "            param_groups,\n",
    "            weight_decay=self.weight_decay,\n",
    "        )\n",
    "\n",
    "        # Cosine annealing with warmup\n",
    "        def lr_lambda(epoch):\n",
    "            if epoch < self.warmup_epochs:\n",
    "                return (epoch + 1) / self.warmup_epochs\n",
    "            return 0.5 * (\n",
    "                1\n",
    "                + torch.cos(\n",
    "                    torch.tensor(\n",
    "                        (epoch - self.warmup_epochs)\n",
    "                        / (50 - self.warmup_epochs)\n",
    "                        * 3.14159\n",
    "                    )\n",
    "                ).item()\n",
    "            )\n",
    "\n",
    "        scheduler = torch.optim.lr_scheduler.LambdaLR(optimizer, lr_lambda=lr_lambda)\n",
    "\n",
    "        return {\n",
    "            \"optimizer\": optimizer,\n",
    "            \"lr_scheduler\": {\n",
    "                \"scheduler\": scheduler,\n",
    "                \"interval\": \"epoch\",\n",
    "                \"frequency\": 1,\n",
    "            },\n",
    "        }\n",
    "\n",
    "    def on_train_epoch_start(self):\n",
    "        \"\"\"Called at the start of each training epoch.\"\"\"\n",
    "        if (\n",
    "            self.freeze_backbone_epochs > 0\n",
    "            and self.current_epoch == self.freeze_backbone_epochs\n",
    "        ):\n",
    "            self._unfreeze_backbone()\n",
    "            self.trainer.strategy.setup_optimizers(self.trainer)\n",
    "\n",
    "    def on_test_epoch_end(self):\n",
    "        \"\"\"Log confusion matrix and per-class metrics.\"\"\"\n",
    "        confmat = self.test_confmat.compute()\n",
    "\n",
    "        print(\"\\n\" + \"=\" * 60)\n",
    "        print(\"CONFUSION MATRIX:\")\n",
    "        print(\"=\" * 60)\n",
    "        print(confmat.cpu().numpy())\n",
    "\n",
    "        print(\"\\n\" + \"=\" * 60)\n",
    "        print(\"PER-CLASS METRICS:\")\n",
    "        print(\"=\" * 60)\n",
    "\n",
    "        per_class_acc = confmat.diag() / confmat.sum(dim=1)\n",
    "        class_names = [\"Luminal A\", \"Luminal B\", \"HER2(+)\", \"Triple negative\"]\n",
    "        for i, class_name in enumerate(class_names):\n",
    "            print(f\"{class_name:20s}: Accuracy = {per_class_acc[i]:.4f}\")\n",
    "\n",
    "        print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20",
   "metadata": {},
   "source": [
    "## Train logic\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21",
   "metadata": {},
   "outputs": [],
   "source": [
    "from lightning.pytorch.callbacks import EarlyStopping, ModelCheckpoint, RichProgressBar\n",
    "\n",
    "total_samples = 580\n",
    "class_counts = torch.tensor(\n",
    "    [158, 203, 150, 69], dtype=torch.float\n",
    ")  # Order matches Config.CLASSES\n",
    "class_weights = total_samples / (len(class_counts) * class_counts)\n",
    "\n",
    "print(f\"Calculated Class Weights: {class_weights}\")\n",
    "\n",
    "# Initialize\n",
    "datamodule = PathologyDataModule(\n",
    "    use_mask=True,\n",
    "    use_patches=True,\n",
    "    patch_size=64,\n",
    "    num_patches=10,\n",
    "    img_size=224,\n",
    "    batch_size=16,\n",
    "    min_tissue_ratio=0.01,\n",
    ")\n",
    "model = PathologyModel(\n",
    "    model_name=\"convnext_tiny\",\n",
    "    use_patches=True,\n",
    "    patch_aggregation=\"clam\",  # or 'clam', 'gated_attention', 'transmil'\n",
    "    # class_weights=class_weights,\n",
    "    learning_rate=1e-4,\n",
    "    weight_decay=1e-4,\n",
    "    dropout_rate=0.2,\n",
    "    label_smoothing=0.1,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train\n",
    "trainer = L.Trainer(\n",
    "    max_epochs=Config.MAX_EPOCHS,\n",
    "    accelerator=\"auto\",\n",
    "    callbacks=[\n",
    "        ModelCheckpoint(monitor=\"val/f1\", mode=\"max\"),\n",
    "        EarlyStopping(monitor=\"val/acc\", patience=20),\n",
    "        RichProgressBar(),\n",
    "    ],\n",
    "    accumulate_grad_batches=1,\n",
    "    # gradient_clip_val=0.5,\n",
    "    precision=\"16-mixed\",\n",
    "    log_every_n_steps=5,\n",
    "    devices=2,\n",
    ")\n",
    "trainer.fit(model, datamodule)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(trainer.checkpoint_callback.best_model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24",
   "metadata": {},
   "source": [
    "## Inference logic\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inference on test set\n",
    "# 1. Load the best model from the checkpoint\n",
    "best_checkpoint = trainer.checkpoint_callback.best_model_path\n",
    "\n",
    "print(f\"Loading model from: {best_checkpoint}\")\n",
    "best_model = PathologyModel.load_from_checkpoint(best_checkpoint)\n",
    "\n",
    "datamodule = PathologyDataModule()\n",
    "datamodule.setup(stage=\"test\")\n",
    "\n",
    "trainer = L.Trainer(\n",
    "    accelerator=\"auto\",\n",
    "    precision=\"16-mixed\",\n",
    ")\n",
    "\n",
    "# 2. Run prediction using the Trainer\n",
    "print(\"Generating predictions...\")\n",
    "predictions = trainer.predict(best_model, datamodule=datamodule)\n",
    "\n",
    "# 3. Process the results\n",
    "sample_ids = []\n",
    "pred_labels_encoded = []\n",
    "\n",
    "for batch in predictions:\n",
    "    sample_ids.extend(batch[\"sample_ids\"])\n",
    "    pred_labels_encoded.extend(batch[\"predictions\"].cpu().numpy().tolist())\n",
    "\n",
    "# 4. Decode integer labels back to string labels\n",
    "decoded_labels = datamodule.label_encoder.inverse_transform(pred_labels_encoded)\n",
    "\n",
    "# 5. Format sample_ids back to filenames (e.g., \"1004\" -> \"img_1004.png\")\n",
    "# We assume sample_ids contains the raw ID strings (e.g., \"1004\", \"1005\")\n",
    "formatted_sample_ids = [f\"img_{sid}.png\" for sid in sample_ids]\n",
    "\n",
    "# 6. Create DataFrame\n",
    "submission_df = pd.DataFrame(\n",
    "    {\"sample_index\": formatted_sample_ids, \"label\": decoded_labels}\n",
    ")\n",
    "\n",
    "# Optional: Sort by sample_index for a cleaner look\n",
    "submission_df = submission_df.sort_values(\"sample_index\").reset_index(drop=True)\n",
    "\n",
    "# 7. Save to CSV\n",
    "output_csv_path = \"submission.csv\"\n",
    "submission_df.to_csv(output_csv_path, index=False)\n",
    "\n",
    "print(\"\\n\" + \"=\" * 50)\n",
    "print(f\"Submission saved to: {output_csv_path}\")\n",
    "print(f\"Total samples predicted: {len(submission_df)}\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Preview the first few rows\n",
    "print(submission_df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26",
   "metadata": {},
   "outputs": [],
   "source": [
    "submission_df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
