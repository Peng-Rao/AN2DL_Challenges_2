{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# install necessary packages\n",
    "%pip install gdown lightning lion-pytorch wandb torchstain[torch]\n",
    "# uninstall tensorflow to avoid conflicts\n",
    "%pip uninstall -y tensorflow keras tensorboard"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1",
   "metadata": {},
   "source": [
    "## Get data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "!gdown https://drive.google.com/uc?id=1fZG_-FwADGMI_TqbPaA8z10ZMGPvb_vG\n",
    "!unzip -q data.zip\n",
    "!mv an2dl2526c2v2 data\n",
    "!ls -l data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the trash list\n",
    "!gdown https://drive.google.com/uc?id=1D8tGrxR4oHZmOxKvINNQR5x-zI6MavVP\n",
    "!mv trash_list.txt data/trash_list.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls -l"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5",
   "metadata": {},
   "source": [
    "## Import Libraries\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from pathlib import Path\n",
    "from typing import Any, Dict, List, Optional, Tuple\n",
    "\n",
    "import cv2\n",
    "import lightning as L\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import timm\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from lion_pytorch import Lion\n",
    "from PIL import Image\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchmetrics import AUROC, Accuracy, ConfusionMatrix, F1Score\n",
    "from torchvision import transforms\n",
    "\n",
    "L.seed_everything(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7",
   "metadata": {},
   "source": [
    "## Utilities\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8",
   "metadata": {},
   "source": [
    "### Config\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Config:\n",
    "    # Data paths\n",
    "    DATA_DIR = \"./data\"\n",
    "    TRAIN_DATA_DIR = \"./data/train_data\"\n",
    "    TEST_DATA_DIR = \"./data/test_data\"\n",
    "    TRAIN_LABELS_PATH = \"./data/train_labels.csv\"\n",
    "    # Class labels\n",
    "    CLASSES = [\"Luminal A\", \"Luminal B\", \"HER2(+)\", \"Triple negative\"]\n",
    "    NUM_CLASSES = 4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10",
   "metadata": {},
   "source": [
    "### Extract tissue\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "\n",
    "\n",
    "class TissueExtractor:\n",
    "    \"\"\"\n",
    "    Extract patches from images centered around cancer point annotations.\n",
    "    Designed for workflow where annotations mark cancer locations, and we want\n",
    "    to extract surrounding tissue context.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, patch_size: int = 224, min_annotation_pixels: int = 1):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            patch_size: Size of square patches to extract.\n",
    "            min_annotation_pixels: Minimum number of annotation pixels required in patch.\n",
    "        \"\"\"\n",
    "        self.patch_size = patch_size\n",
    "        self.min_annotation_pixels = min_annotation_pixels\n",
    "\n",
    "    def _validate_inputs(self, img: np.ndarray, mask: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"Validate inputs and return processed mask.\"\"\"\n",
    "        if img.shape[:2] != mask.shape[:2]:\n",
    "            raise ValueError(\n",
    "                f\"Image shape {img.shape[:2]} doesn't match mask shape {mask.shape[:2]}\"\n",
    "            )\n",
    "\n",
    "        h, w = img.shape[:2]\n",
    "        if h < self.patch_size or w < self.patch_size:\n",
    "            raise ValueError(\n",
    "                f\"Image dimensions ({h}, {w}) smaller than patch_size ({self.patch_size})\"\n",
    "            )\n",
    "\n",
    "        if len(mask.shape) == 3:\n",
    "            mask = mask[:, :, 0]\n",
    "\n",
    "        if mask.dtype in [np.float32, np.float64]:\n",
    "            warnings.warn(\"Float mask detected, thresholding at 0.5\")\n",
    "            mask = (mask > 0.5).astype(np.uint8)\n",
    "\n",
    "        return mask\n",
    "\n",
    "    def get_valid_patches(\n",
    "        self,\n",
    "        img: np.ndarray,\n",
    "        mask: np.ndarray,\n",
    "        num_patches: int = 8,\n",
    "        strategy: str = \"random\",\n",
    "        stride: Optional[int] = None,\n",
    "        shuffle: bool = True,\n",
    "        min_distance: int = None,\n",
    "    ) -> Tuple[List[np.ndarray], List[np.ndarray]]:\n",
    "        \"\"\"\n",
    "        Extract tissue patches centered around cancer annotations in mask.\n",
    "\n",
    "        Args:\n",
    "            img: RGB image (H, W, 3) - the full tissue image\n",
    "            mask: Annotation mask (H, W) - cancer point annotations (sparse)\n",
    "            num_patches: Number of patches to extract per image.\n",
    "            strategy: 'random' samples from annotation points; 'grid' finds patches containing annotations.\n",
    "            stride: Step size for grid strategy. Defaults to patch_size (no overlap).\n",
    "            shuffle: Whether to shuffle grid patches before selecting.\n",
    "            min_distance: Minimum pixel distance between patch centers (random strategy).\n",
    "\n",
    "        Returns:\n",
    "            images: List of RGB patches (surrounding tissue context)\n",
    "            masks: List of corresponding annotation patches (sparse cancer markers)\n",
    "        \"\"\"\n",
    "        mask = self._validate_inputs(img, mask)\n",
    "        h, w = img.shape[:2]\n",
    "\n",
    "        # Find all annotation pixel indices\n",
    "        annotation_indices = np.where(mask > 0)\n",
    "\n",
    "        if len(annotation_indices[0]) == 0:\n",
    "            warnings.warn(\"No annotations found in mask!\")\n",
    "            return [], []\n",
    "\n",
    "        if strategy == \"random\":\n",
    "            patches_img, patches_mask = self._extract_random(\n",
    "                img, mask, annotation_indices, num_patches, h, w, min_distance\n",
    "            )\n",
    "        elif strategy == \"grid\":\n",
    "            patches_img, patches_mask = self._extract_grid(\n",
    "                img, mask, num_patches, h, w, stride, shuffle\n",
    "            )\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown strategy: {strategy}. Use 'random' or 'grid'.\")\n",
    "\n",
    "        if len(patches_img) < num_patches:\n",
    "            warnings.warn(\n",
    "                f\"Only {len(patches_img)} patches extracted (requested {num_patches})\"\n",
    "            )\n",
    "\n",
    "        return patches_img, patches_mask\n",
    "\n",
    "    def _extract_random(\n",
    "        self,\n",
    "        img: np.ndarray,\n",
    "        mask: np.ndarray,\n",
    "        annotation_indices: Tuple[np.ndarray, np.ndarray],\n",
    "        num_patches: int,\n",
    "        h: int,\n",
    "        w: int,\n",
    "        min_distance: Optional[int] = None,\n",
    "    ) -> Tuple[List[np.ndarray], List[np.ndarray]]:\n",
    "        \"\"\"\n",
    "        Random sampling: center patches on annotation points to capture surrounding tissue.\n",
    "\n",
    "        Args:\n",
    "            img: RGB image (H, W, 3)\n",
    "            mask: Annotation mask (H, W)\n",
    "            annotation_indices: Tuple of arrays with y and x indices of annotation pixels.\n",
    "            num_patches: Number of patches to extract.\n",
    "            h: Height of the image.\n",
    "            w: Width of the image.\n",
    "            min_distance: Minimum distance between patch centers.\n",
    "        \"\"\"\n",
    "        patches_img = []\n",
    "        patches_mask = []\n",
    "        selected_centers = []\n",
    "\n",
    "        if min_distance is None:\n",
    "            min_distance = self.patch_size // 2\n",
    "\n",
    "        attempts = 0\n",
    "        max_attempts = num_patches * 100\n",
    "\n",
    "        while len(patches_img) < num_patches and attempts < max_attempts:\n",
    "            attempts += 1\n",
    "\n",
    "            # Sample a random annotation point\n",
    "            idx = np.random.randint(len(annotation_indices[0]))\n",
    "            cy, cx = annotation_indices[0][idx], annotation_indices[1][idx]\n",
    "\n",
    "            # Check minimum distance from existing patches\n",
    "            if min_distance > 0 and selected_centers:\n",
    "                too_close = False\n",
    "                for prev_cy, prev_cx in selected_centers:\n",
    "                    dist = np.sqrt((cy - prev_cy) ** 2 + (cx - prev_cx) ** 2)\n",
    "                    if dist < min_distance:\n",
    "                        too_close = True\n",
    "                        break\n",
    "                if too_close:\n",
    "                    continue\n",
    "\n",
    "            # Calculate patch bounds centered on annotation point\n",
    "            half_size = self.patch_size // 2\n",
    "            y_min = cy - half_size\n",
    "            x_min = cx - half_size\n",
    "            y_max = y_min + self.patch_size\n",
    "            x_max = x_min + self.patch_size\n",
    "\n",
    "            # Boundary check\n",
    "            if y_min < 0 or x_min < 0 or y_max > h or x_max > w:\n",
    "                continue\n",
    "\n",
    "            # Extract patches - image contains surrounding tissue, mask contains annotation\n",
    "            img_patch = img[y_min:y_max, x_min:x_max]\n",
    "            mask_patch = mask[y_min:y_max, x_min:x_max]\n",
    "\n",
    "            # Check minimum annotation pixels (at least some annotation in patch)\n",
    "            if np.count_nonzero(mask_patch) >= self.min_annotation_pixels:\n",
    "                patches_img.append(img_patch)\n",
    "                patches_mask.append(mask_patch)\n",
    "                selected_centers.append((cy, cx))\n",
    "\n",
    "        return patches_img, patches_mask\n",
    "\n",
    "    def _extract_grid(\n",
    "        self,\n",
    "        img: np.ndarray,\n",
    "        mask: np.ndarray,\n",
    "        num_patches: int,\n",
    "        h: int,\n",
    "        w: int,\n",
    "        stride: Optional[int] = None,\n",
    "        shuffle: bool = True,\n",
    "    ) -> Tuple[List[np.ndarray], List[np.ndarray]]:\n",
    "        \"\"\"\n",
    "        Grid strategy: find patches that contain annotation points.\n",
    "        Prioritizes patches with more annotation pixels.\n",
    "\n",
    "        Args:\n",
    "            img: RGB image (H, W, 3)\n",
    "            mask: Annotation mask (H, W)\n",
    "            num_patches: Number of patches to extract.\n",
    "            h: Height of the image.\n",
    "            w: Width of the image.\n",
    "            stride: Step size for grid sampling. Defaults to patch_size (no overlap).\n",
    "            shuffle: Whether to shuffle valid patches before selection.\n",
    "        \"\"\"\n",
    "        if stride is None:\n",
    "            stride = self.patch_size\n",
    "\n",
    "        if stride <= 0:\n",
    "            raise ValueError(f\"Stride must be positive, got {stride}\")\n",
    "\n",
    "        # Find annotation bounding box to focus search\n",
    "        annotation_rows = np.any(mask > 0, axis=1)\n",
    "        annotation_cols = np.any(mask > 0, axis=0)\n",
    "\n",
    "        if not annotation_rows.any() or not annotation_cols.any():\n",
    "            return [], []\n",
    "\n",
    "        y_min_ann, y_max_ann = np.where(annotation_rows)[0][[0, -1]]\n",
    "        x_min_ann, x_max_ann = np.where(annotation_cols)[0][[0, -1]]\n",
    "\n",
    "        # Expand search region to capture surrounding tissue\n",
    "        padding = self.patch_size\n",
    "        y_start = max(0, y_min_ann - padding)\n",
    "        y_end = min(h, y_max_ann + padding)\n",
    "        x_start = max(0, x_min_ann - padding)\n",
    "        x_end = min(w, x_max_ann + padding)\n",
    "\n",
    "        y_positions = list(range(y_start, y_end - self.patch_size + 1, stride))\n",
    "        x_positions = list(range(x_start, x_end - self.patch_size + 1, stride))\n",
    "\n",
    "        valid_patches = []\n",
    "\n",
    "        for y_min in y_positions:\n",
    "            for x_min in x_positions:\n",
    "                y_max = y_min + self.patch_size\n",
    "                x_max = x_min + self.patch_size\n",
    "\n",
    "                mask_patch = mask[y_min:y_max, x_min:x_max]\n",
    "                annotation_count = np.count_nonzero(mask_patch)\n",
    "\n",
    "                # Only include patches that contain annotations\n",
    "                if annotation_count >= self.min_annotation_pixels:\n",
    "                    valid_patches.append((y_min, x_min, annotation_count))\n",
    "\n",
    "        if shuffle:\n",
    "            np.random.shuffle(valid_patches)\n",
    "        else:\n",
    "            # Sort by annotation count (highest first) for deterministic selection\n",
    "            valid_patches.sort(key=lambda x: x[2], reverse=True)\n",
    "\n",
    "        patches_img = []\n",
    "        patches_mask = []\n",
    "\n",
    "        for y_min, x_min, _ in valid_patches[:num_patches]:\n",
    "            y_max = y_min + self.patch_size\n",
    "            x_max = x_min + self.patch_size\n",
    "\n",
    "            patches_img.append(img[y_min:y_max, x_min:x_max])\n",
    "            patches_mask.append(mask[y_min:y_max, x_min:x_max])\n",
    "\n",
    "        return patches_img, patches_mask\n",
    "\n",
    "    def get_all_valid_patches(\n",
    "        self,\n",
    "        img: np.ndarray,\n",
    "        mask: np.ndarray,\n",
    "        stride: Optional[int] = None,\n",
    "    ) -> Tuple[List[np.ndarray], List[np.ndarray], List[Tuple[int, int]]]:\n",
    "        \"\"\"Extract ALL patches containing annotations from the image.\"\"\"\n",
    "\n",
    "        mask = self._validate_inputs(img, mask)\n",
    "        h, w = img.shape[:2]\n",
    "\n",
    "        if stride is None:\n",
    "            stride = self.patch_size\n",
    "\n",
    "        # Focus on region around annotations\n",
    "        annotation_rows = np.any(mask > 0, axis=1)\n",
    "        annotation_cols = np.any(mask > 0, axis=0)\n",
    "\n",
    "        if not annotation_rows.any() or not annotation_cols.any():\n",
    "            return [], [], []\n",
    "\n",
    "        y_min_ann, y_max_ann = np.where(annotation_rows)[0][[0, -1]]\n",
    "        x_min_ann, x_max_ann = np.where(annotation_cols)[0][[0, -1]]\n",
    "\n",
    "        padding = self.patch_size\n",
    "        y_start = max(0, y_min_ann - padding)\n",
    "        y_end = min(h, y_max_ann + padding)\n",
    "        x_start = max(0, x_min_ann - padding)\n",
    "        x_end = min(w, x_max_ann + padding)\n",
    "\n",
    "        y_positions = list(range(y_start, y_end - self.patch_size + 1, stride))\n",
    "        x_positions = list(range(x_start, x_end - self.patch_size + 1, stride))\n",
    "\n",
    "        patches_img = []\n",
    "        patches_mask = []\n",
    "        coordinates = []\n",
    "\n",
    "        for y_min in y_positions:\n",
    "            for x_min in x_positions:\n",
    "                y_max = y_min + self.patch_size\n",
    "                x_max = x_min + self.patch_size\n",
    "\n",
    "                mask_patch = mask[y_min:y_max, x_min:x_max]\n",
    "                annotation_count = np.count_nonzero(mask_patch)\n",
    "\n",
    "                if annotation_count >= self.min_annotation_pixels:\n",
    "                    patches_img.append(img[y_min:y_max, x_min:x_max])\n",
    "                    patches_mask.append(mask_patch)\n",
    "                    coordinates.append((y_min, x_min))\n",
    "\n",
    "        return patches_img, patches_mask, coordinates"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12",
   "metadata": {},
   "source": [
    "## Custom dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PathologyDataset(Dataset):\n",
    "    \"\"\"Dataset optimized for histopathology images.\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        data_dir: str,\n",
    "        labels_df: Optional[pd.DataFrame] = None,\n",
    "        transform: Optional[transforms.Compose] = None,\n",
    "        use_mask: bool = True,\n",
    "        use_patches: bool = False,\n",
    "        patch_size: int = 224,\n",
    "        num_patches: int = 8,\n",
    "        patch_strategy: str = \"random\",\n",
    "        min_annotation_pixels: int = 100,\n",
    "        is_test: bool = False,\n",
    "        label_encoder: Optional[LabelEncoder] = None,\n",
    "        use_dual_stream: bool = False,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            data_dir: Directory with images and masks.\n",
    "            labels_df: DataFrame with 'sample_index' and 'label' columns (None for test).\n",
    "            transform: torchvision transforms to apply to images.\n",
    "            use_mask: Whether to load and use masks.\n",
    "            use_patches: Whether to extract patches or use full images.\n",
    "            patch_size: Size of square patches to extract.\n",
    "            num_patches: Number of patches to extract per image.\n",
    "            patch_strategy: 'random' or 'grid' strategy for patch extraction.\n",
    "            min_annotation_pixels: Minimum annotation pixels required in patch.\n",
    "            is_test: Whether the dataset is for testing (no labels).\n",
    "            label_encoder: Pre-fitted LabelEncoder (if None, will fit on training labels).\n",
    "            use_dual_stream: Whether to return masks alongside images.\n",
    "        \"\"\"\n",
    "        self.data_dir = Path(data_dir)\n",
    "        self.transform = transform\n",
    "        self.use_mask = use_mask\n",
    "        self.use_patches = use_patches\n",
    "        self.patch_size = patch_size\n",
    "        self.num_patches = num_patches\n",
    "        self.patch_strategy = patch_strategy\n",
    "        self.min_annotation_pixels = min_annotation_pixels\n",
    "        self.is_test = is_test\n",
    "        self.label_encoder = label_encoder\n",
    "        self.use_dual_stream = use_dual_stream\n",
    "\n",
    "        # Initialize helpers\n",
    "        self.tissue_extractor = TissueExtractor(\n",
    "            patch_size=patch_size,\n",
    "            min_annotation_pixels=min_annotation_pixels,\n",
    "        )\n",
    "\n",
    "        if is_test:\n",
    "            self.samples = self._get_test_samples()\n",
    "            self.labels = None\n",
    "            self.encoded_labels = None\n",
    "        else:\n",
    "            if labels_df is None:\n",
    "                raise ValueError(\"labels_df must be provided for training/validation.\")\n",
    "\n",
    "            self.samples = [\n",
    "                self._clean_sample_idx(str(idx))\n",
    "                for idx in labels_df[\"sample_index\"].tolist()\n",
    "            ]\n",
    "            self.labels = labels_df[\"label\"].tolist()\n",
    "\n",
    "            if self.label_encoder is None:\n",
    "                self.label_encoder = LabelEncoder()\n",
    "                self.label_encoder.fit(\n",
    "                    [\"Luminal A\", \"Luminal B\", \"HER2(+)\", \"Triple negative\"]\n",
    "                )\n",
    "            self.encoded_labels = self.label_encoder.transform(self.labels)\n",
    "\n",
    "    def _clean_sample_idx(self, sample_idx: str) -> str:\n",
    "        sample_idx = str(sample_idx)\n",
    "        if sample_idx.startswith(\"img_\"):\n",
    "            sample_idx = sample_idx[4:]\n",
    "        if sample_idx.endswith(\".png\"):\n",
    "            sample_idx = sample_idx[:-4]\n",
    "        return sample_idx\n",
    "\n",
    "    def _get_test_samples(self) -> List[str]:\n",
    "        samples = []\n",
    "        for f in sorted(self.data_dir.glob(\"img_*.png\")):\n",
    "            sample_idx = self._clean_sample_idx(f.stem)\n",
    "            samples.append(sample_idx)\n",
    "        return samples\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return len(self.samples)\n",
    "\n",
    "    def _load_image_and_mask(\n",
    "        self, sample_idx: str\n",
    "    ) -> Tuple[np.ndarray, Optional[np.ndarray]]:\n",
    "        img_path = self.data_dir / f\"img_{sample_idx}.png\"\n",
    "        img = np.array(Image.open(img_path).convert(\"RGB\"))\n",
    "\n",
    "        mask = None\n",
    "        if self.use_mask:\n",
    "            mask_path = self.data_dir / f\"mask_{sample_idx}.png\"\n",
    "            if mask_path.exists():\n",
    "                mask = np.array(Image.open(mask_path).convert(\"L\"))\n",
    "\n",
    "        return img, mask\n",
    "\n",
    "    def _crop_to_tissue_bbox(self, img: np.ndarray, mask: np.ndarray) -> np.ndarray:\n",
    "        rows = np.any(mask > 0, axis=1)\n",
    "        cols = np.any(mask > 0, axis=0)\n",
    "        if not rows.any() or not cols.any():\n",
    "            return img\n",
    "        y_min, y_max = np.where(rows)[0][[0, -1]]\n",
    "        x_min, x_max = np.where(cols)[0][[0, -1]]\n",
    "        padding = 10\n",
    "        y_min = max(0, y_min - padding)\n",
    "        y_max = min(img.shape[0], y_max + padding)\n",
    "        x_min = max(0, x_min - padding)\n",
    "        x_max = min(img.shape[1], x_max + padding)\n",
    "        return img[y_min:y_max, x_min:x_max]\n",
    "\n",
    "    def _load_and_preprocess(self, sample_idx: str) -> np.ndarray:\n",
    "        img, mask = self._load_image_and_mask(sample_idx)\n",
    "        if mask is not None:\n",
    "            img = self._crop_to_tissue_bbox(img, mask)\n",
    "        return img\n",
    "\n",
    "    def _load_patches(\n",
    "        self, sample_idx: str\n",
    "    ) -> Tuple[List[np.ndarray], List[np.ndarray]]:\n",
    "        \"\"\"\n",
    "        Load image patches AND mask patches.\n",
    "        Ensures both lists stay synchronized during padding/augmentation.\n",
    "        \"\"\"\n",
    "        img, mask = self._load_image_and_mask(sample_idx)\n",
    "\n",
    "        if mask is None:\n",
    "            mask = np.ones(img.shape[:2], dtype=np.uint8) * 255\n",
    "\n",
    "        # Extract patches (TissueExtractor returns both)\n",
    "        patches, mask_patches = self.tissue_extractor.get_valid_patches(\n",
    "            img=img,\n",
    "            mask=mask,\n",
    "            num_patches=self.num_patches,\n",
    "            strategy=self.patch_strategy,\n",
    "            stride=self.patch_size // 2,\n",
    "            shuffle=False,\n",
    "            min_distance=32,\n",
    "        )\n",
    "\n",
    "        if len(patches) == 0:\n",
    "            h, w = img.shape[:2]\n",
    "            cy, cx = h // 2, w // 2\n",
    "            half = self.patch_size // 2\n",
    "            y1 = max(0, cy - half)\n",
    "            x1 = max(0, cx - half)\n",
    "            y2 = min(h, y1 + self.patch_size)\n",
    "            x2 = min(w, x1 + self.patch_size)\n",
    "\n",
    "            fallback_patch = img[y1:y2, x1:x2]\n",
    "            fallback_patch = cv2.resize(\n",
    "                fallback_patch, (self.patch_size, self.patch_size)\n",
    "            )\n",
    "\n",
    "            fallback_mask = mask[y1:y2, x1:x2]\n",
    "            fallback_mask = cv2.resize(\n",
    "                fallback_mask,\n",
    "                (self.patch_size, self.patch_size),\n",
    "                interpolation=cv2.INTER_NEAREST,\n",
    "            )\n",
    "\n",
    "            patches = [fallback_patch.copy() for _ in range(self.num_patches)]\n",
    "            mask_patches = [fallback_mask.copy() for _ in range(self.num_patches)]\n",
    "\n",
    "        elif len(patches) < self.num_patches:\n",
    "            num_missing = self.num_patches - len(patches)\n",
    "            indices = [random.randint(0, len(patches) - 1) for _ in range(num_missing)]\n",
    "\n",
    "            for idx in indices:\n",
    "                patch = patches[idx].copy()\n",
    "                mask_patch = mask_patches[idx].copy()\n",
    "\n",
    "                if random.random() > 0.5:\n",
    "                    patch = cv2.flip(patch, 1)\n",
    "                    mask_patch = cv2.flip(mask_patch, 1)\n",
    "\n",
    "                patches.append(patch)\n",
    "                mask_patches.append(mask_patch)\n",
    "\n",
    "        return patches, mask_patches\n",
    "\n",
    "    def __getitem__(self, idx: int) -> Tuple[torch.Tensor, ...]:\n",
    "        sample_idx = self.samples[idx]\n",
    "\n",
    "        if self.use_patches:\n",
    "            # Load both images and masks\n",
    "            patches, mask_patches = self._load_patches(sample_idx)\n",
    "\n",
    "            # Transform Image Patches\n",
    "            transformed_patches = []\n",
    "            for patch in patches:\n",
    "                patch_pil = Image.fromarray(patch)\n",
    "                if self.transform:\n",
    "                    patch_tensor = self.transform(patch_pil)\n",
    "                else:\n",
    "                    patch_tensor = transforms.ToTensor()(patch_pil)\n",
    "                transformed_patches.append(patch_tensor)\n",
    "\n",
    "            # Stack Images: [num_patches, 3, H, W]\n",
    "            img_tensor = torch.stack(transformed_patches)\n",
    "\n",
    "            # Handle Masks for Dual Stream\n",
    "            if self.use_dual_stream:\n",
    "                transformed_masks = []\n",
    "                for mask_p in mask_patches:\n",
    "                    mask_pil = Image.fromarray(mask_p).convert(\"L\")\n",
    "                    mask_tensor = transforms.ToTensor()(mask_pil)\n",
    "                    transformed_masks.append(mask_tensor)\n",
    "                mask_tensor = torch.stack(transformed_masks)\n",
    "            else:\n",
    "                mask_tensor = None\n",
    "\n",
    "        else:\n",
    "            img, mask = self._load_image_and_mask(sample_idx)\n",
    "            img_pil = Image.fromarray(img)\n",
    "            if self.transform:\n",
    "                img_tensor = self.transform(img_pil)\n",
    "            else:\n",
    "                img_tensor = transforms.ToTensor()(img_pil)\n",
    "\n",
    "            mask_tensor = None\n",
    "            if self.use_dual_stream:\n",
    "                if mask is None:\n",
    "                    mask = np.zeros(img.shape[:2], dtype=np.uint8)\n",
    "\n",
    "                mask_pil = Image.fromarray(mask).convert(\"L\")\n",
    "\n",
    "                if self.transform:\n",
    "                    for t in self.transform.transforms:\n",
    "                        if isinstance(t, transforms.Resize):\n",
    "                            mask_pil = t(mask_pil)\n",
    "                            break\n",
    "\n",
    "                mask_tensor = transforms.ToTensor()(mask_pil)\n",
    "\n",
    "        if self.is_test:\n",
    "            if self.use_dual_stream and mask_tensor is not None:\n",
    "                return img_tensor, mask_tensor, sample_idx\n",
    "            return img_tensor, sample_idx\n",
    "        else:\n",
    "            label = self.encoded_labels[idx]\n",
    "            label_t = torch.tensor(label, dtype=torch.long)\n",
    "\n",
    "            if self.use_dual_stream and mask_tensor is not None:\n",
    "                return img_tensor, mask_tensor, label_t\n",
    "            return img_tensor, label_t"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14",
   "metadata": {},
   "source": [
    "## Custom data module\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PathologyDataModule(L.LightningDataModule):\n",
    "    \"\"\"Lightning DataModule for histopathology image classification.\n",
    "\n",
    "    Args:\n",
    "        train_data_dir: Directory containing training images and masks.\n",
    "        test_data_dir: Directory containing test images and masks.\n",
    "        train_labels_path: Path to CSV file with training labels.\n",
    "        batch_size: Batch size for dataloaders.\n",
    "        num_workers: Number of workers for dataloaders.\n",
    "        img_size: Target image size (used when not using patches).\n",
    "        use_mask: Whether to use masks for tissue extraction.\n",
    "        use_patches: Whether to use patch-based loading.\n",
    "        patch_size: Size of patches to extract.\n",
    "        num_patches: Number of patches per image.\n",
    "        min_tissue_ratio: Minimum tissue ratio for valid patches.\n",
    "        val_split: Fraction of training data to use for validation.\n",
    "        random_seed: Random seed for reproducibility.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        train_data_dir: str = Config.TRAIN_DATA_DIR,\n",
    "        test_data_dir: str = Config.TEST_DATA_DIR,\n",
    "        train_labels_path: str = Config.TRAIN_LABELS_PATH,\n",
    "        trash_list_path: str = \"data/trash_list.txt\",\n",
    "        batch_size: int = 16,\n",
    "        num_workers: int = 2,\n",
    "        img_size: int = 224,\n",
    "        use_mask: bool = True,\n",
    "        use_patches: bool = True,\n",
    "        patch_size: int = 64,\n",
    "        num_patches: int = 10,\n",
    "        min_annotation_pixels: int = 50,\n",
    "        val_split: float = 0.2,\n",
    "        random_seed: int = 42,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.save_hyperparameters()\n",
    "\n",
    "        self.train_data_dir = train_data_dir\n",
    "        self.test_data_dir = test_data_dir\n",
    "        self.train_labels_path = train_labels_path\n",
    "        self.trash_list_path = trash_list_path\n",
    "        self.batch_size = batch_size\n",
    "        self.num_workers = num_workers\n",
    "        self.img_size = img_size\n",
    "        self.use_mask = use_mask\n",
    "        self.use_patches = use_patches\n",
    "        self.patch_size = patch_size\n",
    "        self.num_patches = num_patches\n",
    "        self.min_annotation_pixels = min_annotation_pixels\n",
    "        self.val_split = val_split\n",
    "        self.random_seed = random_seed\n",
    "\n",
    "        # Initialize label encoder\n",
    "        self.label_encoder = LabelEncoder()\n",
    "        self.label_encoder.fit(Config.CLASSES)\n",
    "\n",
    "        # Will be set in setup()\n",
    "        self.train_df = None\n",
    "        self.val_df = None\n",
    "        self.train_dataset = None\n",
    "        self.val_dataset = None\n",
    "        self.test_dataset = None\n",
    "\n",
    "    def _get_train_transforms(self) -> transforms.Compose:\n",
    "        \"\"\"Get transforms for training with augmentation.\"\"\"\n",
    "        target_size = self.img_size\n",
    "\n",
    "        transform_list = []\n",
    "\n",
    "        # Standard augmentations\n",
    "        transform_list.extend(\n",
    "            [\n",
    "                transforms.Resize((target_size, target_size)),\n",
    "                transforms.RandomHorizontalFlip(p=0.5),\n",
    "                transforms.RandomVerticalFlip(p=0.5),\n",
    "                transforms.RandomRotation(degrees=90),\n",
    "                transforms.ColorJitter(\n",
    "                    brightness=0.1,\n",
    "                    contrast=0.1,\n",
    "                    saturation=0.1,\n",
    "                    hue=0.05,\n",
    "                ),\n",
    "                transforms.RandomAffine(\n",
    "                    degrees=0, translate=(0.1, 0.1), scale=(0.9, 1.1)\n",
    "                ),\n",
    "                transforms.ToTensor(),\n",
    "                transforms.Normalize(\n",
    "                    mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]\n",
    "                ),\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        return transforms.Compose(transform_list)\n",
    "\n",
    "    def _get_val_transforms(self) -> transforms.Compose:\n",
    "        \"\"\"Get transforms for validation/test (no augmentation).\"\"\"\n",
    "        target_size = self.img_size\n",
    "\n",
    "        transform_list = []\n",
    "\n",
    "        transform_list.extend(\n",
    "            [\n",
    "                transforms.Resize((target_size, target_size)),\n",
    "                transforms.ToTensor(),\n",
    "                transforms.Normalize(\n",
    "                    mean=[0.485, 0.456, 0.406],\n",
    "                    std=[0.229, 0.224, 0.225],\n",
    "                ),\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        return transforms.Compose(transform_list)\n",
    "\n",
    "    def setup(self, stage: Optional[str] = None):\n",
    "        \"\"\"Setup datasets for each stage.\"\"\"\n",
    "        if stage == \"fit\" or stage is None:\n",
    "            # Load and split training data\n",
    "            full_df = pd.read_csv(self.train_labels_path)\n",
    "\n",
    "            trash_path = Path(self.trash_list_path)\n",
    "            if trash_path.exists():\n",
    "                print(f\"Loading trash list from {trash_path}...\")\n",
    "                with open(trash_path, \"r\") as f:\n",
    "                    trash_files = [\n",
    "                        line.strip() for line in f.readlines() if line.strip()\n",
    "                    ]\n",
    "\n",
    "                print(f\"Total lines in trash_list.txt: {len(trash_files)}\")\n",
    "\n",
    "                # Normalize trash filenames to IDs\n",
    "                trash_ids = set()\n",
    "                for t_file in trash_files:\n",
    "                    clean_id = t_file.replace(\"img_\", \"\").replace(\".png\", \"\")\n",
    "                    trash_ids.add(clean_id)\n",
    "\n",
    "                print(\n",
    "                    f\"Unique IDs in trash list (after deduplication): {len(trash_ids)}\"\n",
    "                )\n",
    "\n",
    "                # Helper to clean DataFrame IDs\n",
    "                def clean_df_id(x):\n",
    "                    return str(x).replace(\"img_\", \"\").replace(\".png\", \"\")\n",
    "\n",
    "                # Get all IDs currently in the CSV\n",
    "                csv_ids = set(full_df[\"sample_index\"].apply(clean_df_id))\n",
    "\n",
    "                # Calculate intersection and difference\n",
    "                ids_to_remove = trash_ids.intersection(csv_ids)\n",
    "                ids_not_found = trash_ids - csv_ids\n",
    "\n",
    "                print(f\"IDs from trash list FOUND in CSV: {len(ids_to_remove)}\")\n",
    "                print(f\"IDs from trash list NOT FOUND in CSV: {len(ids_not_found)}\")\n",
    "\n",
    "                if len(ids_not_found) > 0:\n",
    "                    print(f\"Example missing IDs: {list(ids_not_found)[:5]}\")\n",
    "\n",
    "                # Apply the filter\n",
    "                initial_count = len(full_df)\n",
    "                mask = full_df[\"sample_index\"].apply(clean_df_id).isin(trash_ids)\n",
    "                full_df = full_df[~mask].reset_index(drop=True)\n",
    "\n",
    "                dropped_count = initial_count - len(full_df)\n",
    "                print(f\"Final check: Removed {dropped_count} rows from dataframe.\")\n",
    "                print(f\"Remaining samples: {len(full_df)}\")\n",
    "            else:\n",
    "                print(\"No trash_list.txt found, skipping filtering.\")\n",
    "\n",
    "            self.train_df, self.val_df = train_test_split(\n",
    "                full_df,\n",
    "                test_size=self.val_split,\n",
    "                random_state=self.random_seed,\n",
    "            )\n",
    "\n",
    "            # Calculate class weights for balanced sampling\n",
    "            self.class_weights, self.sample_weights = self._compute_sample_weights(\n",
    "                self.train_df\n",
    "            )\n",
    "\n",
    "            # Training dataset: random strategy with half overlap\n",
    "            self.train_dataset = PathologyDataset(\n",
    "                data_dir=self.train_data_dir,\n",
    "                labels_df=self.train_df,\n",
    "                transform=self._get_train_transforms(),\n",
    "                use_mask=self.use_mask,\n",
    "                use_patches=self.use_patches,\n",
    "                patch_size=self.patch_size,\n",
    "                num_patches=self.num_patches,\n",
    "                patch_strategy=\"random\",\n",
    "                min_annotation_pixels=self.min_annotation_pixels,\n",
    "                is_test=False,\n",
    "                label_encoder=self.label_encoder,\n",
    "            )\n",
    "\n",
    "            # Validation dataset: grid strategy with no overlap\n",
    "            self.val_dataset = PathologyDataset(\n",
    "                data_dir=self.train_data_dir,\n",
    "                labels_df=self.val_df,\n",
    "                transform=self._get_val_transforms(),\n",
    "                use_mask=self.use_mask,\n",
    "                use_patches=self.use_patches,\n",
    "                patch_size=self.patch_size,\n",
    "                num_patches=self.num_patches,\n",
    "                patch_strategy=\"grid\",  # Grid for validation\n",
    "                min_annotation_pixels=self.min_annotation_pixels,\n",
    "                is_test=False,\n",
    "                label_encoder=self.label_encoder,\n",
    "            )\n",
    "\n",
    "        if stage == \"test\" or stage == \"predict\" or stage is None:\n",
    "            # Test dataset: grid strategy with no overlap\n",
    "            self.test_dataset = PathologyDataset(\n",
    "                data_dir=self.test_data_dir,\n",
    "                labels_df=None,\n",
    "                transform=self._get_val_transforms(),\n",
    "                use_mask=self.use_mask,\n",
    "                use_patches=self.use_patches,\n",
    "                patch_size=self.patch_size,\n",
    "                num_patches=self.num_patches,\n",
    "                patch_strategy=\"grid\",\n",
    "                min_annotation_pixels=self.min_annotation_pixels,\n",
    "                is_test=True,\n",
    "                label_encoder=self.label_encoder,\n",
    "            )\n",
    "\n",
    "    def _compute_sample_weights(self, df: pd.DataFrame):\n",
    "        \"\"\"Compute sample weights for balanced sampling.\"\"\"\n",
    "        # Encode labels\n",
    "        labels = self.label_encoder.transform(df[\"label\"].values)\n",
    "\n",
    "        # Count samples per class\n",
    "        class_counts = np.bincount(labels, minlength=len(Config.CLASSES))\n",
    "\n",
    "        # Compute class weights (inverse frequency)\n",
    "        class_weights = 1.0 / (class_counts + 1e-6)  # avoid division by zero\n",
    "        class_weights = (\n",
    "            class_weights / class_weights.sum() * len(Config.CLASSES)\n",
    "        )  # normalize\n",
    "\n",
    "        # Assign weight to each sample based on its class\n",
    "        sample_weights = class_weights[labels]\n",
    "\n",
    "        return class_weights, sample_weights\n",
    "\n",
    "    def train_dataloader(self) -> DataLoader:\n",
    "        return DataLoader(\n",
    "            self.train_dataset,\n",
    "            batch_size=self.batch_size,\n",
    "            shuffle=False,\n",
    "            num_workers=self.num_workers,\n",
    "            pin_memory=True,\n",
    "            drop_last=True,\n",
    "            sampler=torch.utils.data.WeightedRandomSampler(\n",
    "                weights=self.sample_weights,\n",
    "                num_samples=len(self.sample_weights),\n",
    "                replacement=True,\n",
    "            ),\n",
    "        )\n",
    "\n",
    "    def val_dataloader(self) -> DataLoader:\n",
    "        return DataLoader(\n",
    "            self.val_dataset,\n",
    "            batch_size=self.batch_size,\n",
    "            shuffle=False,\n",
    "            num_workers=self.num_workers,\n",
    "            pin_memory=True,\n",
    "            drop_last=False,\n",
    "        )\n",
    "\n",
    "    def test_dataloader(self) -> DataLoader:\n",
    "        return DataLoader(\n",
    "            self.test_dataset,\n",
    "            batch_size=self.batch_size,\n",
    "            shuffle=False,\n",
    "            num_workers=self.num_workers,\n",
    "            pin_memory=True,\n",
    "            drop_last=False,\n",
    "        )\n",
    "\n",
    "    def predict_dataloader(self) -> DataLoader:\n",
    "        return self.test_dataloader()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16",
   "metadata": {},
   "source": [
    "## Define neural network model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17",
   "metadata": {},
   "source": [
    "### Attention\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Optional, Tuple\n",
    "\n",
    "import lightning as L\n",
    "import torch\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# Attention Modules\n",
    "# =============================================================================\n",
    "class SimpleAttention(nn.Module):\n",
    "    \"\"\"Simple attention mechanism with proper softmax.\"\"\"\n",
    "\n",
    "    def __init__(self, feature_dim: int, hidden_dim: int = 256, dropout: float = 0.2):\n",
    "        super().__init__()\n",
    "        self.attention = nn.Sequential(\n",
    "            nn.Linear(feature_dim, hidden_dim),\n",
    "            nn.Tanh(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(hidden_dim, 1),\n",
    "        )\n",
    "\n",
    "    def forward(self, features):\n",
    "        # features: [B, num_patches, feature_dim]\n",
    "        attention_scores = self.attention(features)  # [B, num_patches, 1]\n",
    "        attention_weights = F.softmax(attention_scores, dim=1)\n",
    "        aggregated = torch.sum(attention_weights * features, dim=1)  # [B, feature_dim]\n",
    "        return aggregated\n",
    "\n",
    "\n",
    "class GatedAttention(nn.Module):\n",
    "    \"\"\"\n",
    "    Gated Attention Mechanism (Ilse et al. 2018)\n",
    "    Paper: https://arxiv.org/abs/1802.04712\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, feature_dim: int, hidden_dim: int = 256, dropout: float = 0.25):\n",
    "        super().__init__()\n",
    "        self.attention_V = nn.Sequential(\n",
    "            nn.Linear(feature_dim, hidden_dim),\n",
    "            nn.Tanh(),\n",
    "            nn.Dropout(dropout),\n",
    "        )\n",
    "        self.attention_U = nn.Sequential(\n",
    "            nn.Linear(feature_dim, hidden_dim),\n",
    "            nn.Sigmoid(),\n",
    "            nn.Dropout(dropout),\n",
    "        )\n",
    "        self.attention_w = nn.Linear(hidden_dim, 1)\n",
    "\n",
    "    def forward(self, features):\n",
    "        # features: [B, num_patches, feature_dim]\n",
    "        A_V = self.attention_V(features)  # [B, num_patches, hidden_dim]\n",
    "        A_U = self.attention_U(features)  # [B, num_patches, hidden_dim]\n",
    "        attention_scores = self.attention_w(A_V * A_U)  # [B, num_patches, 1]\n",
    "        attention_weights = F.softmax(attention_scores, dim=1)\n",
    "        aggregated = torch.sum(attention_weights * features, dim=1)  # [B, feature_dim]\n",
    "        return aggregated\n",
    "\n",
    "\n",
    "class CLAMAttention(nn.Module):\n",
    "    \"\"\"\n",
    "    CLAM-style Attention (simplified, no unused parameters)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        feature_dim: int,\n",
    "        hidden_dim: int = 256,\n",
    "        dropout: float = 0.25,\n",
    "        num_classes: int = 4,  # kept for API compatibility but not used\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        # Gated attention network\n",
    "        self.attention_a = nn.Sequential(\n",
    "            nn.Linear(feature_dim, hidden_dim),\n",
    "            nn.Tanh(),\n",
    "            nn.Dropout(dropout),\n",
    "        )\n",
    "        self.attention_b = nn.Sequential(\n",
    "            nn.Linear(feature_dim, hidden_dim),\n",
    "            nn.Sigmoid(),\n",
    "            nn.Dropout(dropout),\n",
    "        )\n",
    "        self.attention_c = nn.Linear(hidden_dim, 1)\n",
    "\n",
    "    def forward(self, features, return_attention=False):\n",
    "        # features: [B, num_patches, feature_dim]\n",
    "\n",
    "        # Gated attention\n",
    "        a = self.attention_a(features)\n",
    "        b = self.attention_b(features)\n",
    "        attention_scores = self.attention_c(a * b)  # [B, num_patches, 1]\n",
    "        attention_weights = F.softmax(attention_scores, dim=1)\n",
    "\n",
    "        # Weighted aggregation\n",
    "        aggregated = torch.sum(attention_weights * features, dim=1)  # [B, feature_dim]\n",
    "\n",
    "        if return_attention:\n",
    "            return aggregated, attention_weights.squeeze(-1)\n",
    "        return aggregated\n",
    "\n",
    "\n",
    "class TransMIL(nn.Module):\n",
    "    \"\"\"\n",
    "    Transformer-based Multiple Instance Learning (Shao et al. 2021)\n",
    "    Paper: https://arxiv.org/abs/2106.00908\n",
    "\n",
    "    Uses transformer encoder with learnable class token for aggregation.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        feature_dim: int,\n",
    "        num_heads: int = 8,\n",
    "        num_layers: int = 2,\n",
    "        dropout: float = 0.1,\n",
    "        max_patches: int = 512,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.feature_dim = feature_dim\n",
    "\n",
    "        # Input projection (in case feature_dim is not divisible by num_heads)\n",
    "        self.input_proj = nn.Linear(feature_dim, feature_dim)\n",
    "\n",
    "        # Learnable positional embedding\n",
    "        self.pos_embedding = nn.Parameter(\n",
    "            torch.randn(1, max_patches + 1, feature_dim) * 0.02\n",
    "        )\n",
    "\n",
    "        # Learnable class token\n",
    "        self.cls_token = nn.Parameter(torch.randn(1, 1, feature_dim) * 0.02)\n",
    "\n",
    "        # Transformer encoder\n",
    "        encoder_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=feature_dim,\n",
    "            nhead=num_heads,\n",
    "            dim_feedforward=feature_dim * 4,\n",
    "            dropout=dropout,\n",
    "            activation=\"gelu\",\n",
    "            batch_first=True,\n",
    "            norm_first=True,  # Pre-norm for better training stability\n",
    "        )\n",
    "        self.transformer = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)\n",
    "\n",
    "        # Final layer norm\n",
    "        self.norm = nn.LayerNorm(feature_dim)\n",
    "\n",
    "    def forward(self, features):\n",
    "        # features: [B, num_patches, feature_dim]\n",
    "        B, N, D = features.shape\n",
    "\n",
    "        # Project input\n",
    "        x = self.input_proj(features)\n",
    "\n",
    "        # Add class token\n",
    "        cls_tokens = self.cls_token.expand(B, -1, -1)\n",
    "        x = torch.cat([cls_tokens, x], dim=1)  # [B, 1+N, D]\n",
    "\n",
    "        # Add positional embedding\n",
    "        x = x + self.pos_embedding[:, : N + 1, :]\n",
    "\n",
    "        # Transformer\n",
    "        x = self.transformer(x)\n",
    "        x = self.norm(x)\n",
    "\n",
    "        # Return class token as bag representation\n",
    "        return x[:, 0]  # [B, feature_dim]\n",
    "\n",
    "\n",
    "class MultiHeadAttentionMIL(nn.Module):\n",
    "    \"\"\"\n",
    "    Multi-Head Self-Attention for MIL with learnable class token.\n",
    "    Simpler than TransMIL but still effective.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, feature_dim: int, num_heads: int = 8, dropout: float = 0.1):\n",
    "        super().__init__()\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = feature_dim // num_heads\n",
    "        self.scale = self.head_dim**-0.5\n",
    "\n",
    "        self.qkv = nn.Linear(feature_dim, feature_dim * 3)\n",
    "        self.attn_drop = nn.Dropout(dropout)\n",
    "        self.proj = nn.Linear(feature_dim, feature_dim)\n",
    "        self.proj_drop = nn.Dropout(dropout)\n",
    "\n",
    "        # Learnable class token\n",
    "        self.cls_token = nn.Parameter(torch.randn(1, 1, feature_dim) * 0.02)\n",
    "\n",
    "        # Layer norm\n",
    "        self.norm1 = nn.LayerNorm(feature_dim)\n",
    "        self.norm2 = nn.LayerNorm(feature_dim)\n",
    "\n",
    "        # FFN\n",
    "        self.ffn = nn.Sequential(\n",
    "            nn.Linear(feature_dim, feature_dim * 4),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(feature_dim * 4, feature_dim),\n",
    "            nn.Dropout(dropout),\n",
    "        )\n",
    "\n",
    "    def forward(self, features):\n",
    "        # features: [B, num_patches, feature_dim]\n",
    "        B, N, D = features.shape\n",
    "\n",
    "        # Add class token\n",
    "        cls_tokens = self.cls_token.expand(B, -1, -1)\n",
    "        x = torch.cat([cls_tokens, features], dim=1)  # [B, 1+N, D]\n",
    "\n",
    "        # Self-attention with residual\n",
    "        x_norm = self.norm1(x)\n",
    "        qkv = (\n",
    "            self.qkv(x_norm)\n",
    "            .reshape(B, N + 1, 3, self.num_heads, self.head_dim)\n",
    "            .permute(2, 0, 3, 1, 4)\n",
    "        )\n",
    "        q, k, v = qkv[0], qkv[1], qkv[2]\n",
    "\n",
    "        attn = (q @ k.transpose(-2, -1)) * self.scale\n",
    "        attn = attn.softmax(dim=-1)\n",
    "        attn = self.attn_drop(attn)\n",
    "\n",
    "        out = (attn @ v).transpose(1, 2).reshape(B, N + 1, D)\n",
    "        out = self.proj(out)\n",
    "        out = self.proj_drop(out)\n",
    "        x = x + out\n",
    "\n",
    "        # FFN with residual\n",
    "        x = x + self.ffn(self.norm2(x))\n",
    "\n",
    "        # Return class token\n",
    "        return x[:, 0]  # [B, feature_dim]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19",
   "metadata": {},
   "source": [
    "### Custom Optimizer\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20",
   "metadata": {},
   "source": [
    "#### Ranger\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Lookahead(torch.optim.Optimizer):\n",
    "    \"\"\"\n",
    "    Lookahead optimizer wrapper (Zhang et al. 2019).\n",
    "\n",
    "    Wraps any optimizer and maintains slow weights that are updated\n",
    "    by interpolating towards fast weights every k steps.\n",
    "\n",
    "    Reference: https://arxiv.org/abs/1907.08610\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, base_optimizer, k: int = 6, alpha: float = 0.5):\n",
    "        self.base_optimizer = base_optimizer\n",
    "        self.k = k\n",
    "        self.alpha = alpha\n",
    "        self.param_groups = base_optimizer.param_groups\n",
    "        self.state = {}\n",
    "        self._step_count = 0\n",
    "\n",
    "        # Initialize slow weights\n",
    "        for group in self.param_groups:\n",
    "            for p in group[\"params\"]:\n",
    "                if p.requires_grad:\n",
    "                    self.state[p] = {\"slow_weights\": p.data.clone()}\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def step(self, closure=None):\n",
    "        \"\"\"Perform optimization step.\"\"\"\n",
    "        loss = self.base_optimizer.step(closure)\n",
    "        self._step_count += 1\n",
    "\n",
    "        if self._step_count % self.k == 0:\n",
    "            for group in self.param_groups:\n",
    "                for p in group[\"params\"]:\n",
    "                    if p.requires_grad and p in self.state:\n",
    "                        slow = self.state[p][\"slow_weights\"]\n",
    "                        # Interpolate: slow = slow + alpha * (fast - slow)\n",
    "                        slow.add_(p.data - slow, alpha=self.alpha)\n",
    "                        # Copy slow weights to fast weights\n",
    "                        p.data.copy_(slow)\n",
    "\n",
    "        return loss\n",
    "\n",
    "    def zero_grad(self, set_to_none: bool = False):\n",
    "        self.base_optimizer.zero_grad(set_to_none=set_to_none)\n",
    "\n",
    "    @property\n",
    "    def defaults(self):\n",
    "        return self.base_optimizer.defaults\n",
    "\n",
    "    def state_dict(self):\n",
    "        return {\n",
    "            \"base_optimizer\": self.base_optimizer.state_dict(),\n",
    "            \"slow_weights\": {\n",
    "                id(p): self.state[p][\"slow_weights\"]\n",
    "                for group in self.param_groups\n",
    "                for p in group[\"params\"]\n",
    "                if p in self.state\n",
    "            },\n",
    "            \"step_count\": self._step_count,\n",
    "        }\n",
    "\n",
    "    def load_state_dict(self, state_dict):\n",
    "        self.base_optimizer.load_state_dict(state_dict[\"base_optimizer\"])\n",
    "        self._step_count = state_dict[\"step_count\"]\n",
    "\n",
    "\n",
    "class RAdam(torch.optim.Optimizer):\n",
    "    \"\"\"\n",
    "    RAdam optimizer (Liu et al. 2019).\n",
    "\n",
    "    Rectified Adam - automatically adjusts adaptive learning rate\n",
    "    based on variance of second moment estimate.\n",
    "\n",
    "    Reference: https://arxiv.org/abs/1908.03265\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        params,\n",
    "        lr: float = 1e-3,\n",
    "        betas: Tuple[float, float] = (0.9, 0.999),\n",
    "        eps: float = 1e-8,\n",
    "        weight_decay: float = 0,\n",
    "    ):\n",
    "        defaults = dict(lr=lr, betas=betas, eps=eps, weight_decay=weight_decay)\n",
    "        super().__init__(params, defaults)\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def step(self, closure=None):\n",
    "        loss = None\n",
    "        if closure is not None:\n",
    "            with torch.enable_grad():\n",
    "                loss = closure()\n",
    "\n",
    "        for group in self.param_groups:\n",
    "            for p in group[\"params\"]:\n",
    "                if p.grad is None:\n",
    "                    continue\n",
    "\n",
    "                grad = p.grad\n",
    "                if grad.is_sparse:\n",
    "                    raise RuntimeError(\"RAdam does not support sparse gradients\")\n",
    "\n",
    "                state = self.state[p]\n",
    "\n",
    "                # State initialization\n",
    "                if len(state) == 0:\n",
    "                    state[\"step\"] = 0\n",
    "                    state[\"exp_avg\"] = torch.zeros_like(p)\n",
    "                    state[\"exp_avg_sq\"] = torch.zeros_like(p)\n",
    "\n",
    "                exp_avg, exp_avg_sq = state[\"exp_avg\"], state[\"exp_avg_sq\"]\n",
    "                beta1, beta2 = group[\"betas\"]\n",
    "\n",
    "                state[\"step\"] += 1\n",
    "                step = state[\"step\"]\n",
    "\n",
    "                # Decoupled weight decay\n",
    "                if group[\"weight_decay\"] != 0:\n",
    "                    p.mul_(1 - group[\"lr\"] * group[\"weight_decay\"])\n",
    "\n",
    "                # Decay the first and second moment running average coefficient\n",
    "                exp_avg.mul_(beta1).add_(grad, alpha=1 - beta1)\n",
    "                exp_avg_sq.mul_(beta2).addcmul_(grad, grad, value=1 - beta2)\n",
    "\n",
    "                # Bias correction\n",
    "                bias_correction1 = 1 - beta1**step\n",
    "                bias_correction2 = 1 - beta2**step\n",
    "\n",
    "                # Compute the maximum length of the approximated SMA\n",
    "                rho_inf = 2 / (1 - beta2) - 1\n",
    "                # Compute the length of the approximated SMA\n",
    "                rho_t = rho_inf - 2 * step * (beta2**step) / bias_correction2\n",
    "\n",
    "                # Variance rectification\n",
    "                if rho_t > 5:\n",
    "                    # Compute variance rectification term\n",
    "                    rect = (\n",
    "                        (rho_t - 4)\n",
    "                        * (rho_t - 2)\n",
    "                        * rho_inf\n",
    "                        / ((rho_inf - 4) * (rho_inf - 2) * rho_t)\n",
    "                    ) ** 0.5\n",
    "\n",
    "                    # Compute adaptive learning rate\n",
    "                    step_size = group[\"lr\"] * rect / bias_correction1\n",
    "\n",
    "                    denom = exp_avg_sq.sqrt().add_(group[\"eps\"])\n",
    "                    p.addcdiv_(exp_avg, denom, value=-step_size)\n",
    "                else:\n",
    "                    # Use unadapted learning rate\n",
    "                    step_size = group[\"lr\"] / bias_correction1\n",
    "                    p.add_(exp_avg, alpha=-step_size)\n",
    "\n",
    "        return loss\n",
    "\n",
    "\n",
    "def Ranger(\n",
    "    params,\n",
    "    lr: float = 1e-3,\n",
    "    betas=(0.9, 0.999),\n",
    "    eps=1e-8,\n",
    "    weight_decay=0,\n",
    "    k=6,\n",
    "    alpha=0.5,\n",
    "):\n",
    "    \"\"\"\n",
    "    Ranger optimizer: RAdam + Lookahead.\n",
    "\n",
    "    Combines the variance rectification of RAdam with the\n",
    "    stabilizing effect of Lookahead for robust training.\n",
    "\n",
    "    Args:\n",
    "        params: Model parameters\n",
    "        lr: Learning rate\n",
    "        betas: Coefficients for computing running averages\n",
    "        eps: Term added to denominator for numerical stability\n",
    "        weight_decay: Weight decay (L2 penalty)\n",
    "        k: Lookahead step interval\n",
    "        alpha: Lookahead interpolation factor\n",
    "\n",
    "    Reference: https://github.com/lessw2020/Ranger-Deep-Learning-Optimizer\n",
    "    \"\"\"\n",
    "    base = RAdam(params, lr=lr, betas=betas, eps=eps, weight_decay=weight_decay)\n",
    "    return Lookahead(base, k=k, alpha=alpha)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22",
   "metadata": {},
   "source": [
    "### Main model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PathologyModel(L.LightningModule):\n",
    "    \"\"\"\n",
    "    Lightning Module for pathology image classification.\n",
    "\n",
    "    Supports both single-image and multi-instance learning (patch-based) approaches\n",
    "    with flexible backbone architectures and aggregation strategies.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        model_name: str = \"resnet50\",\n",
    "        num_classes: int = 4,\n",
    "        pretrained: bool = True,\n",
    "        learning_rate: float = 1e-4,\n",
    "        weight_decay: float = 1e-4,\n",
    "        use_patches: bool = True,\n",
    "        patch_aggregation: str = \"clam\",\n",
    "        optimizer_name: str = \"adamw\",\n",
    "        dropout_rate: float = 0.3,\n",
    "        label_smoothing: float = 0.1,\n",
    "        class_weights: Optional[torch.Tensor] = None,\n",
    "        warmup_epochs: int = 5,\n",
    "        freeze_backbone_epochs: int = 0,\n",
    "        mixup_alpha: float = 0.0,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            model_name: Name of the timm model ['resnet', 'convnext_tiny', 'efficientnet_b0', etc.].\n",
    "            num_classes: Number of output classes.\n",
    "            pretrained: Whether to use ImageNet pretrained weights.\n",
    "            learning_rate: Base learning rate for optimizer.\n",
    "            weight_decay: L2 regularization weight.\n",
    "            use_patches: Whether input is patch-based [B, num_patches, C, H, W].\n",
    "            patch_aggregation: Aggregation method:\n",
    "                - 'mean': Simple mean pooling\n",
    "                - 'max': Max pooling\n",
    "                - 'attention': Simple attention with softmax\n",
    "                - 'gated_attention': Gated attention\n",
    "                - 'clam': CLAM attention\n",
    "                - 'transmil': Transformer MIL\n",
    "                - 'multihead': Multi-head self-attention\n",
    "            optimizer_name: Optimizer to use:\n",
    "                - 'adamw': AdamW\n",
    "                - 'lion': Lion\n",
    "                - 'ranger': RAdam\n",
    "            dropout_rate: Dropout probability before classifier.\n",
    "            label_smoothing: Label smoothing factor for cross-entropy.\n",
    "            class_weights: Optional class weights for imbalanced datasets.\n",
    "            warmup_epochs: Number of warmup epochs for learning rate.\n",
    "            freeze_backbone_epochs: Number of epochs to freeze backbone, default 0 (no freezing).\n",
    "            mixup_alpha: Alpha parameter for Mixup augmentation (0 = no Mixup).\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.save_hyperparameters(ignore=[\"class_weights\"])\n",
    "\n",
    "        # Store hyperparameters\n",
    "        self.model_name = model_name\n",
    "        self.num_classes = num_classes\n",
    "        self.learning_rate = learning_rate\n",
    "        self.weight_decay = weight_decay\n",
    "        self.use_patches = use_patches\n",
    "        self.patch_aggregation = patch_aggregation\n",
    "        self.optimizer_name = optimizer_name.lower()\n",
    "        self.label_smoothing = label_smoothing\n",
    "        self.class_weights = class_weights\n",
    "        self.warmup_epochs = warmup_epochs\n",
    "        self.freeze_backbone_epochs = freeze_backbone_epochs\n",
    "        self.mixup_alpha = mixup_alpha\n",
    "\n",
    "        # Validate optimizer choice\n",
    "        valid_optimizers = [\"adamw\", \"lion\", \"ranger\"]\n",
    "        if self.optimizer_name not in valid_optimizers:\n",
    "            raise ValueError(\n",
    "                f\"Unknown optimizer: {optimizer_name}. Choose from: {valid_optimizers}\"\n",
    "            )\n",
    "\n",
    "        # Check if Lion is available when requested\n",
    "        if self.optimizer_name == \"lion\" and Lion is None:\n",
    "            raise ImportError(\n",
    "                \"Lion optimizer requires lion-pytorch package. \"\n",
    "                \"Install with: pip install lion-pytorch\"\n",
    "            )\n",
    "\n",
    "        # Build model architecture\n",
    "        self._build_model(model_name, pretrained, dropout_rate)\n",
    "\n",
    "        # Initialize loss and metrics\n",
    "        self._setup_loss()\n",
    "        self._setup_metrics()\n",
    "\n",
    "        # Freeze backbone if requested\n",
    "        if freeze_backbone_epochs > 0:\n",
    "            self._freeze_backbone()\n",
    "\n",
    "    def _build_model(self, model_name: str, pretrained: bool, dropout_rate: float):\n",
    "        \"\"\"Build the model architecture.\"\"\"\n",
    "        # Create backbone using timm\n",
    "        self.backbone = timm.create_model(\n",
    "            model_name,\n",
    "            pretrained=pretrained,\n",
    "            num_classes=0,\n",
    "            drop_rate=0.0,\n",
    "        )\n",
    "\n",
    "        # Get feature dimension\n",
    "        self.feature_dim = self.backbone.num_features\n",
    "\n",
    "        # Build aggregation module for patches\n",
    "        if self.use_patches:\n",
    "            self.aggregation = self._build_aggregation_module()\n",
    "        else:\n",
    "            self.aggregation = None\n",
    "\n",
    "        # Build classifier head\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.LayerNorm(\n",
    "                self.feature_dim\n",
    "            ),  # LayerNorm often better than BatchNorm for MIL\n",
    "            nn.Dropout(p=dropout_rate),\n",
    "            nn.Linear(self.feature_dim, self.feature_dim // 2),\n",
    "            nn.GELU(),  # GELU often better than ReLU\n",
    "            nn.LayerNorm(self.feature_dim // 2),\n",
    "            nn.Dropout(p=dropout_rate / 2),\n",
    "            nn.Linear(self.feature_dim // 2, self.num_classes),\n",
    "        )\n",
    "\n",
    "    def _build_aggregation_module(self) -> Optional[nn.Module]:\n",
    "        \"\"\"Build patch aggregation module based on strategy.\"\"\"\n",
    "\n",
    "        if self.patch_aggregation in [\"mean\", \"max\"]:\n",
    "            return None\n",
    "\n",
    "        elif self.patch_aggregation == \"attention\":\n",
    "            return SimpleAttention(self.feature_dim)\n",
    "\n",
    "        elif self.patch_aggregation == \"gated_attention\":\n",
    "            return GatedAttention(self.feature_dim)\n",
    "\n",
    "        elif self.patch_aggregation == \"clam\":\n",
    "            return CLAMAttention(\n",
    "                self.feature_dim,\n",
    "                num_classes=self.num_classes,\n",
    "            )\n",
    "\n",
    "        elif self.patch_aggregation == \"transmil\":\n",
    "            return TransMIL(self.feature_dim)\n",
    "\n",
    "        elif self.patch_aggregation == \"multihead\":\n",
    "            return MultiHeadAttentionMIL(self.feature_dim)\n",
    "\n",
    "        else:\n",
    "            raise ValueError(\n",
    "                f\"Unknown aggregation: {self.patch_aggregation}. \"\n",
    "                f\"Choose from: mean, max, attention, gated_attention, clam, transmil, multihead\"\n",
    "            )\n",
    "\n",
    "    def _setup_loss(self):\n",
    "        \"\"\"Initialize loss function.\"\"\"\n",
    "        self.criterion = nn.CrossEntropyLoss(\n",
    "            weight=self.class_weights,\n",
    "            label_smoothing=self.label_smoothing,\n",
    "        )\n",
    "\n",
    "    def _setup_metrics(self):\n",
    "        \"\"\"Initialize metrics for each stage.\"\"\"\n",
    "        metric_kwargs = {\"task\": \"multiclass\", \"num_classes\": self.num_classes}\n",
    "\n",
    "        # Training metrics\n",
    "        self.train_acc = Accuracy(**metric_kwargs)\n",
    "        self.train_f1 = F1Score(**metric_kwargs, average=\"macro\")\n",
    "\n",
    "        # Validation metrics\n",
    "        self.val_acc = Accuracy(**metric_kwargs)\n",
    "        self.val_f1 = F1Score(**metric_kwargs, average=\"macro\")\n",
    "        self.val_auroc = AUROC(**metric_kwargs)\n",
    "\n",
    "        # Test metrics\n",
    "        self.test_acc = Accuracy(**metric_kwargs)\n",
    "        self.test_f1 = F1Score(**metric_kwargs, average=\"macro\")\n",
    "        self.test_auroc = AUROC(**metric_kwargs)\n",
    "        self.test_confmat = ConfusionMatrix(**metric_kwargs)\n",
    "\n",
    "    def _freeze_backbone(self):\n",
    "        \"\"\"Freeze backbone parameters for transfer learning.\"\"\"\n",
    "        for param in self.backbone.parameters():\n",
    "            param.requires_grad = False\n",
    "        print(f\"Backbone frozen for {self.freeze_backbone_epochs} epochs\")\n",
    "\n",
    "    def _unfreeze_backbone(self):\n",
    "        \"\"\"Unfreeze backbone parameters.\"\"\"\n",
    "        for param in self.backbone.parameters():\n",
    "            param.requires_grad = True\n",
    "        print(\"Backbone unfrozen\")\n",
    "\n",
    "    def _apply_mixup(self, x: torch.Tensor, y: torch.Tensor):\n",
    "        \"\"\"\n",
    "        Applies Mixup augmentation to the batch.\n",
    "        Returns:\n",
    "            mixed_x: The mixed input tensor\n",
    "            target_a: Original labels\n",
    "            target_b: Shuffled labels\n",
    "            lam: The mixing coefficient (lambda)\n",
    "        \"\"\"\n",
    "        # 1. Sample lambda from Beta distribution\n",
    "        if self.mixup_alpha > 0:\n",
    "            lam = np.random.beta(self.mixup_alpha, self.mixup_alpha)\n",
    "        else:\n",
    "            lam = 1.0\n",
    "\n",
    "        # 2. Generate permutation indices\n",
    "        batch_size = x.size(0)\n",
    "        index = torch.randperm(batch_size, device=x.device)\n",
    "\n",
    "        # 3. Create mixed inputs\n",
    "        # This works for both [B, C, H, W] and [B, Num_Patches, C, H, W]\n",
    "        mixed_x = lam * x + (1 - lam) * x[index, :]\n",
    "\n",
    "        # 4. Get pair of targets\n",
    "        target_a, target_b = y, y[index]\n",
    "\n",
    "        return mixed_x, target_a, target_b, lam\n",
    "\n",
    "    def _forward_single(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"Forward pass for single images.\"\"\"\n",
    "        return self.backbone(x)\n",
    "\n",
    "    def _forward_patches(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"Forward pass for patch-based images.\"\"\"\n",
    "        batch_size, num_patches, c, h, w = x.shape\n",
    "\n",
    "        # Reshape to process all patches\n",
    "        x = x.view(batch_size * num_patches, c, h, w)\n",
    "\n",
    "        # Extract features\n",
    "        features = self.backbone(x)  # [B * num_patches, feature_dim]\n",
    "\n",
    "        # Reshape back\n",
    "        features = features.view(batch_size, num_patches, -1)\n",
    "\n",
    "        # Aggregate patches\n",
    "        features = self._aggregate_patches(features)\n",
    "\n",
    "        return features\n",
    "\n",
    "    def _aggregate_patches(self, features: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Aggregate patch features.\n",
    "\n",
    "        Args:\n",
    "            features: [B, num_patches, feature_dim]\n",
    "\n",
    "        Returns:\n",
    "            Aggregated features [B, feature_dim]\n",
    "        \"\"\"\n",
    "        if self.patch_aggregation == \"mean\":\n",
    "            return features.mean(dim=1)\n",
    "\n",
    "        elif self.patch_aggregation == \"max\":\n",
    "            return features.max(dim=1)[0]\n",
    "\n",
    "        else:\n",
    "            return self.aggregation(features)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Forward pass.\n",
    "\n",
    "        Args:\n",
    "            x: Input tensor\n",
    "               - If use_patches=False: [B, C, H, W]\n",
    "               - If use_patches=True: [B, num_patches, C, H, W]\n",
    "\n",
    "        Returns:\n",
    "            Logits of shape [B, num_classes]\n",
    "        \"\"\"\n",
    "        if self.use_patches and x.dim() == 5:\n",
    "            features = self._forward_patches(x)\n",
    "        else:\n",
    "            features = self._forward_single(x)\n",
    "\n",
    "        logits = self.classifier(features)\n",
    "        return logits\n",
    "\n",
    "    def training_step(self, batch: Tuple, batch_idx: int) -> torch.Tensor:\n",
    "        \"\"\"Training step with Mixup support.\"\"\"\n",
    "        x, y = batch\n",
    "\n",
    "        # Check if we should apply mixup\n",
    "        # We generally only apply mixup if alpha > 0\n",
    "        if self.mixup_alpha > 0:\n",
    "            mixed_x, target_a, target_b, lam = self._apply_mixup(x, y)\n",
    "\n",
    "            # Forward pass with mixed input\n",
    "            logits = self(mixed_x)\n",
    "\n",
    "            # Mixup Loss: weighted sum of loss against both targets\n",
    "            loss_a = self.criterion(logits, target_a)\n",
    "            loss_b = self.criterion(logits, target_b)\n",
    "            loss = lam * loss_a + (1 - lam) * loss_b\n",
    "\n",
    "            # For logging accuracy, we look at the 'dominant' label (optional)\n",
    "            # Or we can simply skip accuracy logging for mixup steps as it's noisy.\n",
    "            # Here, we calculate acc against the target with higher weight.\n",
    "            if lam >= 0.5:\n",
    "                preds = torch.argmax(logits, dim=1)\n",
    "                self.train_acc(preds, target_a)\n",
    "                self.train_f1(preds, target_a)\n",
    "            else:\n",
    "                preds = torch.argmax(logits, dim=1)\n",
    "                self.train_acc(preds, target_b)\n",
    "                self.train_f1(preds, target_b)\n",
    "\n",
    "        else:\n",
    "            # Standard training (No Mixup)\n",
    "            logits = self(x)\n",
    "            loss = self.criterion(logits, y)\n",
    "            preds = torch.argmax(logits, dim=1)\n",
    "            self.train_acc(preds, y)\n",
    "            self.train_f1(preds, y)\n",
    "\n",
    "        # Log metrics\n",
    "        self.log(\"train/loss\", loss, on_step=True, on_epoch=True, prog_bar=True)\n",
    "        self.log(\n",
    "            \"train/acc\", self.train_acc, on_step=False, on_epoch=True, prog_bar=True\n",
    "        )\n",
    "        self.log(\"train/f1\", self.train_f1, on_step=False, on_epoch=True)\n",
    "\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch: Tuple, batch_idx: int):\n",
    "        \"\"\"Validation step.\"\"\"\n",
    "        x, y = batch\n",
    "        logits = self(x)\n",
    "        loss = self.criterion(logits, y)\n",
    "\n",
    "        preds = torch.argmax(logits, dim=1)\n",
    "        probs = F.softmax(logits, dim=1)\n",
    "\n",
    "        self.val_acc(preds, y)\n",
    "        self.val_f1(preds, y)\n",
    "        self.val_auroc(probs, y)\n",
    "\n",
    "        self.log(\"val/loss\", loss, on_step=False, on_epoch=True, prog_bar=True)\n",
    "        self.log(\"val/acc\", self.val_acc, on_step=False, on_epoch=True, prog_bar=True)\n",
    "        self.log(\"val/f1\", self.val_f1, on_step=False, on_epoch=True, prog_bar=True)\n",
    "        self.log(\"val/auroc\", self.val_auroc, on_step=False, on_epoch=True)\n",
    "\n",
    "    def test_step(self, batch: Tuple, batch_idx: int):\n",
    "        \"\"\"Test step.\"\"\"\n",
    "        x, y = batch\n",
    "        logits = self(x)\n",
    "        loss = self.criterion(logits, y)\n",
    "\n",
    "        preds = torch.argmax(logits, dim=1)\n",
    "        probs = F.softmax(logits, dim=1)\n",
    "\n",
    "        self.test_acc(preds, y)\n",
    "        self.test_f1(preds, y)\n",
    "        self.test_auroc(probs, y)\n",
    "        self.test_confmat(preds, y)\n",
    "\n",
    "        self.log(\"test/loss\", loss)\n",
    "        self.log(\"test/acc\", self.test_acc)\n",
    "        self.log(\"test/f1\", self.test_f1)\n",
    "        self.log(\"test/auroc\", self.test_auroc)\n",
    "\n",
    "    def predict_step(self, batch: Tuple, batch_idx: int) -> Dict[str, Any]:\n",
    "        \"\"\"\n",
    "        Prediction step with Test Time Augmentation (TTA).\n",
    "        Averages predictions across: Original, Horizontal Flip, Vertical Flip, and Rotations.\n",
    "        \"\"\"\n",
    "        x, sample_ids = batch\n",
    "        # x shape is either [B, C, H, W] or [B, N, C, H, W]\n",
    "        # augment the spatial dims: H and W (the last two dimensions)\n",
    "        spatial_dims = [-2, -1]\n",
    "\n",
    "        # Define the list of augmentations to apply\n",
    "        augmentations = [\n",
    "            lambda t: t,\n",
    "            lambda t: torch.flip(t, dims=[-1]),  # Horizontal Flip\n",
    "            lambda t: torch.flip(t, dims=[-2]),  # Vertical Flip\n",
    "            lambda t: torch.rot90(t, k=1, dims=spatial_dims),  # 90 degree rotation\n",
    "        ]\n",
    "\n",
    "        logits_sum = 0\n",
    "\n",
    "        # Loop through augmentations\n",
    "        for aug_func in augmentations:\n",
    "            aug_x = aug_func(x)\n",
    "            logits = self(aug_x)\n",
    "            logits_sum += logits\n",
    "\n",
    "        # Average the logits\n",
    "        avg_logits = logits_sum / len(augmentations)\n",
    "\n",
    "        # Compute final probabilities and predictions\n",
    "        probs = F.softmax(avg_logits, dim=1)\n",
    "        preds = torch.argmax(avg_logits, dim=1)\n",
    "\n",
    "        return {\n",
    "            \"sample_ids\": sample_ids,\n",
    "            \"predictions\": preds,\n",
    "            \"probabilities\": probs,\n",
    "            \"logits\": avg_logits,\n",
    "        }\n",
    "\n",
    "    def configure_optimizers(self) -> Dict[str, Any]:\n",
    "        \"\"\"Configure optimizer and scheduler.\"\"\"\n",
    "        # Separate parameters for differential learning rates\n",
    "        backbone_params = list(self.backbone.parameters())\n",
    "        classifier_params = list(self.classifier.parameters())\n",
    "        if self.aggregation is not None:\n",
    "            classifier_params += list(self.aggregation.parameters())\n",
    "\n",
    "        # Adjust hyperparameters based on optimizer\n",
    "        if self.optimizer_name == \"lion\":\n",
    "            backbone_lr = self.learning_rate * 0.03  # Even lower for backbone\n",
    "            classifier_lr = self.learning_rate * 0.3\n",
    "            wd = self.weight_decay * 10\n",
    "        else:\n",
    "            backbone_lr = self.learning_rate * 0.1\n",
    "            classifier_lr = self.learning_rate\n",
    "            wd = self.weight_decay\n",
    "\n",
    "        # Differential learning rates\n",
    "        param_groups = [\n",
    "            {\n",
    "                \"params\": backbone_params,\n",
    "                \"lr\": backbone_lr,\n",
    "                \"name\": \"backbone\",\n",
    "            },\n",
    "            {\n",
    "                \"params\": classifier_params,\n",
    "                \"lr\": classifier_lr,\n",
    "                \"name\": \"classifier\",\n",
    "            },\n",
    "        ]\n",
    "\n",
    "        # Create optimizer based on selection\n",
    "        if self.optimizer_name == \"adamw\":\n",
    "            optimizer = torch.optim.AdamW(\n",
    "                param_groups,\n",
    "                weight_decay=wd,\n",
    "            )\n",
    "        elif self.optimizer_name == \"lion\":\n",
    "            optimizer = Lion(\n",
    "                param_groups,\n",
    "                weight_decay=wd,\n",
    "                betas=(0.9, 0.99),\n",
    "            )\n",
    "        elif self.optimizer_name == \"ranger\":\n",
    "            optimizer = Ranger(\n",
    "                param_groups,\n",
    "                weight_decay=wd,\n",
    "                k=6,  # Lookahead step\n",
    "                alpha=0.5,  # Lookahead alpha\n",
    "            )\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown optimizer: {self.optimizer_name}\")\n",
    "\n",
    "        # Log optimizer info\n",
    "        # print(f\"\\n{'=' * 60}\")\n",
    "        # print(f\"Optimizer: {self.optimizer_name.upper()}\")\n",
    "        # print(f\"Backbone LR: {backbone_lr:.2e}\")\n",
    "        # print(f\"Classifier LR: {classifier_lr:.2e}\")\n",
    "        # print(f\"Weight Decay: {wd:.2e}\")\n",
    "        # print(f\"{'=' * 60}\\n\")\n",
    "\n",
    "        # Cosine annealing with warmup\n",
    "        def lr_lambda(epoch):\n",
    "            if epoch < self.warmup_epochs:\n",
    "                return (epoch + 1) / self.warmup_epochs\n",
    "            return 0.5 * (\n",
    "                1\n",
    "                + torch.cos(\n",
    "                    torch.tensor(\n",
    "                        (epoch - self.warmup_epochs)\n",
    "                        / (50 - self.warmup_epochs)\n",
    "                        * 3.14159\n",
    "                    )\n",
    "                ).item()\n",
    "            )\n",
    "\n",
    "        scheduler = torch.optim.lr_scheduler.LambdaLR(optimizer, lr_lambda=lr_lambda)\n",
    "\n",
    "        return {\n",
    "            \"optimizer\": optimizer,\n",
    "            \"lr_scheduler\": {\n",
    "                \"scheduler\": scheduler,\n",
    "                \"interval\": \"epoch\",\n",
    "                \"frequency\": 1,\n",
    "            },\n",
    "        }\n",
    "\n",
    "    def on_train_epoch_start(self):\n",
    "        \"\"\"Called at the start of each training epoch.\"\"\"\n",
    "        if (\n",
    "            self.freeze_backbone_epochs > 0\n",
    "            and self.current_epoch == self.freeze_backbone_epochs\n",
    "        ):\n",
    "            self._unfreeze_backbone()\n",
    "            self.trainer.strategy.setup_optimizers(self.trainer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24",
   "metadata": {},
   "source": [
    "## Train logic\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25",
   "metadata": {},
   "outputs": [],
   "source": [
    "from lightning.pytorch.callbacks import (\n",
    "    EarlyStopping,\n",
    "    LearningRateMonitor,\n",
    "    ModelCheckpoint,\n",
    ")\n",
    "\n",
    "# Initialize\n",
    "datamodule = PathologyDataModule(\n",
    "    use_mask=True,\n",
    "    use_patches=True,\n",
    "    patch_size=112,\n",
    "    num_patches=8,\n",
    "    img_size=224,\n",
    "    batch_size=16,\n",
    "    min_annotation_pixels=1,\n",
    ")\n",
    "model = PathologyModel(\n",
    "    model_name=\"convnext_tiny\",\n",
    "    use_patches=True,\n",
    "    patch_aggregation=\"clam\",\n",
    "    learning_rate=2e-4,\n",
    "    weight_decay=1e-2,\n",
    "    dropout_rate=0.3,\n",
    "    label_smoothing=0.1,\n",
    "    # freeze_backbone_epochs=5,\n",
    "    optimizer_name=\"adamw\",\n",
    "    mixup_alpha=1,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train\n",
    "trainer = L.Trainer(\n",
    "    max_epochs=50,\n",
    "    accelerator=\"auto\",\n",
    "    callbacks=[\n",
    "        ModelCheckpoint(\n",
    "            monitor=\"val/f1\", mode=\"max\", filename=\"{epoch:02d}-{val/f1:.4f}\"\n",
    "        ),\n",
    "        EarlyStopping(monitor=\"val/loss\", patience=10, mode=\"min\"),\n",
    "        LearningRateMonitor(logging_interval=\"epoch\"),\n",
    "    ],\n",
    "    accumulate_grad_batches=1,\n",
    "    # gradient_clip_val=0.5,\n",
    "    precision=\"16-mixed\",\n",
    "    log_every_n_steps=5,\n",
    "    devices=\"auto\",\n",
    "    logger=None,\n",
    ")\n",
    "trainer.fit(model, datamodule)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(trainer.checkpoint_callback.best_model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28",
   "metadata": {},
   "source": [
    "## Inference logic\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inference on test set\n",
    "# 1. Load the best model from the checkpoint\n",
    "best_checkpoint = trainer.checkpoint_callback.best_model_path\n",
    "\n",
    "print(f\"Loading model from: {best_checkpoint}\")\n",
    "best_model = PathologyModel.load_from_checkpoint(best_checkpoint)\n",
    "\n",
    "datamodule = PathologyDataModule()\n",
    "datamodule.setup(stage=\"test\")\n",
    "\n",
    "trainer = L.Trainer(\n",
    "    accelerator=\"auto\",\n",
    "    precision=\"16-mixed\",\n",
    ")\n",
    "\n",
    "# 2. Run prediction using the Trainer\n",
    "print(\"Generating predictions...\")\n",
    "predictions = trainer.predict(best_model, datamodule=datamodule)\n",
    "\n",
    "# 3. Process the results\n",
    "sample_ids = []\n",
    "pred_labels_encoded = []\n",
    "\n",
    "for batch in predictions:\n",
    "    sample_ids.extend(batch[\"sample_ids\"])\n",
    "    pred_labels_encoded.extend(batch[\"predictions\"].cpu().numpy().tolist())\n",
    "\n",
    "# 4. Decode integer labels back to string labels\n",
    "decoded_labels = datamodule.label_encoder.inverse_transform(pred_labels_encoded)\n",
    "\n",
    "# 5. Format sample_ids back to filenames (e.g., \"1004\" -> \"img_1004.png\")\n",
    "# We assume sample_ids contains the raw ID strings (e.g., \"1004\", \"1005\")\n",
    "formatted_sample_ids = [f\"img_{sid}.png\" for sid in sample_ids]\n",
    "\n",
    "# 6. Create DataFrame\n",
    "submission_df = pd.DataFrame(\n",
    "    {\"sample_index\": formatted_sample_ids, \"label\": decoded_labels}\n",
    ")\n",
    "\n",
    "# Optional: Sort by sample_index for a cleaner look\n",
    "submission_df = submission_df.sort_values(\"sample_index\").reset_index(drop=True)\n",
    "\n",
    "# 7. Save to CSV\n",
    "output_csv_path = \"submission.csv\"\n",
    "submission_df.to_csv(output_csv_path, index=False)\n",
    "\n",
    "print(\"\\n\" + \"=\" * 50)\n",
    "print(f\"Submission saved to: {output_csv_path}\")\n",
    "print(f\"Total samples predicted: {len(submission_df)}\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Preview the first few rows\n",
    "print(submission_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30",
   "metadata": {},
   "outputs": [],
   "source": [
    "submission_df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
