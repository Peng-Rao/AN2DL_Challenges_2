{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install gdown lightning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {},
   "outputs": [],
   "source": [
    "!gdown https://drive.google.com/uc?id=1rh_21CJliIkuahqqaWETH8Zf06qEA7eG\n",
    "!mkdir temp\n",
    "!mkdir data\n",
    "!unzip -q data.zip -d temp\n",
    "!mv temp/data/* data/\n",
    "!rm -rf temp\n",
    "!ls -l data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from typing import List, Optional, Tuple\n",
    "\n",
    "import cv2\n",
    "import lightning.pytorch as pl\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from lightning.pytorch import LightningDataModule, LightningModule, Trainer\n",
    "from lightning.pytorch.callbacks import (\n",
    "    EarlyStopping,\n",
    "    LearningRateMonitor,\n",
    "    ModelCheckpoint,\n",
    ")\n",
    "from lightning.pytorch.loggers import TensorBoardLogger\n",
    "from PIL import Image\n",
    "from scipy import ndimage\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from torch.utils.data import DataLoader, Dataset, WeightedRandomSampler\n",
    "from torchvision import models, transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# Configuration\n",
    "# ============================================================================\n",
    "class Config:\n",
    "    # Data paths\n",
    "    DATA_DIR = \"./data\"\n",
    "    TRAIN_DATA_DIR = \"./data/train_data\"\n",
    "    TEST_DATA_DIR = \"./data/test_data\"\n",
    "    TRAIN_LABELS_PATH = \"./data/train_labels_cleaned.csv\"\n",
    "    OUTPUT_PATH = \"./predictions.csv\"\n",
    "\n",
    "    # Class labels\n",
    "    CLASSES = [\"Luminal A\", \"Luminal B\", \"HER2(+)\", \"Triple negative\"]\n",
    "    NUM_CLASSES = 4\n",
    "\n",
    "    # Image settings\n",
    "    IMG_SIZE = 512  # Larger size for histopathology\n",
    "    USE_MASK = True\n",
    "\n",
    "    # Tissue detection settings\n",
    "    TISSUE_THRESHOLD = 0.8  # Threshold for tissue detection (lower = more sensitive)\n",
    "    MIN_TISSUE_AREA = 0.05  # Minimum tissue area ratio\n",
    "    PADDING = 50  # Padding around tissue bounding box\n",
    "\n",
    "    # Patch-based settings (for very large images)\n",
    "    USE_PATCHES = True\n",
    "    PATCH_SIZE = 512\n",
    "    NUM_PATCHES = 8  # Number of patches to sample per image\n",
    "\n",
    "    # Stain normalization\n",
    "    USE_STAIN_NORMALIZATION = True\n",
    "\n",
    "    # Training settings\n",
    "    BATCH_SIZE = 4\n",
    "    NUM_WORKERS = 2\n",
    "    MAX_EPOCHS = 100\n",
    "    LEARNING_RATE = 1e-4\n",
    "    WEIGHT_DECAY = 1e-4\n",
    "\n",
    "    # Model settings\n",
    "    MODEL_NAME = \"efficientnet_b3\"\n",
    "    PRETRAINED = True\n",
    "\n",
    "    # Validation split\n",
    "    VAL_SPLIT = 0.2\n",
    "    RANDOM_SEED = 42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# Stain Normalization (Macenko method simplified)\n",
    "# ============================================================================\n",
    "class StainNormalizer:\n",
    "    \"\"\"Simple stain normalization for H&E images.\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        # Reference stain vectors (standard H&E)\n",
    "        self.target_means = np.array([148.60, 41.56, 105.97])  # LAB color space\n",
    "        self.target_stds = np.array([41.56, 9.01, 6.67])\n",
    "\n",
    "    def normalize(self, img: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"Normalize stain colors using LAB color space.\"\"\"\n",
    "        if img.dtype != np.uint8:\n",
    "            img = (img * 255).astype(np.uint8)\n",
    "\n",
    "        # Convert to LAB\n",
    "        lab = cv2.cvtColor(img, cv2.COLOR_RGB2LAB).astype(np.float32)\n",
    "\n",
    "        # Normalize each channel\n",
    "        for i in range(3):\n",
    "            channel = lab[:, :, i]\n",
    "            channel_mean = channel.mean()\n",
    "            channel_std = channel.std() + 1e-6\n",
    "\n",
    "            # Normalize to target distribution\n",
    "            lab[:, :, i] = ((channel - channel_mean) / channel_std) * self.target_stds[\n",
    "                i\n",
    "            ] + self.target_means[i]\n",
    "\n",
    "        # Clip and convert back\n",
    "        lab = np.clip(lab, 0, 255).astype(np.uint8)\n",
    "        normalized = cv2.cvtColor(lab, cv2.COLOR_LAB2RGB)\n",
    "\n",
    "        return normalized\n",
    "\n",
    "\n",
    "class ReinhardNormalizer:\n",
    "    \"\"\"Reinhard stain normalization.\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        # Target statistics (can be computed from a reference image)\n",
    "        self.target_means = None\n",
    "        self.target_stds = None\n",
    "        self._set_default_target()\n",
    "\n",
    "    def _set_default_target(self):\n",
    "        \"\"\"Set default target statistics for H&E.\"\"\"\n",
    "        self.target_means = np.array([180.0, 135.0, 165.0])\n",
    "        self.target_stds = np.array([25.0, 15.0, 20.0])\n",
    "\n",
    "    def fit(self, target_img: np.ndarray):\n",
    "        \"\"\"Fit normalizer to a target image.\"\"\"\n",
    "        lab = cv2.cvtColor(target_img, cv2.COLOR_RGB2LAB).astype(np.float32)\n",
    "        self.target_means = np.array([lab[:, :, i].mean() for i in range(3)])\n",
    "        self.target_stds = np.array([lab[:, :, i].std() for i in range(3)])\n",
    "\n",
    "    def normalize(self, img: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"Normalize image to target statistics.\"\"\"\n",
    "        lab = cv2.cvtColor(img, cv2.COLOR_RGB2LAB).astype(np.float32)\n",
    "\n",
    "        for i in range(3):\n",
    "            src_mean = lab[:, :, i].mean()\n",
    "            src_std = lab[:, :, i].std() + 1e-6\n",
    "            lab[:, :, i] = ((lab[:, :, i] - src_mean) / src_std) * self.target_stds[\n",
    "                i\n",
    "            ] + self.target_means[i]\n",
    "\n",
    "        lab = np.clip(lab, 0, 255).astype(np.uint8)\n",
    "        return cv2.cvtColor(lab, cv2.COLOR_LAB2RGB)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# Data Module\n",
    "# ============================================================================\n",
    "class PathologyDataModule(LightningDataModule):\n",
    "    \"\"\"DataModule for pathology images.\"\"\"\n",
    "\n",
    "    def __init__(self, config: Config = Config()):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        self.label_encoder = LabelEncoder()\n",
    "        self.label_encoder.fit(config.CLASSES)\n",
    "\n",
    "        # Transforms\n",
    "        self.train_transform = transforms.Compose(\n",
    "            [\n",
    "                transforms.Resize((config.IMG_SIZE, config.IMG_SIZE)),\n",
    "                transforms.RandomHorizontalFlip(p=0.5),\n",
    "                transforms.RandomVerticalFlip(p=0.5),\n",
    "                transforms.RandomRotation(degrees=90),\n",
    "                transforms.ColorJitter(\n",
    "                    brightness=0.2, contrast=0.2, saturation=0.2, hue=0.05\n",
    "                ),\n",
    "                transforms.RandomAffine(\n",
    "                    degrees=0, translate=(0.1, 0.1), scale=(0.9, 1.1)\n",
    "                ),\n",
    "                transforms.ToTensor(),\n",
    "                transforms.Normalize(\n",
    "                    mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]\n",
    "                ),\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        self.val_transform = transforms.Compose(\n",
    "            [\n",
    "                transforms.Resize((config.IMG_SIZE, config.IMG_SIZE)),\n",
    "                transforms.ToTensor(),\n",
    "                transforms.Normalize(\n",
    "                    mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]\n",
    "                ),\n",
    "            ]\n",
    "        )\n",
    "\n",
    "    def setup(self, stage: Optional[str] = None):\n",
    "        if stage == \"fit\" or stage is None:\n",
    "            labels_df = pd.read_csv(self.config.TRAIN_LABELS_PATH)\n",
    "\n",
    "            def clean_idx(idx):\n",
    "                idx = str(idx)\n",
    "                if idx.startswith(\"img_\"):\n",
    "                    idx = idx[4:]\n",
    "                if idx.endswith(\".png\"):\n",
    "                    idx = idx[:-4]\n",
    "                return idx\n",
    "\n",
    "            labels_df[\"sample_index\"] = labels_df[\"sample_index\"].apply(clean_idx)\n",
    "\n",
    "            train_df, val_df = train_test_split(\n",
    "                labels_df,\n",
    "                test_size=self.config.VAL_SPLIT,\n",
    "                stratify=labels_df[\"label\"],\n",
    "                random_state=self.config.RANDOM_SEED,\n",
    "            )\n",
    "\n",
    "            self.train_dataset = PathologyDataset(\n",
    "                data_dir=self.config.TRAIN_DATA_DIR,\n",
    "                labels_df=train_df,\n",
    "                transform=self.train_transform,\n",
    "                img_size=self.config.IMG_SIZE,\n",
    "                use_mask=self.config.USE_MASK,\n",
    "                use_patches=self.config.USE_PATCHES,\n",
    "                patch_size=self.config.PATCH_SIZE,\n",
    "                num_patches=self.config.NUM_PATCHES,\n",
    "                use_stain_norm=self.config.USE_STAIN_NORMALIZATION,\n",
    "                label_encoder=self.label_encoder,\n",
    "            )\n",
    "\n",
    "            self.val_dataset = PathologyDataset(\n",
    "                data_dir=self.config.TRAIN_DATA_DIR,\n",
    "                labels_df=val_df,\n",
    "                transform=self.val_transform,\n",
    "                img_size=self.config.IMG_SIZE,\n",
    "                use_mask=self.config.USE_MASK,\n",
    "                use_patches=self.config.USE_PATCHES,\n",
    "                patch_size=self.config.PATCH_SIZE,\n",
    "                num_patches=self.config.NUM_PATCHES,\n",
    "                use_stain_norm=self.config.USE_STAIN_NORMALIZATION,\n",
    "                label_encoder=self.label_encoder,\n",
    "            )\n",
    "\n",
    "            # Calculate class weights for balanced sampling\n",
    "            class_counts = train_df[\"label\"].value_counts()\n",
    "            weights = 1.0 / class_counts[train_df[\"label\"].values].values\n",
    "            self.sample_weights = torch.DoubleTensor(weights)\n",
    "\n",
    "        if stage == \"test\" or stage == \"predict\" or stage is None:\n",
    "            self.test_dataset = PathologyDataset(\n",
    "                data_dir=self.config.TEST_DATA_DIR,\n",
    "                transform=self.val_transform,\n",
    "                img_size=self.config.IMG_SIZE,\n",
    "                use_mask=self.config.USE_MASK,\n",
    "                use_patches=self.config.USE_PATCHES,\n",
    "                patch_size=self.config.PATCH_SIZE,\n",
    "                num_patches=self.config.NUM_PATCHES,\n",
    "                use_stain_norm=self.config.USE_STAIN_NORMALIZATION,\n",
    "                is_test=True,\n",
    "                label_encoder=self.label_encoder,\n",
    "            )\n",
    "\n",
    "    def train_dataloader(self) -> DataLoader:\n",
    "        sampler = WeightedRandomSampler(\n",
    "            self.sample_weights, len(self.sample_weights), replacement=True\n",
    "        )\n",
    "\n",
    "        return DataLoader(\n",
    "            self.train_dataset,\n",
    "            batch_size=self.config.BATCH_SIZE,\n",
    "            sampler=sampler,\n",
    "            num_workers=self.config.NUM_WORKERS,\n",
    "            pin_memory=True,\n",
    "            drop_last=True,\n",
    "        )\n",
    "\n",
    "    def val_dataloader(self) -> DataLoader:\n",
    "        return DataLoader(\n",
    "            self.val_dataset,\n",
    "            batch_size=self.config.BATCH_SIZE,\n",
    "            shuffle=False,\n",
    "            num_workers=self.config.NUM_WORKERS,\n",
    "            pin_memory=True,\n",
    "        )\n",
    "\n",
    "    def test_dataloader(self) -> DataLoader:\n",
    "        return DataLoader(\n",
    "            self.test_dataset,\n",
    "            batch_size=self.config.BATCH_SIZE,\n",
    "            shuffle=False,\n",
    "            num_workers=self.config.NUM_WORKERS,\n",
    "            pin_memory=True,\n",
    "        )\n",
    "\n",
    "    def predict_dataloader(self) -> DataLoader:\n",
    "        return self.test_dataloader()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# Model with Multi-Instance Learning (MIL) for patches\n",
    "# ============================================================================\n",
    "class AttentionMIL(nn.Module):\n",
    "    \"\"\"Attention-based Multiple Instance Learning pooling.\"\"\"\n",
    "\n",
    "    def __init__(self, feature_dim: int, hidden_dim: int = 256):\n",
    "        super().__init__()\n",
    "        self.attention = nn.Sequential(\n",
    "            nn.Linear(feature_dim, hidden_dim), nn.Tanh(), nn.Linear(hidden_dim, 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: [batch, num_instances, features]\n",
    "        Returns:\n",
    "            pooled: [batch, features]\n",
    "            attention_weights: [batch, num_instances]\n",
    "        \"\"\"\n",
    "        # Compute attention scores\n",
    "        attn_scores = self.attention(x)  # [batch, num_instances, 1]\n",
    "        attn_weights = F.softmax(attn_scores, dim=1)  # [batch, num_instances, 1]\n",
    "\n",
    "        # Weighted sum\n",
    "        pooled = torch.sum(x * attn_weights, dim=1)  # [batch, features]\n",
    "\n",
    "        return pooled, attn_weights.squeeze(-1)\n",
    "\n",
    "\n",
    "class PathologyClassifier(LightningModule):\n",
    "    \"\"\"Classifier optimized for histopathology with optional MIL.\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        num_classes: int = Config.NUM_CLASSES,\n",
    "        model_name: str = Config.MODEL_NAME,\n",
    "        pretrained: bool = Config.PRETRAINED,\n",
    "        learning_rate: float = Config.LEARNING_RATE,\n",
    "        weight_decay: float = Config.WEIGHT_DECAY,\n",
    "        class_names: List[str] = Config.CLASSES,\n",
    "        use_patches: bool = Config.USE_PATCHES,\n",
    "        num_patches: int = Config.NUM_PATCHES,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.save_hyperparameters()\n",
    "\n",
    "        self.num_classes = num_classes\n",
    "        self.learning_rate = learning_rate\n",
    "        self.weight_decay = weight_decay\n",
    "        self.class_names = class_names\n",
    "        self.use_patches = use_patches\n",
    "        self.num_patches = num_patches\n",
    "\n",
    "        # Create backbone\n",
    "        self.backbone, self.feature_dim = self._create_backbone(model_name, pretrained)\n",
    "\n",
    "        # MIL attention pooling (for patch-based)\n",
    "        if use_patches:\n",
    "            self.mil_attention = AttentionMIL(self.feature_dim)\n",
    "\n",
    "        # Classification head\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(self.feature_dim, 512),\n",
    "            nn.BatchNorm1d(512),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(512, num_classes),\n",
    "        )\n",
    "\n",
    "        # Loss function\n",
    "        self.criterion = nn.CrossEntropyLoss(label_smoothing=0.1)\n",
    "\n",
    "        # Metrics storage\n",
    "        self.validation_step_outputs = []\n",
    "\n",
    "    def _create_backbone(\n",
    "        self, model_name: str, pretrained: bool\n",
    "    ) -> Tuple[nn.Module, int]:\n",
    "        \"\"\"Create backbone model.\"\"\"\n",
    "\n",
    "        if model_name == \"resnet50\":\n",
    "            weights = models.ResNet50_Weights.IMAGENET1K_V2 if pretrained else None\n",
    "            model = models.resnet50(weights=weights)\n",
    "            feature_dim = 2048\n",
    "            backbone = nn.Sequential(*list(model.children())[:-1], nn.Flatten())\n",
    "\n",
    "        elif model_name == \"efficientnet_b3\":\n",
    "            weights = (\n",
    "                models.EfficientNet_B3_Weights.IMAGENET1K_V1 if pretrained else None\n",
    "            )\n",
    "            model = models.efficientnet_b3(weights=weights)\n",
    "            feature_dim = 1536\n",
    "            backbone = nn.Sequential(\n",
    "                model.features, nn.AdaptiveAvgPool2d(1), nn.Flatten()\n",
    "            )\n",
    "\n",
    "        elif model_name == \"efficientnet_b4\":\n",
    "            weights = (\n",
    "                models.EfficientNet_B4_Weights.IMAGENET1K_V1 if pretrained else None\n",
    "            )\n",
    "            model = models.efficientnet_b4(weights=weights)\n",
    "            feature_dim = 1792\n",
    "            backbone = nn.Sequential(\n",
    "                model.features, nn.AdaptiveAvgPool2d(1), nn.Flatten()\n",
    "            )\n",
    "\n",
    "        elif model_name == \"convnext_small\":\n",
    "            weights = (\n",
    "                models.ConvNeXt_Small_Weights.IMAGENET1K_V1 if pretrained else None\n",
    "            )\n",
    "            model = models.convnext_small(weights=weights)\n",
    "            feature_dim = 768\n",
    "            backbone = nn.Sequential(\n",
    "                model.features, nn.AdaptiveAvgPool2d(1), nn.Flatten()\n",
    "            )\n",
    "\n",
    "        elif model_name == \"densenet121\":\n",
    "            weights = models.DenseNet121_Weights.IMAGENET1K_V1 if pretrained else None\n",
    "            model = models.densenet121(weights=weights)\n",
    "            feature_dim = 1024\n",
    "            backbone = nn.Sequential(\n",
    "                model.features,\n",
    "                nn.ReLU(inplace=True),\n",
    "                nn.AdaptiveAvgPool2d(1),\n",
    "                nn.Flatten(),\n",
    "            )\n",
    "\n",
    "        else:\n",
    "            raise ValueError(f\"Unsupported model: {model_name}\")\n",
    "\n",
    "        return backbone, feature_dim\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        if self.use_patches:\n",
    "            # x shape: [batch, num_patches, C, H, W]\n",
    "            batch_size, num_patches, C, H, W = x.shape\n",
    "\n",
    "            # Flatten batch and patches\n",
    "            x = x.view(batch_size * num_patches, C, H, W)\n",
    "\n",
    "            # Extract features\n",
    "            features = self.backbone(x)  # [batch*num_patches, feature_dim]\n",
    "\n",
    "            # Reshape back\n",
    "            features = features.view(\n",
    "                batch_size, num_patches, -1\n",
    "            )  # [batch, num_patches, feature_dim]\n",
    "\n",
    "            # MIL attention pooling\n",
    "            pooled, _ = self.mil_attention(features)  # [batch, feature_dim]\n",
    "\n",
    "            # Classify\n",
    "            logits = self.classifier(pooled)\n",
    "        else:\n",
    "            # x shape: [batch, C, H, W]\n",
    "            features = self.backbone(x)\n",
    "            logits = self.classifier(features)\n",
    "\n",
    "        return logits\n",
    "\n",
    "    def training_step(\n",
    "        self, batch: Tuple[torch.Tensor, torch.Tensor], batch_idx: int\n",
    "    ) -> torch.Tensor:\n",
    "        images, labels = batch\n",
    "        logits = self(images)\n",
    "        loss = self.criterion(logits, labels)\n",
    "\n",
    "        preds = torch.argmax(logits, dim=1)\n",
    "        acc = (preds == labels).float().mean()\n",
    "\n",
    "        self.log(\"train_loss\", loss, on_step=True, on_epoch=True, prog_bar=True)\n",
    "        self.log(\"train_acc\", acc, on_step=True, on_epoch=True, prog_bar=True)\n",
    "\n",
    "        return loss\n",
    "\n",
    "    def validation_step(\n",
    "        self, batch: Tuple[torch.Tensor, torch.Tensor], batch_idx: int\n",
    "    ) -> dict:\n",
    "        images, labels = batch\n",
    "        logits = self(images)\n",
    "        loss = self.criterion(logits, labels)\n",
    "\n",
    "        preds = torch.argmax(logits, dim=1)\n",
    "        acc = (preds == labels).float().mean()\n",
    "\n",
    "        self.log(\"val_loss\", loss, on_step=False, on_epoch=True, prog_bar=True)\n",
    "        self.log(\"val_acc\", acc, on_step=False, on_epoch=True, prog_bar=True)\n",
    "\n",
    "        self.validation_step_outputs.append({\"preds\": preds, \"labels\": labels})\n",
    "\n",
    "        return {\"val_loss\": loss, \"val_acc\": acc}\n",
    "\n",
    "    def on_validation_epoch_end(self):\n",
    "        all_preds = torch.cat([x[\"preds\"] for x in self.validation_step_outputs])\n",
    "        all_labels = torch.cat([x[\"labels\"] for x in self.validation_step_outputs])\n",
    "\n",
    "        for i, class_name in enumerate(self.class_names):\n",
    "            mask = all_labels == i\n",
    "            if mask.sum() > 0:\n",
    "                class_acc = (all_preds[mask] == all_labels[mask]).float().mean()\n",
    "                self.log(f\"val_acc_{class_name}\", class_acc, on_epoch=True)\n",
    "\n",
    "        self.validation_step_outputs.clear()\n",
    "\n",
    "    def predict_step(\n",
    "        self, batch: Tuple[torch.Tensor, str], batch_idx: int\n",
    "    ) -> Tuple[torch.Tensor, List[str]]:\n",
    "        images, sample_indices = batch\n",
    "        logits = self(images)\n",
    "        preds = torch.argmax(logits, dim=1)\n",
    "        return preds, sample_indices\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.AdamW(\n",
    "            self.parameters(), lr=self.learning_rate, weight_decay=self.weight_decay\n",
    "        )\n",
    "\n",
    "        scheduler = torch.optim.lr_scheduler.CosineAnnealingWarmRestarts(\n",
    "            optimizer, T_0=10, T_mult=2, eta_min=1e-7\n",
    "        )\n",
    "\n",
    "        return {\n",
    "            \"optimizer\": optimizer,\n",
    "            \"lr_scheduler\": {\"scheduler\": scheduler, \"interval\": \"epoch\"},\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# Training and Inference\n",
    "# ============================================================================\n",
    "def train_model(\n",
    "    config: Config = Config(),\n",
    ") -> Tuple[PathologyClassifier, PathologyDataModule, Trainer]:\n",
    "    \"\"\"Train the pathology classifier.\"\"\"\n",
    "\n",
    "    pl.seed_everything(config.RANDOM_SEED)\n",
    "\n",
    "    data_module = PathologyDataModule(config)\n",
    "\n",
    "    model = PathologyClassifier(\n",
    "        num_classes=config.NUM_CLASSES,\n",
    "        model_name=config.MODEL_NAME,\n",
    "        pretrained=config.PRETRAINED,\n",
    "        learning_rate=config.LEARNING_RATE,\n",
    "        weight_decay=config.WEIGHT_DECAY,\n",
    "        class_names=config.CLASSES,\n",
    "        use_patches=config.USE_PATCHES,\n",
    "        num_patches=config.NUM_PATCHES,\n",
    "    )\n",
    "\n",
    "    checkpoint_callback = ModelCheckpoint(\n",
    "        dirpath=\"checkpoints\",\n",
    "        filename=\"pathology-{epoch:02d}-{val_acc:.4f}\",\n",
    "        monitor=\"val_acc\",\n",
    "        mode=\"max\",\n",
    "        save_top_k=3,\n",
    "        save_last=True,\n",
    "    )\n",
    "\n",
    "    early_stopping = EarlyStopping(monitor=\"val_acc\", patience=15, mode=\"max\")\n",
    "\n",
    "    lr_monitor = LearningRateMonitor(logging_interval=\"epoch\")\n",
    "\n",
    "    logger = TensorBoardLogger(\"logs\", name=\"pathology\")\n",
    "\n",
    "    trainer = Trainer(\n",
    "        max_epochs=config.MAX_EPOCHS,\n",
    "        accelerator=\"auto\",\n",
    "        devices=\"auto\",\n",
    "        precision=\"16-mixed\",\n",
    "        callbacks=[checkpoint_callback, early_stopping, lr_monitor],\n",
    "        logger=logger,\n",
    "        gradient_clip_val=1.0,\n",
    "        accumulate_grad_batches=16,\n",
    "        deterministic=True,\n",
    "    )\n",
    "\n",
    "    trainer.fit(model, datamodule=data_module)\n",
    "\n",
    "    return model, data_module, trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_and_save(\n",
    "    model: PathologyClassifier,\n",
    "    data_module: PathologyDataModule,\n",
    "    trainer: Trainer,\n",
    "    output_path: str = Config.OUTPUT_PATH,\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"Run inference and save predictions.\"\"\"\n",
    "\n",
    "    data_module.setup(stage=\"predict\")\n",
    "    predictions = trainer.predict(model, datamodule=data_module)\n",
    "\n",
    "    all_preds = []\n",
    "    all_indices = []\n",
    "\n",
    "    for preds, indices in predictions:\n",
    "        all_preds.extend(preds.cpu().numpy())\n",
    "        all_indices.extend(indices)\n",
    "\n",
    "    label_encoder = data_module.label_encoder\n",
    "    predicted_labels = label_encoder.inverse_transform(all_preds)\n",
    "\n",
    "    results_df = pd.DataFrame({\"sample_index\": all_indices, \"label\": predicted_labels})\n",
    "\n",
    "    results_df = results_df.sort_values(\"sample_index\").reset_index(drop=True)\n",
    "    results_df.to_csv(output_path, index=False)\n",
    "\n",
    "    print(f\"Predictions saved to: {output_path}\")\n",
    "\n",
    "    return results_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Main training and inference pipeline.\"\"\"\n",
    "\n",
    "config = Config()\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"Pathology-Optimized Molecular Subtype Classification\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Model: {config.MODEL_NAME}\")\n",
    "print(f\"Image Size: {config.IMG_SIZE}\")\n",
    "print(f\"Use Patches: {config.USE_PATCHES}\")\n",
    "if config.USE_PATCHES:\n",
    "    print(f\"  - Patch Size: {config.PATCH_SIZE}\")\n",
    "    print(f\"  - Num Patches: {config.NUM_PATCHES}\")\n",
    "print(f\"Stain Normalization: {config.USE_STAIN_NORMALIZATION}\")\n",
    "print(f\"Batch Size: {config.BATCH_SIZE}\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "print(\"\\n[1/2] Training model...\")\n",
    "model, data_module, trainer = train_model(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n[2/2] Running inference on test set...\")\n",
    "results_df = predict_and_save(model, data_module, trainer, config.OUTPUT_PATH)\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"Prediction Summary\")\n",
    "print(\"=\" * 60)\n",
    "print(results_df[\"label\"].value_counts())\n",
    "print(\"\\nDone!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
