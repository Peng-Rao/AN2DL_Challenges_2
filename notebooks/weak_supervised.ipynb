{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "import cv2\n",
    "import lightning as L\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from PIL import Image\n",
    "from torch.utils.data import Dataset\n",
    "from torchvision import transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TissueExtractor:\n",
    "    \"\"\"\n",
    "    Extract patches from images using existing masks.\n",
    "    Designed for workflow where ground truth masks are already available.\n",
    "\n",
    "    Args:\n",
    "        patch_size: Size of the square patch to extract.\n",
    "        min_tissue_ratio: Minimum ratio of tissue pixels required in a patch.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, patch_size: int = 224, min_tissue_ratio: float = 0.05):\n",
    "        self.patch_size = patch_size\n",
    "        self.min_tissue_ratio = min_tissue_ratio\n",
    "\n",
    "    def get_valid_patches(\n",
    "        self,\n",
    "        img: np.ndarray,\n",
    "        mask: np.ndarray,\n",
    "        num_patches: int = 8,\n",
    "        strategy: str = \"random\",  # 'random' or 'grid'\n",
    "        stride: int = None,  # For grid strategy: step size between patches\n",
    "        shuffle: bool = True,  # For grid strategy: shuffle valid patches before selecting\n",
    "    ) -> Tuple[List[np.ndarray], List[np.ndarray]]:\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            img: RGB image (H, W, 3)\n",
    "            mask: Binary or Index mask (H, W). Assumes tissue > 0.\n",
    "            num_patches: Number of patches to extract per image.\n",
    "            strategy: 'random' samples points from mask; 'grid' slides across image.\n",
    "            stride: Step size for grid strategy. Defaults to patch_size (no overlap).\n",
    "            shuffle: Whether to shuffle grid patches before selecting (for diversity).\n",
    "\n",
    "        Returns:\n",
    "            images: List of RGB patches\n",
    "            masks: List of corresponding Mask patches\n",
    "        \"\"\"\n",
    "\n",
    "        h, w = img.shape[:2]\n",
    "\n",
    "        # if mask has 3 channels, convert to single channel\n",
    "        if len(mask.shape) == 3:\n",
    "            mask = mask[:, :, 0]\n",
    "\n",
    "        # Find all tissue pixel indices\n",
    "        tissue_indices = np.where(mask > 0)\n",
    "\n",
    "        # If no tissue found, return empty lists\n",
    "        if len(tissue_indices[0]) == 0:\n",
    "            print(\"Warning: No tissue found in mask!\")\n",
    "            return [], []\n",
    "\n",
    "        if strategy == \"random\":\n",
    "            return self._extract_random(img, mask, tissue_indices, num_patches, h, w)\n",
    "        elif strategy == \"grid\":\n",
    "            return self._extract_grid(img, mask, num_patches, h, w, stride, shuffle)\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown strategy: {strategy}. Use 'random' or 'grid'.\")\n",
    "\n",
    "    def _extract_random(\n",
    "        self,\n",
    "        img: np.ndarray,\n",
    "        mask: np.ndarray,\n",
    "        tissue_indices: Tuple[np.ndarray, np.ndarray],\n",
    "        num_patches: int,\n",
    "        h: int,\n",
    "        w: int,\n",
    "    ) -> Tuple[List[np.ndarray], List[np.ndarray]]:\n",
    "        \"\"\"Random sampling strategy: randomly select tissue pixels as patch centers.\"\"\"\n",
    "\n",
    "        patches_img = []\n",
    "        patches_mask = []\n",
    "\n",
    "        # Protection mechanism: limit the number of attempts to avoid infinite loop\n",
    "        attempts = 0\n",
    "        max_attempts = num_patches * 50\n",
    "\n",
    "        while len(patches_img) < num_patches and attempts < max_attempts:\n",
    "            attempts += 1\n",
    "\n",
    "            # Randomly select a tissue pixel as center\n",
    "            idx = np.random.randint(len(tissue_indices[0]))\n",
    "            cy, cx = tissue_indices[0][idx], tissue_indices[1][idx]\n",
    "\n",
    "            # Calculate top-left and bottom-right corners to ensure no out-of-bounds\n",
    "            half_size = self.patch_size // 2\n",
    "\n",
    "            # Simple center cropping logic\n",
    "            y_min = int(cy - half_size)\n",
    "            x_min = int(cx - half_size)\n",
    "            y_max = y_min + self.patch_size\n",
    "            x_max = x_min + self.patch_size\n",
    "\n",
    "            # Boundary check: if the patch goes out of image bounds, skip and retry\n",
    "            if y_min < 0 or x_min < 0 or y_max > h or x_max > w:\n",
    "                continue\n",
    "\n",
    "            # Extract Mask Patch for validation\n",
    "            mask_patch = mask[y_min:y_max, x_min:x_max]\n",
    "\n",
    "            # Calculate the proportion of non-zero pixels\n",
    "            current_ratio = np.count_nonzero(mask_patch) / mask_patch.size\n",
    "\n",
    "            if current_ratio >= self.min_tissue_ratio:\n",
    "                # Extract Image Patch\n",
    "                img_patch = img[y_min:y_max, x_min:x_max]\n",
    "\n",
    "                patches_img.append(img_patch)\n",
    "                patches_mask.append(mask_patch)\n",
    "\n",
    "        return patches_img, patches_mask\n",
    "\n",
    "    def _extract_grid(\n",
    "        self,\n",
    "        img: np.ndarray,\n",
    "        mask: np.ndarray,\n",
    "        num_patches: int,\n",
    "        h: int,\n",
    "        w: int,\n",
    "        stride: int = None,\n",
    "        shuffle: bool = True,\n",
    "    ) -> Tuple[List[np.ndarray], List[np.ndarray]]:\n",
    "        \"\"\"\n",
    "        Grid strategy: systematically slide across the image.\n",
    "\n",
    "        Args:\n",
    "            img: RGB image\n",
    "            mask: Binary mask\n",
    "            num_patches: Maximum number of patches to extract\n",
    "            h, w: Image dimensions\n",
    "            stride: Step size between patches. Defaults to patch_size (no overlap).\n",
    "            shuffle: If True, shuffle valid patches before selecting to add diversity.\n",
    "        \"\"\"\n",
    "\n",
    "        if stride is None:\n",
    "            stride = self.patch_size  # No overlap by default\n",
    "\n",
    "        # Calculate all valid grid positions\n",
    "        y_positions = list(range(0, h - self.patch_size + 1, stride))\n",
    "        x_positions = list(range(0, w - self.patch_size + 1, stride))\n",
    "\n",
    "        # Collect all valid patches first\n",
    "        valid_patches = []  # List of (y_min, x_min, tissue_ratio)\n",
    "\n",
    "        for y_min in y_positions:\n",
    "            for x_min in x_positions:\n",
    "                y_max = y_min + self.patch_size\n",
    "                x_max = x_min + self.patch_size\n",
    "\n",
    "                # Extract mask patch for validation\n",
    "                mask_patch = mask[y_min:y_max, x_min:x_max]\n",
    "\n",
    "                # Calculate tissue ratio\n",
    "                tissue_ratio = np.count_nonzero(mask_patch) / mask_patch.size\n",
    "\n",
    "                if tissue_ratio >= self.min_tissue_ratio:\n",
    "                    valid_patches.append((y_min, x_min, tissue_ratio))\n",
    "\n",
    "        # Shuffle or sort based on preference\n",
    "        if shuffle:\n",
    "            np.random.shuffle(valid_patches)\n",
    "        else:\n",
    "            # Sort by tissue ratio (descending) to prioritize patches with more tissue\n",
    "            valid_patches.sort(key=lambda x: x[2], reverse=True)\n",
    "\n",
    "        # Extract the requested number of patches\n",
    "        patches_img = []\n",
    "        patches_mask = []\n",
    "\n",
    "        for y_min, x_min, _ in valid_patches[:num_patches]:\n",
    "            y_max = y_min + self.patch_size\n",
    "            x_max = x_min + self.patch_size\n",
    "\n",
    "            img_patch = img[y_min:y_max, x_min:x_max]\n",
    "            mask_patch = mask[y_min:y_max, x_min:x_max]\n",
    "\n",
    "            patches_img.append(img_patch)\n",
    "            patches_mask.append(mask_patch)\n",
    "\n",
    "        return patches_img, patches_mask\n",
    "\n",
    "    def get_all_valid_patches(\n",
    "        self,\n",
    "        img: np.ndarray,\n",
    "        mask: np.ndarray,\n",
    "        stride: int = None,\n",
    "    ) -> Tuple[List[np.ndarray], List[np.ndarray], List[Tuple[int, int]]]:\n",
    "        \"\"\"\n",
    "        Extract ALL valid patches from the image using grid strategy.\n",
    "        Useful for inference or when you need complete coverage.\n",
    "\n",
    "        Args:\n",
    "            img: RGB image (H, W, 3)\n",
    "            mask: Binary or Index mask (H, W)\n",
    "            stride: Step size between patches. Defaults to patch_size.\n",
    "\n",
    "        Returns:\n",
    "            images: List of RGB patches\n",
    "            masks: List of corresponding mask patches\n",
    "            coordinates: List of (y_min, x_min) for each patch\n",
    "        \"\"\"\n",
    "        h, w = img.shape[:2]\n",
    "\n",
    "        if len(mask.shape) == 3:\n",
    "            mask = mask[:, :, 0]\n",
    "\n",
    "        if stride is None:\n",
    "            stride = self.patch_size\n",
    "\n",
    "        y_positions = list(range(0, h - self.patch_size + 1, stride))\n",
    "        x_positions = list(range(0, w - self.patch_size + 1, stride))\n",
    "\n",
    "        patches_img = []\n",
    "        patches_mask = []\n",
    "        coordinates = []\n",
    "\n",
    "        for y_min in y_positions:\n",
    "            for x_min in x_positions:\n",
    "                y_max = y_min + self.patch_size\n",
    "                x_max = x_min + self.patch_size\n",
    "\n",
    "                mask_patch = mask[y_min:y_max, x_min:x_max]\n",
    "                tissue_ratio = np.count_nonzero(mask_patch) / mask_patch.size\n",
    "\n",
    "                if tissue_ratio >= self.min_tissue_ratio:\n",
    "                    img_patch = img[y_min:y_max, x_min:x_max]\n",
    "                    patches_img.append(img_patch)\n",
    "                    patches_mask.append(mask_patch)\n",
    "                    coordinates.append((y_min, x_min))\n",
    "\n",
    "        return patches_img, patches_mask, coordinates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PathologyDataset(Dataset):\n",
    "    \"\"\"Dataset optimized for histopathology images.\n",
    "\n",
    "    Args:\n",
    "        data_dir: Directory containing images and masks.\n",
    "        labels_df: DataFrame with 'sample_index' and 'label' columns for training/validation.\n",
    "        transform: torchvision transforms to apply to images.\n",
    "        img_size: Target size to resize images to (img_size x img_size). If using patches, this is ignored.\n",
    "        use_mask: Whether to use existing masks for tissue extraction.\n",
    "        use_patches: Whether to load images as patches.\n",
    "        patch_size: Size of each patch if using patches.\n",
    "        num_patches: Number of patches to extract per image.\n",
    "        patch_strategy: Strategy for patch extraction ('random' or 'grid').\n",
    "        min_tissue_ratio: Minimum tissue ratio for valid patches.\n",
    "        use_stain_norm: Whether to apply stain normalization.\n",
    "        is_test: Whether the dataset is for testing (no labels).\n",
    "        label_encoder: Optional LabelEncoder for encoding labels.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        data_dir: str,\n",
    "        labels_df: Optional[pd.DataFrame] = None,\n",
    "        transform: Optional[transforms.Compose] = None,\n",
    "        use_mask: bool = True,\n",
    "        use_patches: bool = False,\n",
    "        patch_size: int = 224,\n",
    "        num_patches: int = 8,\n",
    "        patch_strategy: str = \"random\",\n",
    "        min_tissue_ratio: float = 0.05,\n",
    "        use_stain_norm: bool = True,\n",
    "        is_test: bool = False,\n",
    "        label_encoder: Optional[LabelEncoder] = None,\n",
    "    ):\n",
    "        self.data_dir = Path(data_dir)\n",
    "        self.transform = transform\n",
    "        self.use_mask = use_mask\n",
    "        self.use_patches = use_patches\n",
    "        self.patch_size = patch_size\n",
    "        self.num_patches = num_patches\n",
    "        self.patch_strategy = patch_strategy\n",
    "        self.use_stain_norm = use_stain_norm\n",
    "        self.is_test = is_test\n",
    "        self.label_encoder = label_encoder\n",
    "\n",
    "        # Initialize helpers\n",
    "        self.tissue_extractor = TissueExtractor(\n",
    "            patch_size=patch_size,\n",
    "            min_tissue_ratio=min_tissue_ratio,\n",
    "        )\n",
    "        self.stain_normalizer = None if use_stain_norm else None\n",
    "\n",
    "        if is_test:\n",
    "            self.samples = self._get_test_samples()\n",
    "            self.labels = None\n",
    "            self.encoded_labels = None\n",
    "        else:\n",
    "            if labels_df is None:\n",
    "                raise ValueError(\"labels_df must be provided for training/validation.\")\n",
    "\n",
    "            self.samples = [\n",
    "                self._clean_sample_idx(str(idx))\n",
    "                for idx in labels_df[\"sample_index\"].tolist()\n",
    "            ]\n",
    "            self.labels = labels_df[\"label\"].tolist()\n",
    "\n",
    "            if self.label_encoder is None:\n",
    "                self.label_encoder = LabelEncoder()\n",
    "                self.label_encoder.fit(\n",
    "                    [\"Luminal A\", \"Luminal B\", \"HER2(+)\", \"Triple negative\"]\n",
    "                )\n",
    "            self.encoded_labels = self.label_encoder.transform(self.labels)\n",
    "\n",
    "    def _clean_sample_idx(self, sample_idx: str) -> str:\n",
    "        \"\"\"Clean sample index by removing prefix and suffix.\"\"\"\n",
    "        sample_idx = str(sample_idx)\n",
    "        if sample_idx.startswith(\"img_\"):\n",
    "            sample_idx = sample_idx[4:]\n",
    "        if sample_idx.endswith(\".png\"):\n",
    "            sample_idx = sample_idx[:-4]\n",
    "        return sample_idx\n",
    "\n",
    "    def _get_test_samples(self) -> List[str]:\n",
    "        \"\"\"Get list of sample indices from test directory.\"\"\"\n",
    "        samples = []\n",
    "        for f in sorted(self.data_dir.glob(\"img_*.png\")):\n",
    "            sample_idx = self._clean_sample_idx(f.stem)\n",
    "            samples.append(sample_idx)\n",
    "        return samples\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return len(self.samples)\n",
    "\n",
    "    def _load_image_and_mask(\n",
    "        self, sample_idx: str\n",
    "    ) -> Tuple[np.ndarray, Optional[np.ndarray]]:\n",
    "        \"\"\"Load image and optionally its mask.\"\"\"\n",
    "        img_path = self.data_dir / f\"img_{sample_idx}.png\"\n",
    "        img = np.array(Image.open(img_path).convert(\"RGB\"))\n",
    "\n",
    "        mask = None\n",
    "        if self.use_mask:\n",
    "            mask_path = self.data_dir / f\"mask_{sample_idx}.png\"\n",
    "            if mask_path.exists():\n",
    "                mask = np.array(Image.open(mask_path).convert(\"L\"))\n",
    "\n",
    "        return img, mask\n",
    "\n",
    "    def _crop_to_tissue_bbox(self, img: np.ndarray, mask: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"Crop image to bounding box of tissue region.\"\"\"\n",
    "        # Find bounding box of tissue\n",
    "        rows = np.any(mask > 0, axis=1)\n",
    "        cols = np.any(mask > 0, axis=0)\n",
    "\n",
    "        if not rows.any() or not cols.any():\n",
    "            return img  # No tissue found, return original\n",
    "\n",
    "        y_min, y_max = np.where(rows)[0][[0, -1]]\n",
    "        x_min, x_max = np.where(cols)[0][[0, -1]]\n",
    "\n",
    "        # Add small padding\n",
    "        padding = 10\n",
    "        y_min = max(0, y_min - padding)\n",
    "        y_max = min(img.shape[0], y_max + padding)\n",
    "        x_min = max(0, x_min - padding)\n",
    "        x_max = min(img.shape[1], x_max + padding)\n",
    "\n",
    "        return img[y_min:y_max, x_min:x_max]\n",
    "\n",
    "    def _apply_stain_normalization(self, img: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"Apply stain normalization to image.\"\"\"\n",
    "        if self.use_stain_norm and self.stain_normalizer is not None:\n",
    "            try:\n",
    "                return self.stain_normalizer.normalize(img)\n",
    "            except Exception:\n",
    "                pass  # Skip normalization if it fails\n",
    "        return img\n",
    "\n",
    "    def _load_and_preprocess(self, sample_idx: str) -> np.ndarray:\n",
    "        \"\"\"Load and preprocess full image with optional tissue cropping.\"\"\"\n",
    "        img, mask = self._load_image_and_mask(sample_idx)\n",
    "\n",
    "        # Crop to tissue region if mask is available\n",
    "        if mask is not None:\n",
    "            img = self._crop_to_tissue_bbox(img, mask)\n",
    "\n",
    "        # Stain normalization\n",
    "        if self.use_stain_norm:\n",
    "            img = self._apply_stain_normalization(img)\n",
    "\n",
    "        return img\n",
    "\n",
    "    def _load_patches(self, sample_idx: str) -> List[np.ndarray]:\n",
    "        \"\"\"Load image as patches using TissueExtractor.\"\"\"\n",
    "        img, mask = self._load_image_and_mask(sample_idx)\n",
    "\n",
    "        if mask is None:\n",
    "            # If no mask, create a simple one (all tissue)\n",
    "            mask = np.ones(img.shape[:2], dtype=np.uint8) * 255\n",
    "\n",
    "        # Extract patches using TissueExtractor\n",
    "        patches, _ = self.tissue_extractor.get_valid_patches(\n",
    "            img=img,\n",
    "            mask=mask,\n",
    "            num_patches=self.num_patches,\n",
    "            strategy=self.patch_strategy,\n",
    "            stride=self.patch_size // 2 if self.patch_strategy == \"grid\" else None,\n",
    "            shuffle=False,\n",
    "        )\n",
    "\n",
    "        # Handle case where fewer patches are found than requested\n",
    "        if len(patches) == 0:\n",
    "            # Fallback: extract center crop\n",
    "            h, w = img.shape[:2]\n",
    "            cy, cx = h // 2, w // 2\n",
    "            half = self.patch_size // 2\n",
    "            y1 = max(0, cy - half)\n",
    "            x1 = max(0, cx - half)\n",
    "            y2 = min(h, y1 + self.patch_size)\n",
    "            x2 = min(w, x1 + self.patch_size)\n",
    "            fallback_patch = img[y1:y2, x1:x2]\n",
    "            fallback_patch = cv2.resize(\n",
    "                fallback_patch, (self.patch_size, self.patch_size)\n",
    "            )\n",
    "            patches = [fallback_patch] * self.num_patches\n",
    "\n",
    "        elif len(patches) < self.num_patches:\n",
    "            # Duplicate existing patches to reach num_patches\n",
    "            while len(patches) < self.num_patches:\n",
    "                patches.append(patches[len(patches) % len(patches)])\n",
    "\n",
    "        # Apply stain normalization to each patch\n",
    "        normalized_patches = []\n",
    "        for patch in patches:\n",
    "            patch = self._apply_stain_normalization(patch)\n",
    "            normalized_patches.append(patch)\n",
    "\n",
    "        return normalized_patches\n",
    "\n",
    "    def __getitem__(self, idx: int) -> Tuple[torch.Tensor, ...]:\n",
    "        sample_idx = self.samples[idx]\n",
    "\n",
    "        if self.use_patches:\n",
    "            patches = self._load_patches(sample_idx)\n",
    "\n",
    "            # Transform each patch\n",
    "            transformed_patches = []\n",
    "            for patch in patches:\n",
    "                patch_pil = Image.fromarray(patch)\n",
    "                if self.transform:\n",
    "                    patch_tensor = self.transform(patch_pil)\n",
    "                else:\n",
    "                    patch_tensor = transforms.ToTensor()(patch_pil)\n",
    "                transformed_patches.append(patch_tensor)\n",
    "\n",
    "            # Stack patches [num_patches, C, H, W]\n",
    "            img_tensor = torch.stack(transformed_patches)\n",
    "        else:\n",
    "            img = self._load_and_preprocess(sample_idx)\n",
    "            img_pil = Image.fromarray(img)\n",
    "\n",
    "            if self.transform:\n",
    "                img_tensor = self.transform(img_pil)\n",
    "            else:\n",
    "                img_tensor = transforms.ToTensor()(img_pil)\n",
    "\n",
    "        if self.is_test:\n",
    "            return img_tensor, sample_idx\n",
    "        else:\n",
    "            label = self.encoded_labels[idx]\n",
    "            return img_tensor, torch.tensor(label, dtype=torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PathologyDataModule(L.LightningDataModule):\n",
    "    \"\"\"Lightning DataModule for histopathology image classification.\n",
    "\n",
    "    Args:\n",
    "        train_data_dir: Directory containing training images and masks.\n",
    "        test_data_dir: Directory containing test images and masks.\n",
    "        train_labels_path: Path to CSV file with training labels.\n",
    "        batch_size: Batch size for dataloaders.\n",
    "        num_workers: Number of workers for dataloaders.\n",
    "        img_size: Target image size (used when not using patches).\n",
    "        use_mask: Whether to use masks for tissue extraction.\n",
    "        use_patches: Whether to use patch-based loading.\n",
    "        patch_size: Size of patches to extract.\n",
    "        num_patches: Number of patches per image.\n",
    "        min_tissue_ratio: Minimum tissue ratio for valid patches.\n",
    "        use_stain_norm: Whether to apply stain normalization.\n",
    "        val_split: Fraction of training data to use for validation.\n",
    "        random_seed: Random seed for reproducibility.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        train_data_dir: str = Config.TRAIN_DATA_DIR,\n",
    "        test_data_dir: str = Config.TEST_DATA_DIR,\n",
    "        train_labels_path: str = Config.TRAIN_LABELS_PATH,\n",
    "        batch_size: int = Config.BATCH_SIZE,\n",
    "        num_workers: int = Config.NUM_WORKERS,\n",
    "        img_size: int = Config.IMG_SIZE,\n",
    "        use_mask: bool = Config.USE_MASK,\n",
    "        use_patches: bool = Config.USE_PATCHES,\n",
    "        patch_size: int = Config.PATCH_SIZE,\n",
    "        num_patches: int = Config.NUM_PATCHES,\n",
    "        min_tissue_ratio: float = Config.MIN_TISSUE_AREA,\n",
    "        use_stain_norm: bool = Config.USE_STAIN_NORMALIZATION,\n",
    "        val_split: float = Config.VAL_SPLIT,\n",
    "        random_seed: int = Config.RANDOM_SEED,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.save_hyperparameters()\n",
    "\n",
    "        self.train_data_dir = train_data_dir\n",
    "        self.test_data_dir = test_data_dir\n",
    "        self.train_labels_path = train_labels_path\n",
    "        self.batch_size = batch_size\n",
    "        self.num_workers = num_workers\n",
    "        self.img_size = img_size\n",
    "        self.use_mask = use_mask\n",
    "        self.use_patches = use_patches\n",
    "        self.patch_size = patch_size\n",
    "        self.num_patches = num_patches\n",
    "        self.min_tissue_ratio = min_tissue_ratio\n",
    "        self.use_stain_norm = use_stain_norm\n",
    "        self.val_split = val_split\n",
    "        self.random_seed = random_seed\n",
    "\n",
    "        # Initialize label encoder\n",
    "        self.label_encoder = LabelEncoder()\n",
    "        self.label_encoder.fit(Config.CLASSES)\n",
    "\n",
    "        # Will be set in setup()\n",
    "        self.train_df = None\n",
    "        self.val_df = None\n",
    "        self.train_dataset = None\n",
    "        self.val_dataset = None\n",
    "        self.test_dataset = None\n",
    "\n",
    "    def _get_train_transforms(self) -> transforms.Compose:\n",
    "        \"\"\"Get augmentation transforms for training.\"\"\"\n",
    "        target_size = self.patch_size if self.use_patches else self.img_size\n",
    "        return transforms.Compose(\n",
    "            [\n",
    "                transforms.Resize((target_size, target_size)),\n",
    "                transforms.RandomHorizontalFlip(p=0.5),\n",
    "                transforms.RandomVerticalFlip(p=0.5),\n",
    "                transforms.RandomRotation(degrees=90),\n",
    "                transforms.ColorJitter(\n",
    "                    brightness=0.2,\n",
    "                    contrast=0.2,\n",
    "                    saturation=0.1,\n",
    "                    hue=0.05,\n",
    "                ),\n",
    "                transforms.ToTensor(),\n",
    "                transforms.Normalize(\n",
    "                    mean=[0.485, 0.456, 0.406],\n",
    "                    std=[0.229, 0.224, 0.225],\n",
    "                ),\n",
    "            ]\n",
    "        )\n",
    "\n",
    "    def _get_val_transforms(self) -> transforms.Compose:\n",
    "        \"\"\"Get transforms for validation/test (no augmentation).\"\"\"\n",
    "        target_size = self.patch_size if self.use_patches else self.img_size\n",
    "        return transforms.Compose(\n",
    "            [\n",
    "                transforms.Resize((target_size, target_size)),\n",
    "                transforms.ToTensor(),\n",
    "                transforms.Normalize(\n",
    "                    mean=[0.485, 0.456, 0.406],\n",
    "                    std=[0.229, 0.224, 0.225],\n",
    "                ),\n",
    "            ]\n",
    "        )\n",
    "\n",
    "    def setup(self, stage: Optional[str] = None):\n",
    "        \"\"\"Setup datasets for each stage.\"\"\"\n",
    "        if stage == \"fit\" or stage is None:\n",
    "            # Load and split training data\n",
    "            full_df = pd.read_csv(self.train_labels_path)\n",
    "\n",
    "            # Stratified split\n",
    "            from sklearn.model_selection import train_test_split\n",
    "\n",
    "            self.train_df, self.val_df = train_test_split(\n",
    "                full_df,\n",
    "                test_size=self.val_split,\n",
    "                stratify=full_df[\"label\"],\n",
    "                random_state=self.random_seed,\n",
    "            )\n",
    "\n",
    "            # Training dataset: random strategy with half overlap\n",
    "            self.train_dataset = PathologyDataset(\n",
    "                data_dir=self.train_data_dir,\n",
    "                labels_df=self.train_df,\n",
    "                transform=self._get_train_transforms(),\n",
    "                use_mask=self.use_mask,\n",
    "                use_patches=self.use_patches,\n",
    "                patch_size=self.patch_size,\n",
    "                num_patches=self.num_patches,\n",
    "                patch_strategy=\"random\",  # Random for training\n",
    "                min_tissue_ratio=self.min_tissue_ratio,\n",
    "                use_stain_norm=self.use_stain_norm,\n",
    "                is_test=False,\n",
    "                label_encoder=self.label_encoder,\n",
    "            )\n",
    "\n",
    "            # Validation dataset: grid strategy with no overlap\n",
    "            self.val_dataset = PathologyDataset(\n",
    "                data_dir=self.train_data_dir,\n",
    "                labels_df=self.val_df,\n",
    "                transform=self._get_val_transforms(),\n",
    "                use_mask=self.use_mask,\n",
    "                use_patches=self.use_patches,\n",
    "                patch_size=self.patch_size,\n",
    "                num_patches=self.num_patches,\n",
    "                patch_strategy=\"grid\",  # Grid for validation\n",
    "                min_tissue_ratio=self.min_tissue_ratio,\n",
    "                use_stain_norm=self.use_stain_norm,\n",
    "                is_test=False,\n",
    "                label_encoder=self.label_encoder,\n",
    "            )\n",
    "\n",
    "        if stage == \"test\" or stage == \"predict\" or stage is None:\n",
    "            # Test dataset: grid strategy with no overlap\n",
    "            self.test_dataset = PathologyDataset(\n",
    "                data_dir=self.test_data_dir,\n",
    "                labels_df=None,\n",
    "                transform=self._get_val_transforms(),\n",
    "                use_mask=self.use_mask,\n",
    "                use_patches=self.use_patches,\n",
    "                patch_size=self.patch_size,\n",
    "                num_patches=self.num_patches,\n",
    "                patch_strategy=\"grid\",\n",
    "                min_tissue_ratio=self.min_tissue_ratio,\n",
    "                use_stain_norm=self.use_stain_norm,\n",
    "                is_test=True,\n",
    "                label_encoder=self.label_encoder,\n",
    "            )\n",
    "\n",
    "    def train_dataloader(self) -> DataLoader:\n",
    "        return DataLoader(\n",
    "            self.train_dataset,\n",
    "            batch_size=self.batch_size,\n",
    "            shuffle=True,\n",
    "            num_workers=self.num_workers,\n",
    "            pin_memory=True,\n",
    "            drop_last=True,\n",
    "        )\n",
    "\n",
    "    def val_dataloader(self) -> DataLoader:\n",
    "        return DataLoader(\n",
    "            self.val_dataset,\n",
    "            batch_size=self.batch_size,\n",
    "            shuffle=False,\n",
    "            num_workers=self.num_workers,\n",
    "            pin_memory=True,\n",
    "            drop_last=False,\n",
    "        )\n",
    "\n",
    "    def test_dataloader(self) -> DataLoader:\n",
    "        return DataLoader(\n",
    "            self.test_dataset,\n",
    "            batch_size=self.batch_size,\n",
    "            shuffle=False,\n",
    "            num_workers=self.num_workers,\n",
    "            pin_memory=True,\n",
    "            drop_last=False,\n",
    "        )\n",
    "\n",
    "    def predict_dataloader(self) -> DataLoader:\n",
    "        return self.test_dataloader()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "\n",
    "\n",
    "class MILGradCAM:\n",
    "    def __init__(self, model, target_layer_name=\"layer4\"):\n",
    "        self.model = model\n",
    "        self.model.eval()\n",
    "        self.gradients = None\n",
    "        self.activations = None\n",
    "        self.target_layer_name = target_layer_name\n",
    "\n",
    "        # Hook into the backbone (e.g., layer4 of ResNet)\n",
    "        self._register_hooks()\n",
    "\n",
    "    def _register_hooks(self):\n",
    "        def forward_hook(module, input, output):\n",
    "            self.activations = output\n",
    "\n",
    "        def backward_hook(module, grad_in, grad_out):\n",
    "            self.gradients = grad_out[0]\n",
    "\n",
    "        # Find the target layer in the timm backbone\n",
    "        # usually model.backbone.layer4 for ResNet\n",
    "        target_layer = dict([*self.model.backbone.named_modules()])[\n",
    "            self.target_layer_name\n",
    "        ]\n",
    "\n",
    "        target_layer.register_forward_hook(forward_hook)\n",
    "        target_layer.register_full_backward_hook(backward_hook)\n",
    "\n",
    "    def forward(self, x, target_class_idx=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: Input tensor [1, num_patches, C, H, W]\n",
    "            target_class_idx: The class index to visualize. If None, uses predicted class.\n",
    "        \"\"\"\n",
    "        b, n, c, h, w = x.shape\n",
    "\n",
    "        # 1. Zero grads\n",
    "        self.model.zero_grad()\n",
    "\n",
    "        # 2. Forward pass to get logits and attention weights\n",
    "        # We need to manually run parts of the forward pass to capture attention weights\n",
    "        # because the original forward() consumes them internally.\n",
    "\n",
    "        # A. Reshape\n",
    "        x_reshaped = x.view(b * n, c, h, w)\n",
    "\n",
    "        # B. Backbone features (Hooks capture activations here)\n",
    "        features = self.model.backbone(x_reshaped)\n",
    "\n",
    "        # C. Reshape features\n",
    "        features = features.view(b, n, -1)\n",
    "\n",
    "        # D. Get Attention Weights (The \"Weak Supervision\")\n",
    "        attention_weights = None\n",
    "        aggregated_features = None\n",
    "\n",
    "        if self.model.patch_aggregation == \"attention\":\n",
    "            attention_scores = self.model.attention(features)\n",
    "            attention_weights = F.softmax(attention_scores, dim=1)  # [B, N, 1]\n",
    "            aggregated_features = (attention_weights * features).sum(dim=1)\n",
    "        elif self.model.patch_aggregation == \"mean\":\n",
    "            aggregated_features = features.mean(dim=1)\n",
    "            attention_weights = torch.ones((b, n, 1)).to(x.device) / n\n",
    "\n",
    "        # E. Classifier\n",
    "        logits = self.model.classifier(aggregated_features)\n",
    "\n",
    "        # 3. Determine target class\n",
    "        if target_class_idx is None:\n",
    "            target_class_idx = torch.argmax(logits, dim=1).item()\n",
    "\n",
    "        # 4. Backward pass to get gradients\n",
    "        score = logits[0, target_class_idx]\n",
    "        score.backward()\n",
    "\n",
    "        return {\n",
    "            \"logits\": logits,\n",
    "            \"attention_weights\": attention_weights.detach().cpu().numpy(),  # [1, N, 1]\n",
    "            \"activations\": self.activations.detach().cpu().numpy(),  # [N, 2048, 7, 7]\n",
    "            \"gradients\": self.gradients.detach().cpu().numpy(),  # [N, 2048, 7, 7]\n",
    "            \"target_class\": target_class_idx,\n",
    "        }\n",
    "\n",
    "    def generate_cam(self, activations, gradients):\n",
    "        \"\"\"\n",
    "        Compute Grad-CAM heatmap from activations and gradients.\n",
    "        Standard Grad-CAM formula: ReLU(Sum(Weights * Activations))\n",
    "        \"\"\"\n",
    "        # Global Average Pooling of gradients to get weights\n",
    "        weights = np.mean(gradients, axis=(2, 3))  # [N, Channels]\n",
    "\n",
    "        # Weighted sum of activations\n",
    "        # activations: [N, C, H, W]\n",
    "        # weights: [N, C] -> reshape to [N, C, 1, 1]\n",
    "        weights = weights[:, :, np.newaxis, np.newaxis]\n",
    "\n",
    "        cam = np.sum(weights * activations, axis=1)  # [N, H, W]\n",
    "\n",
    "        # Apply ReLU\n",
    "        cam = np.maximum(cam, 0)\n",
    "\n",
    "        # Normalize per patch\n",
    "        cams_normalized = []\n",
    "        for c in cam:\n",
    "            if np.max(c) > 0:\n",
    "                c = c / np.max(c)\n",
    "            cams_normalized.append(c)\n",
    "\n",
    "        return np.array(cams_normalized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "def visualize_weakly_supervised_cam(\n",
    "    model, img_path, mask_path=None, patch_size=224, device=\"cuda\"\n",
    "):\n",
    "    \"\"\"\n",
    "    Visualizes:\n",
    "    1. The Attention Map (Which patches define the class).\n",
    "    2. The Grad-CAM (Which pixels inside those patches define the class).\n",
    "    \"\"\"\n",
    "    model.to(device)\n",
    "    grad_cam = MILGradCAM(model, target_layer_name=\"layer4\")\n",
    "\n",
    "    # --- 1. Load Image & Prepare Patches ---\n",
    "    img = np.array(Image.open(img_path).convert(\"RGB\"))\n",
    "    original_h, original_w = img.shape[:2]\n",
    "\n",
    "    # Create a dummy mask if none exists (all tissue)\n",
    "    if mask_path and Path(mask_path).exists():\n",
    "        mask = np.array(Image.open(mask_path).convert(\"L\"))\n",
    "    else:\n",
    "        mask = np.ones((original_h, original_w), dtype=np.uint8) * 255\n",
    "\n",
    "    # Use grid extraction to cover the image\n",
    "    extractor = TissueExtractor(patch_size=patch_size, min_tissue_ratio=0.05)\n",
    "    patches_img, _, coords = extractor.get_all_valid_patches(\n",
    "        img, mask, stride=patch_size\n",
    "    )\n",
    "\n",
    "    if len(patches_img) == 0:\n",
    "        print(\"No tissue found.\")\n",
    "        return\n",
    "\n",
    "    # Transform patches\n",
    "    transform = transforms.Compose(\n",
    "        [\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    input_tensor = torch.stack([transform(Image.fromarray(p)) for p in patches_img])\n",
    "    input_tensor = input_tensor.unsqueeze(0).to(device)  # [1, N, C, H, W]\n",
    "\n",
    "    # --- 2. Run Grad-CAM ---\n",
    "    results = grad_cam.forward(input_tensor)\n",
    "\n",
    "    # --- 3. Process Results ---\n",
    "    attention_weights = results[\"attention_weights\"][0, :, 0]  # [N]\n",
    "    activations = results[\"activations\"]\n",
    "    gradients = results[\"gradients\"]\n",
    "\n",
    "    # Generate pixel-level heatmaps for patches\n",
    "    patch_cams = grad_cam.generate_cam(activations, gradients)  # [N, 7, 7]\n",
    "\n",
    "    # --- 4. Reconstruct Maps ---\n",
    "\n",
    "    # Map 1: Attention Map (Blocky heatmap based on patch importance)\n",
    "    attention_map = np.zeros((original_h, original_w), dtype=np.float32)\n",
    "    # Map 2: Fine-grained Grad-CAM (Detailed heatmap)\n",
    "    gradcam_map = np.zeros((original_h, original_w), dtype=np.float32)\n",
    "    # Count map for averaging overlapping patches (if stride < patch_size)\n",
    "    count_map = np.zeros((original_h, original_w), dtype=np.float32)\n",
    "\n",
    "    # Normalize attention weights for visualization\n",
    "    att_vis = (attention_weights - attention_weights.min()) / (\n",
    "        attention_weights.max() - attention_weights.min() + 1e-8\n",
    "    )\n",
    "\n",
    "    for i, (y, x) in enumerate(coords):\n",
    "        # Resize low-res CAM (7x7) to patch size (224x224)\n",
    "        resized_cam = cv2.resize(patch_cams[i], (patch_size, patch_size))\n",
    "\n",
    "        # Attention value for this patch\n",
    "        att_val = att_vis[i]\n",
    "\n",
    "        # Add to global maps\n",
    "        # Note: We multiply CAM by Attention. High attention patch + High CAM pixel = High importance\n",
    "        gradcam_map[y : y + patch_size, x : x + patch_size] += resized_cam * att_val\n",
    "        attention_map[y : y + patch_size, x : x + patch_size] += att_val\n",
    "        count_map[y : y + patch_size, x : x + patch_size] += 1\n",
    "\n",
    "    # Average out overlaps\n",
    "    mask_indices = count_map > 0\n",
    "    gradcam_map[mask_indices] /= count_map[mask_indices]\n",
    "    attention_map[mask_indices] /= count_map[mask_indices]\n",
    "\n",
    "    # --- 5. Plotting ---\n",
    "    predicted_class = Config.CLASSES[results[\"target_class\"]]\n",
    "\n",
    "    fig, axs = plt.subplots(1, 3, figsize=(20, 6))\n",
    "\n",
    "    # Original\n",
    "    axs[0].imshow(img)\n",
    "    axs[0].set_title(f\"Original Image\\nPred: {predicted_class}\")\n",
    "    axs[0].axis(\"off\")\n",
    "\n",
    "    # Attention Map (Instance Importance)\n",
    "    heatmap_att = cv2.applyColorMap(np.uint8(255 * attention_map), cv2.COLORMAP_JET)\n",
    "    heatmap_att = cv2.cvtColor(heatmap_att, cv2.COLOR_BGR2RGB)\n",
    "    overlay_att = cv2.addWeighted(img, 0.6, heatmap_att, 0.4, 0)\n",
    "    axs[1].imshow(overlay_att)\n",
    "    axs[1].set_title(\"Attention Map (Weak Supervision)\\nShows important patches\")\n",
    "    axs[1].axis(\"off\")\n",
    "\n",
    "    # Grad-CAM (Pixel Importance)\n",
    "    # Normalize for visualization\n",
    "    gradcam_map = gradcam_map / (np.max(gradcam_map) + 1e-8)\n",
    "    heatmap_cam = cv2.applyColorMap(np.uint8(255 * gradcam_map), cv2.COLORMAP_JET)\n",
    "    heatmap_cam = cv2.cvtColor(heatmap_cam, cv2.COLOR_BGR2RGB)\n",
    "    overlay_cam = cv2.addWeighted(img, 0.6, heatmap_cam, 0.4, 0)\n",
    "    axs[2].imshow(overlay_cam)\n",
    "    axs[2].set_title(\"Weighted Grad-CAM\\nAttention * Backbone Gradients\")\n",
    "    axs[2].axis(\"off\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# %% [markdown]\n",
    "# ## Execute Visualization\n",
    "# Select a sample from the test set or validation set to visualize."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "# 1. Load Model\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "checkpoint_path = trainer.checkpoint_callback.best_model_path\n",
    "model = PathologyModel.load_from_checkpoint(checkpoint_path)\n",
    "model.eval()\n",
    "\n",
    "# 2. Pick an image\n",
    "# Let's grab an image from the test folder\n",
    "test_files = sorted(list(Path(Config.TEST_DATA_DIR).glob(\"img_*.png\")))\n",
    "if test_files:\n",
    "    sample_img_path = test_files[0]\n",
    "    sample_mask_path = (\n",
    "        Path(Config.TEST_DATA_DIR) / f\"mask_{sample_img_path.name.split('_')[1]}\"\n",
    "    )\n",
    "\n",
    "    print(f\"Visualizing: {sample_img_path}\")\n",
    "\n",
    "    visualize_weakly_supervised_cam(\n",
    "        model=model,\n",
    "        img_path=str(sample_img_path),\n",
    "        mask_path=str(sample_mask_path),\n",
    "        patch_size=Config.PATCH_SIZE,\n",
    "        device=device,\n",
    "    )\n",
    "else:\n",
    "    print(\"No test images found to visualize.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.13.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
